+ run_with_logger --tag startup-script
+ local tag=dataproc-script
+ local pid=1265
+ [[ --tag == \-\-\t\a\g ]]
+ tag=dataproc-startup-script
+ shift 2
+ [[ 0 -eq 0 ]]
+ exec
++ logger -s -t 'dataproc-startup-script[1265]'
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + cd /tmp
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + trap logstacktrace ERR
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + loginfo 'Starting Dataproc startup script'
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + echo 'Starting Dataproc startup script'
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: Starting Dataproc startup script
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: ++ get_metadata_value cpu-platform
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + loginfo 'CPU platform: Intel Emerald Rapids'
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + echo 'CPU platform: Intel Emerald Rapids'
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: CPU platform: Intel Emerald Rapids
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + set -a
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: ++ get_metadata_project_id
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: ++ get_dataproc_metadata DATAPROC_METADATA_PROJECT_ID ../project/project-id
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + PROJECT=euphoric-coral-451717-v8
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: ++ get_metadata_dataproc_region
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: ++ get_dataproc_metadata DATAPROC_METADATA_REGION attributes/dataproc-region
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + REGION=us-east1
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: ++ get_metadata_zone
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: ++ local zone_uri
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: +++ get_dataproc_metadata DATAPROC_METADATA_ZONE zone
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: +++ set +x
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: ++ zone_uri=projects/679657336577/zones/us-east1-d
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: ++ echo us-east1-d
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + ZONE=us-east1-d
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: ++ get_metadata_role
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: ++ get_dataproc_metadata DATAPROC_METADATA_ROLE attributes/dataproc-role
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + ROLE=Master
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + is_rm_image
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\r\a\y ]]
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: ++ get_metadata_bucket
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: ++ get_dataproc_metadata DATAPROC_METADATA_BUCKET attributes/dataproc-bucket
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + CONFIGBUCKET=dataproc-staging-us-east1-679657336577-kutp8kah
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: ++ get_metadata_temp_bucket
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: ++ get_dataproc_metadata DATAPROC_METADATA_TEMP_BUCKET attributes/dataproc-temp-bucket
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + TEMP_BUCKET=dataproc-temp-us-east1-679657336577-juievcnm
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: ++ get_metadata_cluster_name
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: ++ get_dataproc_metadata DATAPROC_METADATA_CLUSTER_NAME attributes/dataproc-cluster-name
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + CLUSTER_NAME=cluster-dip-01
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: ++ get_metadata_cluster_uuid
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: ++ get_dataproc_metadata DATAPROC_METADATA_CLUSTER_UUID attributes/dataproc-cluster-uuid
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + CLUSTER_UUID=0b63cfa9-1f8a-481e-87c8-a0cf661f3625
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: ++ get_metadata_worker_count
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: ++ get_dataproc_metadata DATAPROC_METADATA_WORKER_COUNT attributes/dataproc-worker-count
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + WORKER_COUNT=3
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: ++ get_metadata_master
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: ++ get_dataproc_metadata DATAPROC_METADATA_MASTER attributes/dataproc-master
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + DATAPROC_MASTER=cluster-dip-01-m
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: ++ get_metadata_master_additional
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: ++ get_dataproc_metadata DATAPROC_METADATA_MASTER_ADDITIONAL attributes/dataproc-master-additional
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + DATAPROC_MASTER_ADDITIONAL=
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + MASTER_HOSTNAMES=($DATAPROC_MASTER ${DATAPROC_MASTER_ADDITIONAL//,/ })
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + MASTER_COUNT=1
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: ++ hostname -s
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + MY_HOSTNAME=cluster-dip-01-m
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: ++ hostname -f
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + MY_FULL_HOSTNAME=cluster-dip-01-m.c.euphoric-coral-451717-v8.internal
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: ++ dnsdomainname
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + DOMAIN=c.euphoric-coral-451717-v8.internal
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + DATAPROC_MASTER_FQDN=cluster-dip-01-m.c.euphoric-coral-451717-v8.internal
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + KEYTAB_DIR=/etc/security/keytab
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + CLUSTER_STAGING_FOLDER=gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + CLUSTER_TEMP_FOLDER=gs://dataproc-temp-us-east1-679657336577-juievcnm/0b63cfa9-1f8a-481e-87c8-a0cf661f3625
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + DATAPROC_ETC_DIR=/etc/google-dataproc
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + CLUSTER_PROPERTIES_DIR=/tmp/cluster/properties
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + UNINSTALL_TMP_DIR=/tmp/dataproc/uninstall
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + UNINSTALL_PRE_ACTIVATE_TMP_DIR=/tmp/dataproc/uninstall-pre-activate
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + COMPONENT_SERVICES=('hadoop-hdfs-namenode' 'hadoop-hdfs-datanode' 'hadoop-hdfs-zkfc' 'hadoop-hdfs-secondarynamenode' 'hadoop-hdfs-journalnode' 'hive-metastore' 'hive-server2' 'mysql-server' 'spark-history-server')
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + readonly COMPONENT_SERVICES
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + [[ standard != \s\t\a\n\d\a\r\d ]]
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + set +a
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + mkdir -p /tmp/dataproc /tmp/dataproc/commands /tmp/dataproc/components /tmp/dataproc/uninstall /tmp/dataproc/uninstall-pre-activate
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + [[ standard == \s\t\a\n\d\a\r\d ]]
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + is_version_at_least 2.2 2.3
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + local reenable_x=false
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + [[ -o xtrace ]]
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + set +x
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + case ${compare_versions_result} in
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + return 1
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + merge_java_properties /tmp/cluster/properties/dataproc.properties /etc/google-dataproc/dataproc.properties
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + local -r src=/tmp/cluster/properties/dataproc.properties
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + local -r dest=/etc/google-dataproc/dataproc.properties
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + local -r 'header=\n# User-supplied properties.'
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + [[ ! -f /tmp/cluster/properties/dataproc.properties ]]
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + echo -e '\n# User-supplied properties.'
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + cat /tmp/cluster/properties/dataproc.properties
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + loginfo 'Merged /tmp/cluster/properties/dataproc.properties.'
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + echo 'Merged /tmp/cluster/properties/dataproc.properties.'
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: Merged /tmp/cluster/properties/dataproc.properties.
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + is_rm_image
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\r\a\y ]]
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + merge_java_properties /etc/google-dataproc/dataproc.custom.properties /etc/google-dataproc/dataproc.properties '\n# Custom image supplied properties'
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + local -r src=/etc/google-dataproc/dataproc.custom.properties
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + local -r dest=/etc/google-dataproc/dataproc.properties
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + local -r 'header=\n# Custom image supplied properties'
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + [[ ! -f /etc/google-dataproc/dataproc.custom.properties ]]
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + loginfo 'Skipping merging /etc/google-dataproc/dataproc.custom.properties, file does not exist.'
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + echo 'Skipping merging /etc/google-dataproc/dataproc.custom.properties, file does not exist.'
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: Skipping merging /etc/google-dataproc/dataproc.custom.properties, file does not exist.
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + return 0
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + [[ standard == \s\t\a\n\d\a\r\d ]]
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + update_credentials_for_tpc
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: + local storage_api_endpoint
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: ++ get_storage_api_endpoint
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: ++ local universe_domain
<13>Apr 27 18:07:21 dataproc-startup-script[1265]: +++ gcloud config get core/universe_domain
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ universe_domain=googleapis.com
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ [[ -n googleapis.com ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ echo storage.googleapis.com
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + storage_api_endpoint=storage.googleapis.com
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ storage.googleapis.com == \s\t\o\r\a\g\e\.\a\p\i\s\-\t\p\c\z\e\r\o\.\g\o\o\g ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + determine_selected_components
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local -a default_components
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + default_components=($(get_default_components))
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ get_default_components
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ set +o pipefail
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ tr '[:upper:]' '[:lower:]'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ get_dataproc_property_or_default dataproc.components.default 'earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn stackdriver-agent-container google-fluentd-container pig fluentbit-ucp otel-ucp miniconda3'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ get_dataproc_property dataproc.components.activate
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + OPTIONAL_COMPONENTS_VALUE='knox proxy-agent'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + add_default_components earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn stackdriver-agent-container google-fluentd-container pig fluentbit-ucp otel-ucp miniconda3
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + default_components=('earlyoom' 'hdfs' 'hive-metastore' 'hive-server2' 'mapreduce' 'mysql' 'npd' 'spark' 'tez' 'yarn' 'stackdriver-agent-container' 'google-fluentd-container' 'pig' 'fluentbit-ucp' 'otel-ucp' 'miniconda3')
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local default_components
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + for component in "${default_components[@]}"
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + is_component_explicitly_unselected earlyoom
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local -r component=earlyoom
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local deactivated_components
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ get_components_to_deactivate
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ tr '[:upper:]' '[:lower:]'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ get_dataproc_property dataproc.components.deactivate
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + deactivated_components='google-fluentd-container stackdriver-agent-container'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ google-fluentd-container stackdriver-agent-container == *earlyoom* ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + add_optional_component earlyoom
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local -r component=earlyoom
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ knox proxy-agent == *earlyoom* ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ -z knox proxy-agent ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + OPTIONAL_COMPONENTS_VALUE+=' earlyoom'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + sed -i 's/dataproc.components.activate=.*/dataproc.components.activate=knox proxy-agent earlyoom/g' /etc/google-dataproc/dataproc.properties
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + for component in "${default_components[@]}"
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + is_component_explicitly_unselected hdfs
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local -r component=hdfs
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local deactivated_components
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ get_components_to_deactivate
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ get_dataproc_property dataproc.components.deactivate
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ tr '[:upper:]' '[:lower:]'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + deactivated_components='google-fluentd-container stackdriver-agent-container'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ google-fluentd-container stackdriver-agent-container == *hdfs* ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + add_optional_component hdfs
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local -r component=hdfs
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ knox proxy-agent earlyoom == *hdfs* ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ -z knox proxy-agent earlyoom ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + OPTIONAL_COMPONENTS_VALUE+=' hdfs'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + sed -i 's/dataproc.components.activate=.*/dataproc.components.activate=knox proxy-agent earlyoom hdfs/g' /etc/google-dataproc/dataproc.properties
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + for component in "${default_components[@]}"
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + is_component_explicitly_unselected hive-metastore
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local -r component=hive-metastore
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local deactivated_components
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ get_components_to_deactivate
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ get_dataproc_property dataproc.components.deactivate
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ tr '[:upper:]' '[:lower:]'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + deactivated_components='google-fluentd-container stackdriver-agent-container'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ google-fluentd-container stackdriver-agent-container == *hive-metastore* ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + add_optional_component hive-metastore
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local -r component=hive-metastore
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ knox proxy-agent earlyoom hdfs == *hive-metastore* ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ -z knox proxy-agent earlyoom hdfs ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + OPTIONAL_COMPONENTS_VALUE+=' hive-metastore'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + sed -i 's/dataproc.components.activate=.*/dataproc.components.activate=knox proxy-agent earlyoom hdfs hive-metastore/g' /etc/google-dataproc/dataproc.properties
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + for component in "${default_components[@]}"
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + is_component_explicitly_unselected hive-server2
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local -r component=hive-server2
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local deactivated_components
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ get_components_to_deactivate
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ get_dataproc_property dataproc.components.deactivate
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ tr '[:upper:]' '[:lower:]'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + deactivated_components='google-fluentd-container stackdriver-agent-container'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ google-fluentd-container stackdriver-agent-container == *hive-server2* ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + add_optional_component hive-server2
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local -r component=hive-server2
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ knox proxy-agent earlyoom hdfs hive-metastore == *hive-server2* ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ -z knox proxy-agent earlyoom hdfs hive-metastore ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + OPTIONAL_COMPONENTS_VALUE+=' hive-server2'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + sed -i 's/dataproc.components.activate=.*/dataproc.components.activate=knox proxy-agent earlyoom hdfs hive-metastore hive-server2/g' /etc/google-dataproc/dataproc.properties
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + for component in "${default_components[@]}"
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + is_component_explicitly_unselected mapreduce
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local -r component=mapreduce
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local deactivated_components
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ get_components_to_deactivate
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ get_dataproc_property dataproc.components.deactivate
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ tr '[:upper:]' '[:lower:]'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + deactivated_components='google-fluentd-container stackdriver-agent-container'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ google-fluentd-container stackdriver-agent-container == *mapreduce* ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + add_optional_component mapreduce
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local -r component=mapreduce
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ knox proxy-agent earlyoom hdfs hive-metastore hive-server2 == *mapreduce* ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ -z knox proxy-agent earlyoom hdfs hive-metastore hive-server2 ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + OPTIONAL_COMPONENTS_VALUE+=' mapreduce'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + sed -i 's/dataproc.components.activate=.*/dataproc.components.activate=knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce/g' /etc/google-dataproc/dataproc.properties
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + for component in "${default_components[@]}"
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + is_component_explicitly_unselected mysql
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local -r component=mysql
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local deactivated_components
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ get_components_to_deactivate
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ get_dataproc_property dataproc.components.deactivate
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ tr '[:upper:]' '[:lower:]'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + deactivated_components='google-fluentd-container stackdriver-agent-container'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ google-fluentd-container stackdriver-agent-container == *mysql* ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + add_optional_component mysql
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local -r component=mysql
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce == *mysql* ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ -z knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + OPTIONAL_COMPONENTS_VALUE+=' mysql'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + sed -i 's/dataproc.components.activate=.*/dataproc.components.activate=knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql/g' /etc/google-dataproc/dataproc.properties
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + for component in "${default_components[@]}"
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + is_component_explicitly_unselected npd
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local -r component=npd
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local deactivated_components
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ get_components_to_deactivate
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ get_dataproc_property dataproc.components.deactivate
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ tr '[:upper:]' '[:lower:]'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + deactivated_components='google-fluentd-container stackdriver-agent-container'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ google-fluentd-container stackdriver-agent-container == *npd* ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + add_optional_component npd
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local -r component=npd
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql == *npd* ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ -z knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + OPTIONAL_COMPONENTS_VALUE+=' npd'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + sed -i 's/dataproc.components.activate=.*/dataproc.components.activate=knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd/g' /etc/google-dataproc/dataproc.properties
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + for component in "${default_components[@]}"
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + is_component_explicitly_unselected spark
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local -r component=spark
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local deactivated_components
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ get_components_to_deactivate
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ get_dataproc_property dataproc.components.deactivate
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ tr '[:upper:]' '[:lower:]'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + deactivated_components='google-fluentd-container stackdriver-agent-container'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ google-fluentd-container stackdriver-agent-container == *spark* ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + add_optional_component spark
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local -r component=spark
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd == *spark* ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ -z knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + OPTIONAL_COMPONENTS_VALUE+=' spark'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + sed -i 's/dataproc.components.activate=.*/dataproc.components.activate=knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark/g' /etc/google-dataproc/dataproc.properties
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + for component in "${default_components[@]}"
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + is_component_explicitly_unselected tez
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local -r component=tez
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local deactivated_components
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ get_components_to_deactivate
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ get_dataproc_property dataproc.components.deactivate
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ tr '[:upper:]' '[:lower:]'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + deactivated_components='google-fluentd-container stackdriver-agent-container'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ google-fluentd-container stackdriver-agent-container == *tez* ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + add_optional_component tez
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local -r component=tez
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark == *tez* ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ -z knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + OPTIONAL_COMPONENTS_VALUE+=' tez'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + sed -i 's/dataproc.components.activate=.*/dataproc.components.activate=knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez/g' /etc/google-dataproc/dataproc.properties
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + for component in "${default_components[@]}"
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + is_component_explicitly_unselected yarn
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local -r component=yarn
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local deactivated_components
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ get_components_to_deactivate
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ tr '[:upper:]' '[:lower:]'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ get_dataproc_property dataproc.components.deactivate
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + deactivated_components='google-fluentd-container stackdriver-agent-container'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ google-fluentd-container stackdriver-agent-container == *yarn* ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + add_optional_component yarn
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local -r component=yarn
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez == *yarn* ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ -z knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + OPTIONAL_COMPONENTS_VALUE+=' yarn'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + sed -i 's/dataproc.components.activate=.*/dataproc.components.activate=knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn/g' /etc/google-dataproc/dataproc.properties
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + for component in "${default_components[@]}"
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + is_component_explicitly_unselected stackdriver-agent-container
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local -r component=stackdriver-agent-container
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local deactivated_components
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ get_components_to_deactivate
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ get_dataproc_property dataproc.components.deactivate
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ tr '[:upper:]' '[:lower:]'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + deactivated_components='google-fluentd-container stackdriver-agent-container'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ google-fluentd-container stackdriver-agent-container == *stackdriver-agent-container* ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + for component in "${default_components[@]}"
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + is_component_explicitly_unselected google-fluentd-container
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local -r component=google-fluentd-container
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local deactivated_components
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ get_components_to_deactivate
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ tr '[:upper:]' '[:lower:]'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ get_dataproc_property dataproc.components.deactivate
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + deactivated_components='google-fluentd-container stackdriver-agent-container'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ google-fluentd-container stackdriver-agent-container == *google-fluentd-container* ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + for component in "${default_components[@]}"
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + is_component_explicitly_unselected pig
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local -r component=pig
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local deactivated_components
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ get_components_to_deactivate
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ get_dataproc_property dataproc.components.deactivate
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ tr '[:upper:]' '[:lower:]'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + deactivated_components='google-fluentd-container stackdriver-agent-container'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ google-fluentd-container stackdriver-agent-container == *pig* ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + add_optional_component pig
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local -r component=pig
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn == *pig* ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ -z knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + OPTIONAL_COMPONENTS_VALUE+=' pig'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + sed -i 's/dataproc.components.activate=.*/dataproc.components.activate=knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig/g' /etc/google-dataproc/dataproc.properties
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + for component in "${default_components[@]}"
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + is_component_explicitly_unselected fluentbit-ucp
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local -r component=fluentbit-ucp
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local deactivated_components
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ get_components_to_deactivate
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ get_dataproc_property dataproc.components.deactivate
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ tr '[:upper:]' '[:lower:]'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + deactivated_components='google-fluentd-container stackdriver-agent-container'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ google-fluentd-container stackdriver-agent-container == *fluentbit-ucp* ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + add_optional_component fluentbit-ucp
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local -r component=fluentbit-ucp
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig == *fluentbit-ucp* ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ -z knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + OPTIONAL_COMPONENTS_VALUE+=' fluentbit-ucp'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + sed -i 's/dataproc.components.activate=.*/dataproc.components.activate=knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp/g' /etc/google-dataproc/dataproc.properties
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + for component in "${default_components[@]}"
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + is_component_explicitly_unselected otel-ucp
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local -r component=otel-ucp
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local deactivated_components
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ get_components_to_deactivate
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ get_dataproc_property dataproc.components.deactivate
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ tr '[:upper:]' '[:lower:]'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + deactivated_components='google-fluentd-container stackdriver-agent-container'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ google-fluentd-container stackdriver-agent-container == *otel-ucp* ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + add_optional_component otel-ucp
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local -r component=otel-ucp
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp == *otel-ucp* ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ -z knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + OPTIONAL_COMPONENTS_VALUE+=' otel-ucp'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + sed -i 's/dataproc.components.activate=.*/dataproc.components.activate=knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp/g' /etc/google-dataproc/dataproc.properties
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + for component in "${default_components[@]}"
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + is_component_explicitly_unselected miniconda3
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local -r component=miniconda3
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local deactivated_components
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ get_components_to_deactivate
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ get_dataproc_property dataproc.components.deactivate
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ tr '[:upper:]' '[:lower:]'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + deactivated_components='google-fluentd-container stackdriver-agent-container'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ google-fluentd-container stackdriver-agent-container == *miniconda3* ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + add_optional_component miniconda3
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local -r component=miniconda3
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp == *miniconda3* ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ -z knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + OPTIONAL_COMPONENTS_VALUE+=' miniconda3'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + sed -i 's/dataproc.components.activate=.*/dataproc.components.activate=knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3/g' /etc/google-dataproc/dataproc.properties
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + is_rm_image
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\r\a\y ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ standard == \s\t\a\n\d\a\r\d ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + (( 1 > 1 ))
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local caching_enabled
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ get_dataproc_property_or_default dataproc.cluster.caching.enabled false
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + caching_enabled=false
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ false == \t\r\u\e ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + SELECTED_COMPONENTS=(${OPTIONAL_COMPONENTS_VALUE})
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + is_component_selected kerberos
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local -r component=kerberos
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local activated_components
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ get_components_to_activate
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ get_dataproc_property dataproc.components.activate
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ tr '[:upper:]' '[:lower:]'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + activated_components='knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3 == *kerberos* ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + HBASE_CONF_DIR=/etc/hbase/conf
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + SPARK_CONF_DIR=/etc/spark/conf
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + is_rm_image
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\r\a\y ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + export_hcfs_root_uri
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ get_property_in_xml /tmp/cluster/properties/core.xml fs.defaultFS
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + HCFS_ROOT_URI=
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ -z '' ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + is_component_selected hdfs
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local -r component=hdfs
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local activated_components
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ get_components_to_activate
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ get_dataproc_property dataproc.components.activate
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ tr '[:upper:]' '[:lower:]'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + activated_components='knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3 == *hdfs* ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + (( 1 > 1 ))
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + HCFS_ROOT_URI=hdfs://cluster-dip-01-m
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + readonly HCFS_ROOT_URI
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + export HCFS_ROOT_URI
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ Master == \M\a\s\t\e\r ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + hostname=cluster-dip-01-m
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + is_component_selected kerberos
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local -r component=kerberos
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local activated_components
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ get_components_to_activate
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ get_dataproc_property dataproc.components.activate
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ tr '[:upper:]' '[:lower:]'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + activated_components='knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3 == *kerberos* ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + for i in "${!MASTER_HOSTNAMES[@]}"
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ cluster-dip-01-m == \c\l\u\s\t\e\r\-\d\i\p\-\0\1\-\m ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + export MASTER_INDEX=0
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + MASTER_INDEX=0
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + break
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + (( 3 == 0 ))
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + (( 1 > 1 ))
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + ARTIFACTS_TO_UNINSTALL=(${DATAPROC_MASTER_HA_SERVICES} ${DATAPROC_WORKER_SERVICES})
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + SERVICES=(${DATAPROC_MASTER_SERVICES} ${DATAPROC_MASTER_EXCLUSIVE_SERVICES} ${DATAPROC_MASTER_STANDALONE_SERVICES})
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + is_ubuntu
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ os_id
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ grep '^ID=' /etc/os-release
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ cut -d= -f2
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ xargs
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ debian == \u\b\u\n\t\u ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + loginfo 'Generating helper scripts'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + echo 'Generating helper scripts'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: Generating helper scripts
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + is_rm_image
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\r\a\y ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + cat
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ (( i = 0 ))
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ (( i < 1 ))
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ echo MASTER_HOSTNAME_0=cluster-dip-01-m
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ (( i++ ))
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ (( i < 1 ))
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + cat
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ standard == \s\t\a\n\d\a\r\d ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + cat
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ cat /usr/local/share/google/dataproc/bdutil/configure_keys.sh /usr/local/share/google/dataproc/bdutil/configure_hadoop.sh /usr/local/share/google/dataproc/bdutil/configure_connectors.sh /usr/local/share/google/dataproc/bdutil/configure_docker.sh /usr/local/share/google/dataproc/bdutil/configure_metadata_proxy.sh
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + cp -r /usr/local/share/google/dataproc/bdutil/conf/bq-mapred-template.xml /usr/local/share/google/dataproc/bdutil/conf/capacity-scheduler-template.xml /usr/local/share/google/dataproc/bdutil/conf/collectd /usr/local/share/google/dataproc/bdutil/conf/collectd_default_filtered_write.conf /usr/local/share/google/dataproc/bdutil/conf/collectd_flink_statsd_metrics.conf /usr/local/share/google/dataproc/bdutil/conf/collectd_hdfs_jmx_metrics.conf /usr/local/share/google/dataproc/bdutil/conf/collectd_hivemetastore_jmx_metrics.conf /usr/local/share/google/dataproc/bdutil/conf/collectd_hiveserver2_jmx_metrics.conf /usr/local/share/google/dataproc/bdutil/conf/collectd_load_jmx_plugin.conf /usr/local/share/google/dataproc/bdutil/conf/collectd_processes_default_metrics.conf /usr/local/share/google/dataproc/bdutil/conf/collectd_shs_jmx_metrics.conf /usr/local/share/google/dataproc/bdutil/conf/collectd_spark_buffer_metrics.conf /usr/local/share/google/dataproc/bdutil/conf/collectd_spark_default_metrics.conf /usr/local/shar
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: e/google/dataproc/bdutil/conf/collectd_spark_yarn_metrics.conf /usr/local/share/google/dataproc/bdutil/conf/collectd_without_monitoring_agent_defaults.conf /usr/local/share/google/dataproc/bdutil/conf/collectd_yarn_jmx_metrics.conf /usr/local/share/google/dataproc/bdutil/conf/core-ha-template.xml /usr/local/share/google/dataproc/bdutil/conf/core-template.xml /usr/local/share/google/dataproc/bdutil/conf/distcp-template.xml /usr/local/share/google/dataproc/bdutil/conf/gcs-core-template.xml /usr/local/share/google/dataproc/bdutil/conf/hdfs-ha-template.xml /usr/local/share/google/dataproc/bdutil/conf/hdfs-simplification-ha-mixins.xml /usr/local/share/google/dataproc/bdutil/conf/hdfs-simplification-mixins.xml /usr/local/share/google/dataproc/bdutil/conf/hdfs-template.xml /usr/local/share/google/dataproc/bdutil/conf/hive-ha-mixins.xml /usr/local/share/google/dataproc/bdutil/conf/hive-template.xml /usr/local/share/google/dataproc/bdutil/conf/knox /usr/local/share/google/dataproc/bdutil/conf/mapred-template.xml /usr/
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: local/share/google/dataproc/bdutil/conf/otel /usr/local/share/google/dataproc/bdutil/conf/otel_gcs_connector_metrics.yaml /usr/local/share/google/dataproc/bdutil/conf/otel_spark_default_metrics.yaml /usr/local/share/google/dataproc/bdutil/conf/yarn-ha-template.xml /usr/local/share/google/dataproc/bdutil/conf/yarn-simplification-ha-mixins.xml /usr/local/share/google/dataproc/bdutil/conf/yarn-simplification-mixins.xml /usr/local/share/google/dataproc/bdutil/conf/yarn-template.xml /tmp
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + cp /usr/local/share/google/dataproc/bdutil/configure_mrv2_mem.py /tmp
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + chmod +x configure_mrv2_mem.py
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + loginfo 'Running helper scripts'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + echo 'Running helper scripts'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: Running helper scripts
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ get_dataproc_property dataproc.localssd.mount.enable
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + MOUNT_DISKS_ENABLED=
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ '' == \f\a\l\s\e ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + DATAPROC_MOUNT_SERVICE_FILE=/usr/lib/systemd/system/google-dataproc-disk-mount.service
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + cat
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + chmod +x /usr/local/share/google/dataproc/bdutil/mount_disks.sh
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + chmod 644 /usr/lib/systemd/system/google-dataproc-disk-mount.service
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + enable_and_start_service google-dataproc-disk-mount
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local -r service=google-dataproc-disk-mount
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + enable_service google-dataproc-disk-mount
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local -r service=google-dataproc-disk-mount
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local -r unit=google-dataproc-disk-mount.service
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + retry_constant_short systemctl enable google-dataproc-disk-mount.service
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + retry_constant_custom 30 1 systemctl enable google-dataproc-disk-mount.service
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local -r max_retry_time=30
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local -r retry_delay=1
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + cmd=('systemctl' 'enable' 'google-dataproc-disk-mount.service')
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local -r cmd
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local -r max_retries=30
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local reenable_x=false
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ -o xtrace ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + set +x
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: About to run 'systemctl enable google-dataproc-disk-mount.service' with retries...
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: Created symlink /etc/systemd/system/multi-user.target.wants/google-dataproc-disk-mount.service → /lib/systemd/system/google-dataproc-disk-mount.service.
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: Created symlink /etc/systemd/system/hadoop-hdfs-namenode.service.wants/google-dataproc-disk-mount.service → /lib/systemd/system/google-dataproc-disk-mount.service.
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: Created symlink /etc/systemd/system/hadoop-hdfs-datanode.service.wants/google-dataproc-disk-mount.service → /lib/systemd/system/google-dataproc-disk-mount.service.
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: Created symlink /etc/systemd/system/hadoop-yarn-resourcemanager.service.wants/google-dataproc-disk-mount.service → /lib/systemd/system/google-dataproc-disk-mount.service.
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: Created symlink /etc/systemd/system/hadoop-yarn-nodemanager.service.wants/google-dataproc-disk-mount.service → /lib/systemd/system/google-dataproc-disk-mount.service.
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: 'systemctl enable google-dataproc-disk-mount.service' succeeded after 1 execution(s).
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + return 0
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local -r drop_in_dir=/etc/systemd/system/google-dataproc-disk-mount.service.d
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + mkdir -p /etc/systemd/system/google-dataproc-disk-mount.service.d
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local props
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ retry_constant_short systemctl show google-dataproc-disk-mount.service -p Restart,RemainAfterExit
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ retry_constant_custom 30 1 systemctl show google-dataproc-disk-mount.service -p Restart,RemainAfterExit
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ local -r max_retry_time=30
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ local -r retry_delay=1
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ cmd=('systemctl' 'show' 'google-dataproc-disk-mount.service' '-p' 'Restart,RemainAfterExit')
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ local -r cmd
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ local -r max_retries=30
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ local reenable_x=false
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ [[ -o xtrace ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: About to run 'systemctl show google-dataproc-disk-mount.service -p Restart,RemainAfterExit' with retries...
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: 'systemctl show google-dataproc-disk-mount.service -p Restart,RemainAfterExit' succeeded after 1 execution(s).
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ return 0
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + props='Restart=no
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: RemainAfterExit=yes'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ google-dataproc-disk-mount != \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ google-dataproc-disk-mount != \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ Restart=no
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: RemainAfterExit=yes == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ Restart=no
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: RemainAfterExit=yes == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local -r gate_start_on_agent_success_drop_in=/etc/systemd/system/common/agent-gate.conf
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ ! -f /etc/systemd/system/common/agent-gate.conf ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ dirname /etc/systemd/system/common/agent-gate.conf
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + mkdir -p /etc/systemd/system/common
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + cat
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ google-dataproc-disk-mount == \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ google-dataproc-disk-mount == \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + start_service google-dataproc-disk-mount
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + start_services google-dataproc-disk-mount
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + units=('google-dataproc-disk-mount.service')
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local -r units
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + retry_constant_short systemctl start google-dataproc-disk-mount.service
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + retry_constant_custom 30 1 systemctl start google-dataproc-disk-mount.service
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local -r max_retry_time=30
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local -r retry_delay=1
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + cmd=('systemctl' 'start' 'google-dataproc-disk-mount.service')
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local -r cmd
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local -r max_retries=30
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local reenable_x=false
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ -o xtrace ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + set +x
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: About to run 'systemctl start google-dataproc-disk-mount.service' with retries...
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: 'systemctl start google-dataproc-disk-mount.service' succeeded after 1 execution(s).
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + return 0
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + bash -e configuration_script.sh
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + readonly METADATA_HOST=metadata.google.internal
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + METADATA_HOST=metadata.google.internal
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + readonly INSTANCE_METADATA_PATH=computeMetadata/v1/instance
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + INSTANCE_METADATA_PATH=computeMetadata/v1/instance
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + readonly IDENTITY_KEY=service-accounts/default/identity
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + IDENTITY_KEY=service-accounts/default/identity
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + readonly GUEST_ATTRIBUTE_PATH=computeMetadata/v1/instance/guest-attributes/dataproc-signed-keys
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + GUEST_ATTRIBUTE_PATH=computeMetadata/v1/instance/guest-attributes/dataproc-signed-keys
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + readonly SERIAL_DEVICE=/dev/ttyS3
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + SERIAL_DEVICE=/dev/ttyS3
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + readonly GENERATE_KEYS_SCRIPT=/usr/bin/google-dataproc-generate-cluster-keys.sh
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + GENERATE_KEYS_SCRIPT=/usr/bin/google-dataproc-generate-cluster-keys.sh
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + readonly TINKEY_BINARY_PATH=/usr/local/bin/tinkey
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + TINKEY_BINARY_PATH=/usr/local/bin/tinkey
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ get_dataproc_property dataproc.encryption.keygen.enabled
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + cluster_keys_enabled=
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ get_dataproc_property_or_default dataproc.encryption.keygen.rotation_hours 6
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + cluster_keys_rotation_hours=6
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + cat
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + chmod u+rwx /usr/bin/google-dataproc-generate-cluster-keys.sh
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + cat
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + cat
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ '' == \t\r\u\e ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + rm /etc/udev/rules.d/80-ttyS3.rules
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + udevadm trigger
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + chown root:dialout /dev/ttyS3
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + set -e
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + loginfo 'Running configure_hadoop.sh'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + echo 'Running configure_hadoop.sh'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: Running configure_hadoop.sh
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + readonly HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + readonly NODEMANAGER_MEMORY_FRACTION=0.8
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + NODEMANAGER_MEMORY_FRACTION=0.8
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + readonly CORES_PER_MAP_TASK=1.0
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + CORES_PER_MAP_TASK=1.0
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + readonly CORES_PER_REDUCE_TASK=2.0
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + CORES_PER_REDUCE_TASK=2.0
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + readonly CORES_PER_APP_MASTER=2.0
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + CORES_PER_APP_MASTER=2.0
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + export HADOOP_TMP_DIR=/hadoop/tmp
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + HADOOP_TMP_DIR=/hadoop/tmp
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + mkdir -p /hadoop/tmp
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + export DEFAULT_NUM_MAPS=30
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + DEFAULT_NUM_MAPS=30
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + export DEFAULT_NUM_REDUCES=12
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + DEFAULT_NUM_REDUCES=12
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ grep -c processor /proc/cpuinfo
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + NUM_CORES=2
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + export NUM_CORES
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ python -c 'print(int(2 // 1.0))'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + MAP_SLOTS=2
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + export MAP_SLOTS
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ python -c 'print(int(2 // 2.0))'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + REDUCE_SLOTS=1
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + export REDUCE_SLOTS
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ awk '/^Mem:/{print $2}'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ free -m
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + TOTAL_MEM_MB=7949
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ python -c 'print(int(7949 * 0.4))'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + HADOOP_MR_MASTER_MEM_MB=3179
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ -x configure_mrv2_mem.py ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ mktemp /tmp/mrv2_XXX_tmp_env.sh
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + TEMP_ENV_FILE=/tmp/mrv2_lBR_tmp_env.sh
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + ./configure_mrv2_mem.py --output_file /tmp/mrv2_lBR_tmp_env.sh --total_memory 7949 --available_memory_ratio 0.8 --total_cores 2 --cores_per_map 1.0 --cores_per_reduce 2.0 --cores_per_app_master 2.0
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + source /tmp/mrv2_lBR_tmp_env.sh
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ export YARN_MIN_MEM_MB=512
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ YARN_MIN_MEM_MB=512
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ export YARN_MAX_MEM_MB=6144
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ YARN_MAX_MEM_MB=6144
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ export NODEMANAGER_MEM_MB=6144
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ NODEMANAGER_MEM_MB=6144
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ export APP_MASTER_MEM_MB=6144
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ APP_MASTER_MEM_MB=6144
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ export CORES_PER_APP_MASTER_ROUNDED=2
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ CORES_PER_APP_MASTER_ROUNDED=2
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ export APP_MASTER_JAVA_OPTS=-Xmx4915m
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ APP_MASTER_JAVA_OPTS=-Xmx4915m
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ export MAP_MEM_MB=3072
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ MAP_MEM_MB=3072
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ export CORES_PER_MAP_ROUNDED=1
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ CORES_PER_MAP_ROUNDED=1
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ export MAP_JAVA_OPTS=-Xmx2457m
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ MAP_JAVA_OPTS=-Xmx2457m
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ export REDUCE_MEM_MB=6144
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ REDUCE_MEM_MB=6144
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ export CORES_PER_REDUCE_ROUNDED=2
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ CORES_PER_REDUCE_ROUNDED=2
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ export REDUCE_JAVA_OPTS=-Xmx4915m
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ REDUCE_JAVA_OPTS=-Xmx4915m
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ python -c 'print(min(32 * 1024, int(7949 / 4)))'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + HADOOP_CLIENT_MEM_MB=1987
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + cat
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + is_version_at_least 2.2 2.1
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local reenable_x=false
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ -o xtrace ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + set +x
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + case ${compare_versions_result} in
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + return 0
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + GC_LOG_OPTS='-Xlog:gc*:stdout:time,level,tags'
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + readonly CLUSTER_PROPS_DIR=/tmp/cluster/properties
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + CLUSTER_PROPS_DIR=/tmp/cluster/properties
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ grep GC_OPTS /tmp/cluster/properties/yarn-env.sh
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ cut -d= -f2
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ head -n1
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + USER_SUPPLIED_GC_OPTS_YARN=
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ -z '' ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + is_version_at_least 2.2 3.0
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local reenable_x=false
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ -o xtrace ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + set +x
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + case ${compare_versions_result} in
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + return 1
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + GC_OPTS=-XX:+UseConcMarkSweepGC
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + GC_JDK17_OPTS=
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + is_version_at_least 2.2 3.0
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local reenable_x=false
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ -o xtrace ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + set +x
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + case ${compare_versions_result} in
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + return 1
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + cat
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ head -n1
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ cut -d= -f2
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: ++ grep GC_OPTS /tmp/cluster/properties/mapred-env.sh
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + USER_SUPPLIED_GC_OPTS_MAPRED=
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ -z '' ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + is_version_at_least 2.2 3.0
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + local reenable_x=false
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + [[ -o xtrace ]]
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + set +x
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + case ${compare_versions_result} in
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + return 1
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + GC_OPTS=-XX:+UseConcMarkSweepGC
<13>Apr 27 18:07:23 dataproc-startup-script[1265]: + cat
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + DATA_DIRS_ARRAY=($(get_data_dirs))
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: ++ get_data_dirs
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: ++ local -a mount_points
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: ++ mapfile -t mount_points
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: +++ find '/mnt/[0-9]*/' -maxdepth 0
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: find: ‘/mnt/[0-9]*/’: No such file or directory
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: +++ true
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: ++ (( 0 ))
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: ++ echo /
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: ++ return
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + MAPRED_DIRS=/hadoop/mapred
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + MAPRED_DIRS_ARRAY=(${MAPRED_DIRS})
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + MAPRED_LOCAL_DIRS=/hadoop/mapred/local
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + MAPRED_LOCAL_DIRS_ARRAY=(${MAPRED_LOCAL_DIRS})
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + YARN_DIRS=/hadoop/yarn
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + YARN_DIRS_ARRAY=(${YARN_DIRS})
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: ++ get_yarn_nm_local_dirs
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: +++ get_data_dirs
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: +++ local -a mount_points
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: +++ mapfile -t mount_points
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: ++++ find '/mnt/[0-9]*/' -maxdepth 0
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: find: ‘/mnt/[0-9]*/’: No such file or directory
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: ++++ true
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: +++ (( 0 ))
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: +++ echo /
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: +++ return
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: ++ data_dirs=('/')
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: ++ local -r data_dirs
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: ++ echo /hadoop/yarn/nm-local-dir
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + NODEMANAGER_LOCAL_DIRS=/hadoop/yarn/nm-local-dir
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + NODEMANAGER_LOCAL_DIRS_ARRAY=(${NODEMANAGER_LOCAL_DIRS})
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + mkdir -p /hadoop/mapred/local /hadoop/yarn/nm-local-dir
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + chgrp hadoop -L -R /hadoop /hadoop/tmp
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + chown -L -R mapred:hadoop /hadoop/mapred
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + chown -L -R yarn:hadoop /hadoop/yarn
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + chmod g+rwx -R /hadoop /hadoop/mapred /hadoop/yarn
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + chmod 777 -R /hadoop/tmp
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + export MAPRED_LOCAL_DIRS=/hadoop/mapred/local
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + MAPRED_LOCAL_DIRS=/hadoop/mapred/local
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + export NODEMANAGER_LOCAL_DIRS=/hadoop/yarn/nm-local-dir
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + NODEMANAGER_LOCAL_DIRS=/hadoop/yarn/nm-local-dir
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + YARN_ENV_FILE=/etc/hadoop/conf/yarn-env.sh
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + [[ -f /etc/hadoop/conf/yarn-env.sh ]]
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + cat
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + [[ 1 -gt 1 ]]
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + readonly CORE_TEMPLATE=core-template.xml
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + CORE_TEMPLATE=core-template.xml
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + readonly YARN_TEMPLATE=yarn-template.xml
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + YARN_TEMPLATE=yarn-template.xml
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + merge_hadoop_configurations core-site.xml core-template.xml
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + local -r config=core-site.xml
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + local -r template=core-template.xml
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/core-site.xml --source_configuration_file core-template.xml --resolve_environment_variables --create_if_absent --clobber
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + merge_hadoop_configurations mapred-site.xml mapred-template.xml
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + local -r config=mapred-site.xml
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + local -r template=mapred-template.xml
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/mapred-site.xml --source_configuration_file mapred-template.xml --resolve_environment_variables --create_if_absent --clobber
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + merge_hadoop_configurations yarn-site.xml yarn-template.xml
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + local -r config=yarn-site.xml
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + local -r template=yarn-template.xml
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/yarn-site.xml --source_configuration_file yarn-template.xml --resolve_environment_variables --create_if_absent --clobber
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + merge_hadoop_configurations capacity-scheduler.xml capacity-scheduler-template.xml
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + local -r config=capacity-scheduler.xml
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + local -r template=capacity-scheduler-template.xml
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/capacity-scheduler.xml --source_configuration_file capacity-scheduler-template.xml --resolve_environment_variables --create_if_absent --clobber
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + merge_hadoop_configurations distcp-default.xml distcp-template.xml
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + local -r config=distcp-default.xml
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + local -r template=distcp-template.xml
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/distcp-default.xml --source_configuration_file distcp-template.xml --resolve_environment_variables --create_if_absent --clobber
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + set_hadoop_property mapred-site.xml mapreduce.application.classpath '$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*,
<13>Apr 27 18:07:24 dataproc-startup-script[1265]:     $HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*,
<13>Apr 27 18:07:24 dataproc-startup-script[1265]:     /usr/local/share/google/dataproc/lib/*'
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + local -r config=mapred-site.xml
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + local -r property_name=mapreduce.application.classpath
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + local -r 'property_value=$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*,
<13>Apr 27 18:07:24 dataproc-startup-script[1265]:     $HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*,
<13>Apr 27 18:07:24 dataproc-startup-script[1265]:     /usr/local/share/google/dataproc/lib/*'
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + bdconfig set_property --configuration_file /etc/hadoop/conf/mapred-site.xml --name mapreduce.application.classpath --value '$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*,
<13>Apr 27 18:07:24 dataproc-startup-script[1265]:     $HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*,
<13>Apr 27 18:07:24 dataproc-startup-script[1265]:     /usr/local/share/google/dataproc/lib/*' --clobber
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + set_hadoop_property mapred-site.xml mapreduce.client.submit.file.replication 2
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + local -r config=mapred-site.xml
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + local -r property_name=mapreduce.client.submit.file.replication
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + local -r property_value=2
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + bdconfig set_property --configuration_file /etc/hadoop/conf/mapred-site.xml --name mapreduce.client.submit.file.replication --value 2 --clobber
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + set_hadoop_property yarn-site.xml yarn.application.classpath '$HADOOP_CONF_DIR,$HADOOP_COMMON_HOME/*,$HADOOP_COMMON_HOME/lib/*,
<13>Apr 27 18:07:24 dataproc-startup-script[1265]:     $HADOOP_HDFS_HOME/*,$HADOOP_HDFS_HOME/lib/*,$HADOOP_MAPRED_HOME/*,
<13>Apr 27 18:07:24 dataproc-startup-script[1265]:     $HADOOP_MAPRED_HOME/lib/*,$HADOOP_YARN_HOME/*,$HADOOP_YARN_HOME/lib/*,
<13>Apr 27 18:07:24 dataproc-startup-script[1265]:     /usr/local/share/google/dataproc/lib/*'
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + local -r config=yarn-site.xml
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + local -r property_name=yarn.application.classpath
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + local -r 'property_value=$HADOOP_CONF_DIR,$HADOOP_COMMON_HOME/*,$HADOOP_COMMON_HOME/lib/*,
<13>Apr 27 18:07:24 dataproc-startup-script[1265]:     $HADOOP_HDFS_HOME/*,$HADOOP_HDFS_HOME/lib/*,$HADOOP_MAPRED_HOME/*,
<13>Apr 27 18:07:24 dataproc-startup-script[1265]:     $HADOOP_MAPRED_HOME/lib/*,$HADOOP_YARN_HOME/*,$HADOOP_YARN_HOME/lib/*,
<13>Apr 27 18:07:24 dataproc-startup-script[1265]:     /usr/local/share/google/dataproc/lib/*'
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.application.classpath --value '$HADOOP_CONF_DIR,$HADOOP_COMMON_HOME/*,$HADOOP_COMMON_HOME/lib/*,
<13>Apr 27 18:07:24 dataproc-startup-script[1265]:     $HADOOP_HDFS_HOME/*,$HADOOP_HDFS_HOME/lib/*,$HADOOP_MAPRED_HOME/*,
<13>Apr 27 18:07:24 dataproc-startup-script[1265]:     $HADOOP_MAPRED_HOME/lib/*,$HADOOP_YARN_HOME/*,$HADOOP_YARN_HOME/lib/*,
<13>Apr 27 18:07:24 dataproc-startup-script[1265]:     /usr/local/share/google/dataproc/lib/*' --clobber
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + is_version_at_least 2.2 2.0
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + local reenable_x=false
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + [[ -o xtrace ]]
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + set +x
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + case ${compare_versions_result} in
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + return 0
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + cat
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + set_hadoop_property yarn-site.xml yarn.nodemanager.env-whitelist PATH,JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME,LD_LIBRARY_PATH,LANG,TZ
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + local -r config=yarn-site.xml
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + local -r property_name=yarn.nodemanager.env-whitelist
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + local -r property_value=PATH,JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME,LD_LIBRARY_PATH,LANG,TZ
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.nodemanager.env-whitelist --value PATH,JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME,LD_LIBRARY_PATH,LANG,TZ --clobber
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + set_hadoop_property yarn-site.xml yarn.nm.liveness-monitor.expiry-interval-ms 120000
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + local -r config=yarn-site.xml
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + local -r property_name=yarn.nm.liveness-monitor.expiry-interval-ms
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + local -r property_value=120000
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.nm.liveness-monitor.expiry-interval-ms --value 120000 --clobber
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + set_hadoop_property yarn-site.xml yarn.log-aggregation-enable false
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + local -r config=yarn-site.xml
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + local -r property_name=yarn.log-aggregation-enable
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + local -r property_value=false
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.log-aggregation-enable --value false --clobber
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + ZK_QUORUM=cluster-dip-01-m:2181,:2181,:2181
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + [[ 1 -gt 1 ]]
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + set_hadoop_property hdfs-site.xml dfs.namenode.file.close.num-committed-allowed 1
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + local -r config=hdfs-site.xml
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + local -r property_name=dfs.namenode.file.close.num-committed-allowed
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + local -r property_value=1
<13>Apr 27 18:07:24 dataproc-startup-script[1265]: + bdconfig set_property --configuration_file /etc/hadoop/conf/hdfs-site.xml --name dfs.namenode.file.close.num-committed-allowed --value 1 --clobber
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: + set_hadoop_property core-site.xml hadoop.http.filter.initializers org.apache.hadoop.security.HttpCrossOriginFilterInitializer,org.apache.hadoop.http.lib.StaticUserWebFilter
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: + local -r config=core-site.xml
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: + local -r property_name=hadoop.http.filter.initializers
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: + local -r property_value=org.apache.hadoop.security.HttpCrossOriginFilterInitializer,org.apache.hadoop.http.lib.StaticUserWebFilter
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: + bdconfig set_property --configuration_file /etc/hadoop/conf/core-site.xml --name hadoop.http.filter.initializers --value org.apache.hadoop.security.HttpCrossOriginFilterInitializer,org.apache.hadoop.http.lib.StaticUserWebFilter --clobber
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: + set_hadoop_property yarn-site.xml yarn.resourcemanager.webapp.cross-origin.enabled true
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: + local -r config=yarn-site.xml
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: + local -r property_name=yarn.resourcemanager.webapp.cross-origin.enabled
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: + local -r property_value=true
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.resourcemanager.webapp.cross-origin.enabled --value true --clobber
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: + set_hadoop_property yarn-site.xml yarn.timeline-service.http-cross-origin.enabled true
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: + local -r config=yarn-site.xml
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: + local -r property_name=yarn.timeline-service.http-cross-origin.enabled
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: + local -r property_value=true
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.http-cross-origin.enabled --value true --clobber
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: + set_hadoop_property yarn-site.xml yarn.timeline-service.enabled true
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: + local -r config=yarn-site.xml
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: + local -r property_name=yarn.timeline-service.enabled
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: + local -r property_value=true
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.enabled --value true --clobber
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: + set_hadoop_property yarn-site.xml yarn.timeline-service.hostname cluster-dip-01-m
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: + local -r config=yarn-site.xml
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: + local -r property_name=yarn.timeline-service.hostname
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: + local -r property_value=cluster-dip-01-m
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.hostname --value cluster-dip-01-m --clobber
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: + set_hadoop_property yarn-site.xml yarn.timeline-service.bind-host 0.0.0.0
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: + local -r config=yarn-site.xml
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: + local -r property_name=yarn.timeline-service.bind-host
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: + local -r property_value=0.0.0.0
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.bind-host --value 0.0.0.0 --clobber
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: + set_hadoop_property yarn-site.xml yarn.resourcemanager.system-metrics-publisher.enabled true
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: + local -r config=yarn-site.xml
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: + local -r property_name=yarn.resourcemanager.system-metrics-publisher.enabled
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: + local -r property_value=true
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.resourcemanager.system-metrics-publisher.enabled --value true --clobber
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: + set_hadoop_property yarn-site.xml yarn.timeline-service.generic-application-history.enabled true
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: + local -r config=yarn-site.xml
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: + local -r property_name=yarn.timeline-service.generic-application-history.enabled
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: + local -r property_value=true
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.generic-application-history.enabled --value true --clobber
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: + is_version_at_least 2.2 2.1
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: + local reenable_x=false
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: + [[ -o xtrace ]]
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: + set +x
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: + case ${compare_versions_result} in
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: + return 0
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: + set_hadoop_property core-site.xml hadoop.security.token.service.use_ip false
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: + local -r config=core-site.xml
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: + local -r property_name=hadoop.security.token.service.use_ip
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: + local -r property_value=false
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: + bdconfig set_property --configuration_file /etc/hadoop/conf/core-site.xml --name hadoop.security.token.service.use_ip --value false --clobber
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: ++ get_dataproc_property am.primary_only
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: + AM_ON_PRIMARY_WORKER_ENABLED=false
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: ++ get_metadata_datanode_enabled
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: ++ get_dataproc_metadata DATAPROC_METADATA_DATANODE_ENABLED attributes/dataproc-datanode-enabled
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: + DATAPROC_DATANODE_ENABLED=false
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: + [[ false == \t\r\u\e ]]
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: ++ create_or_validate_include_file_path
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: ++ local include_path
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: +++ get_include_file_path
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: +++ local include_path
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: ++++ /usr/share/google/get_metadata_value attributes/dataproc-include-file-location
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: +++ include_path=gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_include
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: +++ [[ -z gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_include ]]
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: +++ echo gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_include
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: ++ include_path=gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_include
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: ++ [[ gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_include == gs://* ]]
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: ++ loginfo 'Checking if include membership files exist in GCS: gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_include'
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: ++ echo 'Checking if include membership files exist in GCS: gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_include'
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: Checking if include membership files exist in GCS: gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_include
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: ++ retry_constant_custom 10 3 gcs_file_exists gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_include
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: ++ local -r max_retry_time=10
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: ++ local -r retry_delay=3
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: ++ cmd=('gcs_file_exists' 'gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_include')
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: ++ local -r cmd
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: ++ local -r max_retries=3
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: ++ local reenable_x=false
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: ++ [[ -o xtrace ]]
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:25 dataproc-startup-script[1265]: About to run 'gcs_file_exists gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_include' with retries...
<13>Apr 27 18:07:27 dataproc-startup-script[1265]: 'gcs_file_exists gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_include' succeeded after 1 execution(s).
<13>Apr 27 18:07:27 dataproc-startup-script[1265]: ++ return 0
<13>Apr 27 18:07:27 dataproc-startup-script[1265]: ++ echo gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_include
<13>Apr 27 18:07:27 dataproc-startup-script[1265]: + INCLUDE_PATH=gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_include
<13>Apr 27 18:07:27 dataproc-startup-script[1265]: ++ create_or_validate_exclude_file_path
<13>Apr 27 18:07:27 dataproc-startup-script[1265]: ++ local exclude_path
<13>Apr 27 18:07:27 dataproc-startup-script[1265]: +++ get_exclude_file_path
<13>Apr 27 18:07:27 dataproc-startup-script[1265]: +++ local exclude_path
<13>Apr 27 18:07:27 dataproc-startup-script[1265]: ++++ /usr/share/google/get_metadata_value attributes/dataproc-exclude-file-location
<13>Apr 27 18:07:27 dataproc-startup-script[1265]: +++ exclude_path=gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_exclude.xml
<13>Apr 27 18:07:27 dataproc-startup-script[1265]: +++ [[ -z gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_exclude.xml ]]
<13>Apr 27 18:07:27 dataproc-startup-script[1265]: +++ echo gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_exclude.xml
<13>Apr 27 18:07:27 dataproc-startup-script[1265]: ++ exclude_path=gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_exclude.xml
<13>Apr 27 18:07:27 dataproc-startup-script[1265]: ++ [[ gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_exclude.xml == gs://* ]]
<13>Apr 27 18:07:27 dataproc-startup-script[1265]: ++ loginfo 'Checking if exclude membership files exist in GCS: gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_exclude.xml'
<13>Apr 27 18:07:27 dataproc-startup-script[1265]: ++ echo 'Checking if exclude membership files exist in GCS: gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_exclude.xml'
<13>Apr 27 18:07:27 dataproc-startup-script[1265]: Checking if exclude membership files exist in GCS: gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_exclude.xml
<13>Apr 27 18:07:27 dataproc-startup-script[1265]: ++ retry_constant_custom 10 3 gcs_file_exists gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_exclude.xml
<13>Apr 27 18:07:27 dataproc-startup-script[1265]: ++ local -r max_retry_time=10
<13>Apr 27 18:07:27 dataproc-startup-script[1265]: ++ local -r retry_delay=3
<13>Apr 27 18:07:27 dataproc-startup-script[1265]: ++ cmd=('gcs_file_exists' 'gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_exclude.xml')
<13>Apr 27 18:07:27 dataproc-startup-script[1265]: ++ local -r cmd
<13>Apr 27 18:07:27 dataproc-startup-script[1265]: ++ local -r max_retries=3
<13>Apr 27 18:07:27 dataproc-startup-script[1265]: ++ local reenable_x=false
<13>Apr 27 18:07:27 dataproc-startup-script[1265]: ++ [[ -o xtrace ]]
<13>Apr 27 18:07:27 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:27 dataproc-startup-script[1265]: About to run 'gcs_file_exists gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_exclude.xml' with retries...
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: 'gcs_file_exists gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_exclude.xml' succeeded after 1 execution(s).
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: ++ return 0
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: ++ echo gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_exclude.xml
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + EXCLUDE_PATH=gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_exclude.xml
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + set_hadoop_property yarn-site.xml yarn.resourcemanager.nodes.include-path gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_include
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + local -r config=yarn-site.xml
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + local -r property_name=yarn.resourcemanager.nodes.include-path
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + local -r property_value=gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_include
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.resourcemanager.nodes.include-path --value gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_include --clobber
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + set_hadoop_property yarn-site.xml yarn.resourcemanager.nodes.exclude-path gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_exclude.xml
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + local -r config=yarn-site.xml
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + local -r property_name=yarn.resourcemanager.nodes.exclude-path
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + local -r property_value=gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_exclude.xml
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.resourcemanager.nodes.exclude-path --value gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_exclude.xml --clobber
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: ++ /usr/share/google/get_metadata_value attributes/master-run-driver-location
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + MASTER_RUN_DRIVER_LOCATION=LOCAL
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + [[ LOCAL == \Y\A\R\N ]]
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + [[ 1 -gt 1 ]]
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + readonly YARN_SIMPLIFICATION_MIXINS=yarn-simplification-mixins.xml
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + YARN_SIMPLIFICATION_MIXINS=yarn-simplification-mixins.xml
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + merge_hadoop_configurations yarn-site.xml /usr/local/share/google/dataproc/bdutil/conf/yarn-simplification-mixins.xml
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + local -r config=yarn-site.xml
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + local -r template=/usr/local/share/google/dataproc/bdutil/conf/yarn-simplification-mixins.xml
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/yarn-site.xml --source_configuration_file /usr/local/share/google/dataproc/bdutil/conf/yarn-simplification-mixins.xml --resolve_environment_variables --create_if_absent --clobber
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + is_component_selected kerberos
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + local -r component=kerberos
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + local activated_components
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: ++ get_components_to_activate
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: ++ get_dataproc_property dataproc.components.activate
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: ++ tr '[:upper:]' '[:lower:]'
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + activated_components='knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3'
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + [[ knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3 == *kerberos* ]]
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: ++ get_dataproc_property yarn.docker.enable
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + ENABLE_DOCKER_YARN=
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + [[ -z '' ]]
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: ++ get_dataproc_property docker.yarn.enable
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + ENABLE_DOCKER_YARN=
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + is_version_at_least 2.2 2.0
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + local reenable_x=false
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + [[ -o xtrace ]]
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + set +x
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + case ${compare_versions_result} in
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + return 0
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + is_component_selected docker-ce
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + local -r component=docker-ce
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + local activated_components
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: ++ get_components_to_activate
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: ++ get_dataproc_property dataproc.components.activate
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: ++ tr '[:upper:]' '[:lower:]'
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + activated_components='knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3'
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + [[ knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3 == *docker-ce* ]]
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + set -e
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + loginfo 'Running configure_connectors.sh'
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + echo 'Running configure_connectors.sh'
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: Running configure_connectors.sh
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/core-site.xml --source_configuration_file gcs-core-template.xml --resolve_environment_variables --create_if_absent --noclobber
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/mapred-site.xml --source_configuration_file bq-mapred-template.xml --resolve_environment_variables --create_if_absent --noclobber
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + set -euxo pipefail
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + source /usr/local/share/google/dataproc/bdutil/components/shared/docker.sh
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: ++ readonly DOCKER_PATH=/var/lib/docker
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: ++ DOCKER_PATH=/var/lib/docker
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: ++ readonly GCR_CREDENTIAL_HELPER_VERSION=2.0.0
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: ++ GCR_CREDENTIAL_HELPER_VERSION=2.0.0
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + is_component_selected docker-ce
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + local -r component=docker-ce
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + local activated_components
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: ++ get_components_to_activate
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: ++ get_dataproc_property dataproc.components.activate
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: ++ tr '[:upper:]' '[:lower:]'
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + activated_components='knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3'
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + [[ knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3 == *docker-ce* ]]
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + readonly RUN_SCRIPT_PATH=/usr/local/share/google/dataproc/metadata-proxy.sh
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + RUN_SCRIPT_PATH=/usr/local/share/google/dataproc/metadata-proxy.sh
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + readonly PROXY_SERVICE_CONF=/usr/lib/systemd/system/metadata-proxy.service
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + PROXY_SERVICE_CONF=/usr/lib/systemd/system/metadata-proxy.service
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + readonly METADATA_ADDRESS=169.254.169.254
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + METADATA_ADDRESS=169.254.169.254
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + readonly METADATA_PORT=80
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + METADATA_PORT=80
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + readonly METADATA_PASSTHROUGH_PORT=987
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + METADATA_PASSTHROUGH_PORT=987
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + readonly METADATA_PROXY_PORT=988
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + METADATA_PROXY_PORT=988
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + is_rm_image
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\r\a\y ]]
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + readonly POLLING_SCRIPT_PATH=/usr/local/share/google/dataproc/poll-metadata.sh
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + POLLING_SCRIPT_PATH=/usr/local/share/google/dataproc/poll-metadata.sh
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + readonly POLLING_SERVICE_CONF=/usr/lib/systemd/system/metadata-credentials-polling.service
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + POLLING_SERVICE_CONF=/usr/lib/systemd/system/metadata-credentials-polling.service
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + readonly 'TOKEN_SOURCE_METADATA=TOKEN FROM METADATA ATTRIBUTES'
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + TOKEN_SOURCE_METADATA='TOKEN FROM METADATA ATTRIBUTES'
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + readonly 'TOKEN_SOURCE_GCS=TOKEN FROM GCS'
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + TOKEN_SOURCE_GCS='TOKEN FROM GCS'
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: ++ /usr/share/google/get_metadata_value attributes/dataproc-bucket
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + CONFIGBUCKET=dataproc-staging-us-east1-679657336577-kutp8kah
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + readonly CONFIGBUCKET
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: ++ /usr/share/google/get_metadata_value attributes/dataproc-cluster-uuid
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + CLUSTER_UUID=0b63cfa9-1f8a-481e-87c8-a0cf661f3625
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + readonly CLUSTER_UUID
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: ++ /usr/share/google/get_metadata_value id
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + INSTANCE_UUID=1742420956051393710
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + readonly INSTANCE_UUID
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + readonly TOKEN_METADATA_ATTRIBUTE=attributes/dataproc-injected-credentials
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + TOKEN_METADATA_ATTRIBUTE=attributes/dataproc-injected-credentials
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: ++ get_dataproc_property dataproc.metadata.proxy.enabled
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + METADATA_PROXY_ENABLED=
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + readonly METADATA_PROXY_ENABLED
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: ++ get_dataproc_property dataproc.exclusive.user
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + DATAPROC_EXCLUSIVE_USER=
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + readonly DATAPROC_EXCLUSIVE_USER
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: ++ get_dataproc_property dataproc.personal-auth.override-user-display-name
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + PERSONAL_AUTH_OVERRIDE_USER_DISPLAY_NAME=
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + readonly PERSONAL_AUTH_OVERRIDE_USER_DISPLAY_NAME
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: ++ get_dataproc_property internal.euc.user
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + SERVERLESS_EUC_USER=
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + readonly SERVERLESS_EUC_USER
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + [[ '' == \t\r\u\e ]]
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + [[ standard != \s\t\a\n\d\a\r\d ]]
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + is_component_selected yarn
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + local -r component=yarn
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + local activated_components
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: ++ get_components_to_activate
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: ++ get_dataproc_property dataproc.components.activate
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: ++ tr '[:upper:]' '[:lower:]'
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + activated_components='knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3'
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + [[ knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3 == *yarn* ]]
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: ++ get_dataproc_property_or_default yarn.log-aggregation.enabled false
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + LOG_AGGREGATION_ENABLED=true
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + readonly LOG_AGGREGATION_ENABLED
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + [[ true == \t\r\u\e ]]
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + loginfo 'Enabling YARN log aggregation.'
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + echo 'Enabling YARN log aggregation.'
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: Enabling YARN log aggregation.
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + set_property_yarn_site yarn.log-aggregation-enable true
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + local -r name=yarn.log-aggregation-enable
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + local -r value=true
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + set_property_in_xml /etc/hadoop/conf/yarn-site.xml yarn.log-aggregation-enable true
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + local -r xml_file=/etc/hadoop/conf/yarn-site.xml
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + local -r name=yarn.log-aggregation-enable
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + local -r value=true
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + local -r mode=overwrite
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + local -r delimiter=,
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + local skip=false
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + local old_value
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + local new_value
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + [[ overwrite == \o\v\e\r\w\r\i\t\e ]]
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + new_value=true
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + [[ false == \f\a\l\s\e ]]
<13>Apr 27 18:07:28 dataproc-startup-script[1265]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.log-aggregation-enable --value true --create_if_absent --clobber
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + is_component_selected kerberos
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local -r component=kerberos
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local activated_components
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: ++ get_components_to_activate
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: ++ get_dataproc_property dataproc.components.activate
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: ++ tr '[:upper:]' '[:lower:]'
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + activated_components='knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3'
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + [[ knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3 == *kerberos* ]]
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + readonly YARN_LOG_SERVER_SCHEMA=http
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + YARN_LOG_SERVER_SCHEMA=http
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + readonly YARN_LOG_SERVER_PORT=19888
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + YARN_LOG_SERVER_PORT=19888
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + readonly YARN_LOG_SERVER_URL=http://cluster-dip-01-m:19888/jobhistory/logs
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + YARN_LOG_SERVER_URL=http://cluster-dip-01-m:19888/jobhistory/logs
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + set_property_yarn_site yarn.log.server.url http://cluster-dip-01-m:19888/jobhistory/logs
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local -r name=yarn.log.server.url
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local -r value=http://cluster-dip-01-m:19888/jobhistory/logs
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + set_property_in_xml /etc/hadoop/conf/yarn-site.xml yarn.log.server.url http://cluster-dip-01-m:19888/jobhistory/logs
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local -r xml_file=/etc/hadoop/conf/yarn-site.xml
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local -r name=yarn.log.server.url
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local -r value=http://cluster-dip-01-m:19888/jobhistory/logs
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local -r mode=overwrite
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local -r delimiter=,
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local skip=false
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local old_value
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local new_value
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + [[ overwrite == \o\v\e\r\w\r\i\t\e ]]
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + new_value=http://cluster-dip-01-m:19888/jobhistory/logs
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + [[ false == \f\a\l\s\e ]]
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.log.server.url --value http://cluster-dip-01-m:19888/jobhistory/logs --create_if_absent --clobber
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + [[ -n dataproc-temp-us-east1-679657336577-juievcnm ]]
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + set_property_yarn_site yarn.nodemanager.remote-app-log-dir gs://dataproc-temp-us-east1-679657336577-juievcnm/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/yarn-logs
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local -r name=yarn.nodemanager.remote-app-log-dir
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local -r value=gs://dataproc-temp-us-east1-679657336577-juievcnm/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/yarn-logs
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + set_property_in_xml /etc/hadoop/conf/yarn-site.xml yarn.nodemanager.remote-app-log-dir gs://dataproc-temp-us-east1-679657336577-juievcnm/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/yarn-logs
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local -r xml_file=/etc/hadoop/conf/yarn-site.xml
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local -r name=yarn.nodemanager.remote-app-log-dir
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local -r value=gs://dataproc-temp-us-east1-679657336577-juievcnm/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/yarn-logs
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local -r mode=overwrite
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local -r delimiter=,
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local skip=false
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local old_value
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local new_value
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + [[ overwrite == \o\v\e\r\w\r\i\t\e ]]
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + new_value=gs://dataproc-temp-us-east1-679657336577-juievcnm/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/yarn-logs
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + [[ false == \f\a\l\s\e ]]
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.nodemanager.remote-app-log-dir --value gs://dataproc-temp-us-east1-679657336577-juievcnm/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/yarn-logs --create_if_absent --clobber
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + is_version_at_least 2.2 2.0
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local reenable_x=false
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + [[ -o xtrace ]]
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + set +x
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + case ${compare_versions_result} in
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + return 0
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + set_property_yarn_site yarn.log-aggregation.file-formats IFile,TFile
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local -r name=yarn.log-aggregation.file-formats
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local -r value=IFile,TFile
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + set_property_in_xml /etc/hadoop/conf/yarn-site.xml yarn.log-aggregation.file-formats IFile,TFile
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local -r xml_file=/etc/hadoop/conf/yarn-site.xml
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local -r name=yarn.log-aggregation.file-formats
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local -r value=IFile,TFile
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local -r mode=overwrite
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local -r delimiter=,
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local skip=false
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local old_value
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local new_value
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + [[ overwrite == \o\v\e\r\w\r\i\t\e ]]
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + new_value=IFile,TFile
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + [[ false == \f\a\l\s\e ]]
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.log-aggregation.file-formats --value IFile,TFile --create_if_absent --clobber
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + set_property_yarn_site yarn.log-aggregation.file-controller.IFile.class org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local -r name=yarn.log-aggregation.file-controller.IFile.class
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local -r value=org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + set_property_in_xml /etc/hadoop/conf/yarn-site.xml yarn.log-aggregation.file-controller.IFile.class org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local -r xml_file=/etc/hadoop/conf/yarn-site.xml
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local -r name=yarn.log-aggregation.file-controller.IFile.class
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local -r value=org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local -r mode=overwrite
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local -r delimiter=,
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local skip=false
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local old_value
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local new_value
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + [[ overwrite == \o\v\e\r\w\r\i\t\e ]]
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + new_value=org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + [[ false == \f\a\l\s\e ]]
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.log-aggregation.file-controller.IFile.class --value org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController --create_if_absent --clobber
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: ++ get_dataproc_property_or_default job.history.to-gcs.enabled false
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + PERSIST_HISTORY_TO_GCS=true
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + [[ true == \t\r\u\e ]]
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + loginfo 'Enabling persisting MapReduce job history files to GCS.'
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + echo 'Enabling persisting MapReduce job history files to GCS.'
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: Enabling persisting MapReduce job history files to GCS.
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + [[ -n dataproc-temp-us-east1-679657336577-juievcnm ]]
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + set_property_mapred_site mapreduce.jobhistory.done-dir gs://dataproc-temp-us-east1-679657336577-juievcnm/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/mapreduce-job-history/done
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local -r name=mapreduce.jobhistory.done-dir
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local -r value=gs://dataproc-temp-us-east1-679657336577-juievcnm/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/mapreduce-job-history/done
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + set_property_in_xml /etc/hadoop/conf/mapred-site.xml mapreduce.jobhistory.done-dir gs://dataproc-temp-us-east1-679657336577-juievcnm/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/mapreduce-job-history/done
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local -r xml_file=/etc/hadoop/conf/mapred-site.xml
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local -r name=mapreduce.jobhistory.done-dir
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local -r value=gs://dataproc-temp-us-east1-679657336577-juievcnm/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/mapreduce-job-history/done
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local -r mode=overwrite
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local -r delimiter=,
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local skip=false
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local old_value
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local new_value
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + [[ overwrite == \o\v\e\r\w\r\i\t\e ]]
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + new_value=gs://dataproc-temp-us-east1-679657336577-juievcnm/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/mapreduce-job-history/done
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + [[ false == \f\a\l\s\e ]]
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + bdconfig set_property --configuration_file /etc/hadoop/conf/mapred-site.xml --name mapreduce.jobhistory.done-dir --value gs://dataproc-temp-us-east1-679657336577-juievcnm/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/mapreduce-job-history/done --create_if_absent --clobber
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + set_property_mapred_site mapreduce.jobhistory.intermediate-done-dir gs://dataproc-temp-us-east1-679657336577-juievcnm/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/mapreduce-job-history/done_intermediate
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local -r name=mapreduce.jobhistory.intermediate-done-dir
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local -r value=gs://dataproc-temp-us-east1-679657336577-juievcnm/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/mapreduce-job-history/done_intermediate
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + set_property_in_xml /etc/hadoop/conf/mapred-site.xml mapreduce.jobhistory.intermediate-done-dir gs://dataproc-temp-us-east1-679657336577-juievcnm/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/mapreduce-job-history/done_intermediate
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local -r xml_file=/etc/hadoop/conf/mapred-site.xml
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local -r name=mapreduce.jobhistory.intermediate-done-dir
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local -r value=gs://dataproc-temp-us-east1-679657336577-juievcnm/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/mapreduce-job-history/done_intermediate
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local -r mode=overwrite
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local -r delimiter=,
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local skip=false
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local old_value
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local new_value
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + [[ overwrite == \o\v\e\r\w\r\i\t\e ]]
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + new_value=gs://dataproc-temp-us-east1-679657336577-juievcnm/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/mapreduce-job-history/done_intermediate
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + [[ false == \f\a\l\s\e ]]
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + bdconfig set_property --configuration_file /etc/hadoop/conf/mapred-site.xml --name mapreduce.jobhistory.intermediate-done-dir --value gs://dataproc-temp-us-east1-679657336577-juievcnm/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/mapreduce-job-history/done_intermediate --create_if_absent --clobber
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + set_property_mapred_site mapreduce.jobhistory.move.interval-ms 1000
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local -r name=mapreduce.jobhistory.move.interval-ms
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local -r value=1000
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + set_property_in_xml /etc/hadoop/conf/mapred-site.xml mapreduce.jobhistory.move.interval-ms 1000
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local -r xml_file=/etc/hadoop/conf/mapred-site.xml
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local -r name=mapreduce.jobhistory.move.interval-ms
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local -r value=1000
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local -r mode=overwrite
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local -r delimiter=,
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local skip=false
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local old_value
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local new_value
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + [[ overwrite == \o\v\e\r\w\r\i\t\e ]]
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + new_value=1000
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + [[ false == \f\a\l\s\e ]]
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + bdconfig set_property --configuration_file /etc/hadoop/conf/mapred-site.xml --name mapreduce.jobhistory.move.interval-ms --value 1000 --create_if_absent --clobber
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: ++ get_dataproc_property dataproc.users
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + DATAPROC_USERS=
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + [[ -n '' ]]
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + is_component_selected kerberos
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local -r component=kerberos
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local activated_components
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: ++ get_components_to_activate
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: ++ get_dataproc_property dataproc.components.activate
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: ++ tr '[:upper:]' '[:lower:]'
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + activated_components='knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3'
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + [[ knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3 == *kerberos* ]]
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + [[ standard == \s\t\a\n\d\a\r\d ]]
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + loginfo 'Merging user-specified cluster properties'
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + echo 'Merging user-specified cluster properties'
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: Merging user-specified cluster properties
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + merge_xml_properties /tmp/cluster/properties/capacity-scheduler.xml /etc/hadoop/conf/capacity-scheduler.xml
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local src=/tmp/cluster/properties/capacity-scheduler.xml
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local dest=/etc/hadoop/conf/capacity-scheduler.xml
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + [[ ! -f /tmp/cluster/properties/capacity-scheduler.xml ]]
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/capacity-scheduler.xml --source_configuration_file /tmp/cluster/properties/capacity-scheduler.xml --resolve_environment_variables --create_if_absent --clobber
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + loginfo 'Merged /tmp/cluster/properties/capacity-scheduler.xml.'
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + echo 'Merged /tmp/cluster/properties/capacity-scheduler.xml.'
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: Merged /tmp/cluster/properties/capacity-scheduler.xml.
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + merge_xml_properties /tmp/cluster/properties/core.xml /etc/hadoop/conf/core-site.xml
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local src=/tmp/cluster/properties/core.xml
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local dest=/etc/hadoop/conf/core-site.xml
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + [[ ! -f /tmp/cluster/properties/core.xml ]]
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/core-site.xml --source_configuration_file /tmp/cluster/properties/core.xml --resolve_environment_variables --create_if_absent --clobber
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + loginfo 'Merged /tmp/cluster/properties/core.xml.'
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + echo 'Merged /tmp/cluster/properties/core.xml.'
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: Merged /tmp/cluster/properties/core.xml.
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + merge_xml_properties /tmp/cluster/properties/distcp.xml /etc/hadoop/conf/distcp-default.xml
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local src=/tmp/cluster/properties/distcp.xml
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local dest=/etc/hadoop/conf/distcp-default.xml
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + [[ ! -f /tmp/cluster/properties/distcp.xml ]]
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/distcp-default.xml --source_configuration_file /tmp/cluster/properties/distcp.xml --resolve_environment_variables --create_if_absent --clobber
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + loginfo 'Merged /tmp/cluster/properties/distcp.xml.'
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + echo 'Merged /tmp/cluster/properties/distcp.xml.'
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: Merged /tmp/cluster/properties/distcp.xml.
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + merge_xml_properties /tmp/cluster/properties/mapred.xml /etc/hadoop/conf/mapred-site.xml
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local src=/tmp/cluster/properties/mapred.xml
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + local dest=/etc/hadoop/conf/mapred-site.xml
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + [[ ! -f /tmp/cluster/properties/mapred.xml ]]
<13>Apr 27 18:07:29 dataproc-startup-script[1265]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/mapred-site.xml --source_configuration_file /tmp/cluster/properties/mapred.xml --resolve_environment_variables --create_if_absent --clobber
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + loginfo 'Merged /tmp/cluster/properties/mapred.xml.'
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + echo 'Merged /tmp/cluster/properties/mapred.xml.'
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: Merged /tmp/cluster/properties/mapred.xml.
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + merge_xml_properties /tmp/cluster/properties/yarn.xml /etc/hadoop/conf/yarn-site.xml
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + local src=/tmp/cluster/properties/yarn.xml
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + local dest=/etc/hadoop/conf/yarn-site.xml
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + [[ ! -f /tmp/cluster/properties/yarn.xml ]]
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/yarn-site.xml --source_configuration_file /tmp/cluster/properties/yarn.xml --resolve_environment_variables --create_if_absent --clobber
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + loginfo 'Merged /tmp/cluster/properties/yarn.xml.'
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + echo 'Merged /tmp/cluster/properties/yarn.xml.'
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: Merged /tmp/cluster/properties/yarn.xml.
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + merge_sh_env_vars /tmp/cluster/properties/hadoop-env.sh /etc/hadoop/conf/hadoop-env.sh
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + local src=/tmp/cluster/properties/hadoop-env.sh
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + local dest=/etc/hadoop/conf/hadoop-env.sh
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + [[ ! -f /tmp/cluster/properties/hadoop-env.sh ]]
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + echo -e '\n# User-supplied properties.'
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + cat /tmp/cluster/properties/hadoop-env.sh
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + loginfo 'Merged /tmp/cluster/properties/hadoop-env.sh.'
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + echo 'Merged /tmp/cluster/properties/hadoop-env.sh.'
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: Merged /tmp/cluster/properties/hadoop-env.sh.
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + merge_sh_env_vars /tmp/cluster/properties/mapred-env.sh /etc/hadoop/conf/mapred-env.sh
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + local src=/tmp/cluster/properties/mapred-env.sh
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + local dest=/etc/hadoop/conf/mapred-env.sh
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + [[ ! -f /tmp/cluster/properties/mapred-env.sh ]]
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + echo -e '\n# User-supplied properties.'
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + cat /tmp/cluster/properties/mapred-env.sh
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + loginfo 'Merged /tmp/cluster/properties/mapred-env.sh.'
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + echo 'Merged /tmp/cluster/properties/mapred-env.sh.'
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: Merged /tmp/cluster/properties/mapred-env.sh.
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + merge_sh_env_vars /tmp/cluster/properties/yarn-env.sh /etc/hadoop/conf/yarn-env.sh
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + local src=/tmp/cluster/properties/yarn-env.sh
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + local dest=/etc/hadoop/conf/yarn-env.sh
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + [[ ! -f /tmp/cluster/properties/yarn-env.sh ]]
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + echo -e '\n# User-supplied properties.'
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + cat /tmp/cluster/properties/yarn-env.sh
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + loginfo 'Merged /tmp/cluster/properties/yarn-env.sh.'
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + echo 'Merged /tmp/cluster/properties/yarn-env.sh.'
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: Merged /tmp/cluster/properties/yarn-env.sh.
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + merge_java_properties /tmp/cluster/properties/hadoop-log4j.properties /etc/hadoop/conf/log4j.properties
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + local -r src=/tmp/cluster/properties/hadoop-log4j.properties
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + local -r dest=/etc/hadoop/conf/log4j.properties
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + local -r 'header=\n# User-supplied properties.'
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + [[ ! -f /tmp/cluster/properties/hadoop-log4j.properties ]]
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + echo -e '\n# User-supplied properties.'
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + cat /tmp/cluster/properties/hadoop-log4j.properties
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + loginfo 'Merged /tmp/cluster/properties/hadoop-log4j.properties.'
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + echo 'Merged /tmp/cluster/properties/hadoop-log4j.properties.'
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: Merged /tmp/cluster/properties/hadoop-log4j.properties.
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + [[ -f /etc/hbase/conf/hbase-site.xml ]]
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + DATAPROC_COMPONENTS=(${DATAPROC_OPTIONAL_COMPONENTS})
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + echo 'DATAPROC_COMPONENTS: dpms-proxy hdfs hive-metastore hive-server2 kerberos knox mapreduce mysql npd proxy-agent spark tez yarn zookeeper-server jupyter fluentbit-ucp otel-ucp stackdriver-agent-container google-fluentd-container earlyoom rubix miniconda3 docker-ce hive-webhcat-server zeppelin flink hudi pig ranger solr-server trino iceberg delta jupyter-kernel-gateway'
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: DATAPROC_COMPONENTS: dpms-proxy hdfs hive-metastore hive-server2 kerberos knox mapreduce mysql npd proxy-agent spark tez yarn zookeeper-server jupyter fluentbit-ucp otel-ucp stackdriver-agent-container google-fluentd-container earlyoom rubix miniconda3 docker-ce hive-webhcat-server zeppelin flink hudi pig ranger solr-server trino iceberg delta jupyter-kernel-gateway
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + ARTIFACTS_TO_KEEP=("${SERVICES[@]}" ${DATAPROC_COMMON_PACKAGES})
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + echo 'ARTIFACTS_TO_KEEP: dpms-proxy hadoop-hdfs-namenode hive-metastore hive-server2 knox proxy-agent spark-history-server hadoop-yarn-resourcemanager hive-webhcat-server solr-server jupyter-kernel-gateway hadoop-mapreduce-historyserver mysql-server hadoop-yarn-timelineserver jupyter zeppelin hadoop-hdfs-secondarynamenode autofs bash-completion bc chrony git jq unzip vim wget bigtop-utils hadoop-client hadoop-lzo temurin-11-jdk=11.0.20.1.0+1 temurin-17-jdk hdfs libhdfs0 hive-metastore hive-server2 hive-hcatalog kerberos mapreduce mysql mysql-connector-j npd spark tez yarn zookeeper-server texlive-xetex default-jre- texlive-fonts-recommended texlive-plain-generic fluentbit-ucp otel-ucp stackdriver-agent-container google-fluentd-container earlyoom rubix miniconda3 docker-ce flink hudi pig ranger trino iceberg delta libapr1 libatlas3-base libjansi-java libsnappy-dev libssl-dev libzstd-dev openssl uuid-runtime linux-headers-cloud-amd64 linux-image-cloud-amd64 libopenblas0 netcat-openbsd python3-numpy python3-pi
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: p python3-requests python3-setuptools'
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: ARTIFACTS_TO_KEEP: dpms-proxy hadoop-hdfs-namenode hive-metastore hive-server2 knox proxy-agent spark-history-server hadoop-yarn-resourcemanager hive-webhcat-server solr-server jupyter-kernel-gateway hadoop-mapreduce-historyserver mysql-server hadoop-yarn-timelineserver jupyter zeppelin hadoop-hdfs-secondarynamenode autofs bash-completion bc chrony git jq unzip vim wget bigtop-utils hadoop-client hadoop-lzo temurin-11-jdk=11.0.20.1.0+1 temurin-17-jdk hdfs libhdfs0 hive-metastore hive-server2 hive-hcatalog kerberos mapreduce mysql mysql-connector-j npd spark tez yarn zookeeper-server texlive-xetex default-jre- texlive-fonts-recommended texlive-plain-generic fluentbit-ucp otel-ucp stackdriver-agent-container google-fluentd-container earlyoom rubix miniconda3 docker-ce flink hudi pig ranger trino iceberg delta libapr1 libatlas3-base libjansi-java libsnappy-dev libssl-dev libzstd-dev openssl uuid-runtime linux-headers-cloud-amd64 linux-image-cloud-amd64 libopenblas0 netcat-openbsd python3-numpy python3-pip python
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: 3-requests python3-setuptools
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + is_rm_image
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\r\a\y ]]
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + COMPONENTS_TO_ACTIVATE=($(intersection SELECTED_COMPONENTS ARTIFACTS_TO_KEEP))
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: ++ intersection SELECTED_COMPONENTS ARTIFACTS_TO_KEEP
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: ++ local -n values=SELECTED_COMPONENTS
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: ++ local -n filter=ARTIFACTS_TO_KEEP
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: ++ comm -12 /dev/fd/63 /dev/fd/62
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: +++ sort -u
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: +++ sort -u
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: +++ printf '%s\n' knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: +++ printf '%s\n' dpms-proxy hadoop-hdfs-namenode hive-metastore hive-server2 knox proxy-agent spark-history-server hadoop-yarn-resourcemanager hive-webhcat-server solr-server jupyter-kernel-gateway hadoop-mapreduce-historyserver mysql-server hadoop-yarn-timelineserver jupyter zeppelin hadoop-hdfs-secondarynamenode autofs bash-completion bc chrony git jq unzip vim wget bigtop-utils hadoop-client hadoop-lzo temurin-11-jdk=11.0.20.1.0+1 temurin-17-jdk hdfs libhdfs0 hive-metastore hive-server2 hive-hcatalog kerberos mapreduce mysql mysql-connector-j npd spark tez yarn zookeeper-server texlive-xetex default-jre- texlive-fonts-recommended texlive-plain-generic fluentbit-ucp otel-ucp stackdriver-agent-container google-fluentd-container earlyoom rubix miniconda3 docker-ce flink hudi pig ranger trino iceberg delta libapr1 libatlas3-base libjansi-java libsnappy-dev libssl-dev libzstd-dev openssl uuid-runtime linux-headers-cloud-amd64 linux-image-cloud-amd64 libopenblas0 netcat-openbsd python3-numpy python3-pip python3
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: -requests python3-setuptools
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + echo 'COMPONENTS_TO_ACTIVATE: earlyoom fluentbit-ucp hdfs hive-metastore hive-server2 knox mapreduce miniconda3 mysql npd otel-ucp pig proxy-agent spark tez yarn'
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: COMPONENTS_TO_ACTIVATE: earlyoom fluentbit-ucp hdfs hive-metastore hive-server2 knox mapreduce miniconda3 mysql npd otel-ucp pig proxy-agent spark tez yarn
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + [[ standard == \s\t\a\n\d\a\r\d ]]
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + loginfo 'Generating post_hdfs_env.sh'
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + echo 'Generating post_hdfs_env.sh'
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: Generating post_hdfs_env.sh
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + cat
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + chmod +x /usr/local/share/google/dataproc/bdutil/components/post_hdfs_env.sh
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + NON_ACTIVATED_COMPONENTS=($(difference DATAPROC_COMPONENTS COMPONENTS_TO_ACTIVATE))
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: ++ difference DATAPROC_COMPONENTS COMPONENTS_TO_ACTIVATE
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: ++ local -n values=DATAPROC_COMPONENTS
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: ++ local -n filter=COMPONENTS_TO_ACTIVATE
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: ++ comm -23 /dev/fd/63 /dev/fd/62
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: +++ sort -u
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: +++ printf '%s\n' dpms-proxy hdfs hive-metastore hive-server2 kerberos knox mapreduce mysql npd proxy-agent spark tez yarn zookeeper-server jupyter fluentbit-ucp otel-ucp stackdriver-agent-container google-fluentd-container earlyoom rubix miniconda3 docker-ce hive-webhcat-server zeppelin flink hudi pig ranger solr-server trino iceberg delta jupyter-kernel-gateway
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: +++ printf '%s\n' earlyoom fluentbit-ucp hdfs hive-metastore hive-server2 knox mapreduce miniconda3 mysql npd otel-ucp pig proxy-agent spark tez yarn
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: +++ sort -u
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + echo 'NON_ACTIVATED_COMPONENTS: delta docker-ce dpms-proxy flink google-fluentd-container hive-webhcat-server hudi iceberg jupyter jupyter-kernel-gateway kerberos ranger rubix solr-server stackdriver-agent-container trino zeppelin zookeeper-server'
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: NON_ACTIVATED_COMPONENTS: delta docker-ce dpms-proxy flink google-fluentd-container hive-webhcat-server hudi iceberg jupyter jupyter-kernel-gateway kerberos ranger rubix solr-server stackdriver-agent-container trino zeppelin zookeeper-server
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + ARTIFACTS_TO_UNINSTALL+=($(difference NON_ACTIVATED_COMPONENTS ARTIFACTS_TO_UNINSTALL))
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: ++ difference NON_ACTIVATED_COMPONENTS ARTIFACTS_TO_UNINSTALL
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: ++ local -n values=NON_ACTIVATED_COMPONENTS
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: ++ local -n filter=ARTIFACTS_TO_UNINSTALL
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: ++ comm -23 /dev/fd/63 /dev/fd/62
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: +++ sort -u
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: +++ printf '%s\n' hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-hdfs-datanode hadoop-yarn-nodemanager
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: +++ printf '%s\n' delta docker-ce dpms-proxy flink google-fluentd-container hive-webhcat-server hudi iceberg jupyter jupyter-kernel-gateway kerberos ranger rubix solr-server stackdriver-agent-container trino zeppelin zookeeper-server
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: +++ sort -u
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + echo 'ARTIFACTS_TO_UNINSTALL: hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-hdfs-datanode hadoop-yarn-nodemanager delta docker-ce dpms-proxy flink google-fluentd-container hive-webhcat-server hudi iceberg jupyter jupyter-kernel-gateway kerberos ranger rubix solr-server stackdriver-agent-container trino zeppelin zookeeper-server'
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: ARTIFACTS_TO_UNINSTALL: hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-hdfs-datanode hadoop-yarn-nodemanager delta docker-ce dpms-proxy flink google-fluentd-container hive-webhcat-server hudi iceberg jupyter jupyter-kernel-gateway kerberos ranger rubix solr-server stackdriver-agent-container trino zeppelin zookeeper-server
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + COMPONENTS_TO_UNINSTALL=($(intersection ARTIFACTS_TO_UNINSTALL DATAPROC_COMPONENTS))
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: ++ intersection ARTIFACTS_TO_UNINSTALL DATAPROC_COMPONENTS
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: ++ local -n values=ARTIFACTS_TO_UNINSTALL
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: ++ local -n filter=DATAPROC_COMPONENTS
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: ++ comm -12 /dev/fd/63 /dev/fd/62
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: +++ sort -u
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: +++ printf '%s\n' hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-hdfs-datanode hadoop-yarn-nodemanager delta docker-ce dpms-proxy flink google-fluentd-container hive-webhcat-server hudi iceberg jupyter jupyter-kernel-gateway kerberos ranger rubix solr-server stackdriver-agent-container trino zeppelin zookeeper-server
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: +++ printf '%s\n' dpms-proxy hdfs hive-metastore hive-server2 kerberos knox mapreduce mysql npd proxy-agent spark tez yarn zookeeper-server jupyter fluentbit-ucp otel-ucp stackdriver-agent-container google-fluentd-container earlyoom rubix miniconda3 docker-ce hive-webhcat-server zeppelin flink hudi pig ranger solr-server trino iceberg delta jupyter-kernel-gateway
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: +++ sort -u
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + echo 'COMPONENTS_TO_UNINSTALL: delta docker-ce dpms-proxy flink google-fluentd-container hive-webhcat-server hudi iceberg jupyter jupyter-kernel-gateway kerberos ranger rubix solr-server stackdriver-agent-container trino zeppelin zookeeper-server'
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: COMPONENTS_TO_UNINSTALL: delta docker-ce dpms-proxy flink google-fluentd-container hive-webhcat-server hudi iceberg jupyter jupyter-kernel-gateway kerberos ranger rubix solr-server stackdriver-agent-container trino zeppelin zookeeper-server
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + PACKAGES_TO_UNINSTALL=($(difference ARTIFACTS_TO_UNINSTALL DATAPROC_COMPONENTS))
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: ++ difference ARTIFACTS_TO_UNINSTALL DATAPROC_COMPONENTS
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: ++ local -n values=ARTIFACTS_TO_UNINSTALL
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: ++ local -n filter=DATAPROC_COMPONENTS
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: ++ comm -23 /dev/fd/63 /dev/fd/62
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: +++ sort -u
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: +++ printf '%s\n' hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-hdfs-datanode hadoop-yarn-nodemanager delta docker-ce dpms-proxy flink google-fluentd-container hive-webhcat-server hudi iceberg jupyter jupyter-kernel-gateway kerberos ranger rubix solr-server stackdriver-agent-container trino zeppelin zookeeper-server
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: +++ printf '%s\n' dpms-proxy hdfs hive-metastore hive-server2 kerberos knox mapreduce mysql npd proxy-agent spark tez yarn zookeeper-server jupyter fluentbit-ucp otel-ucp stackdriver-agent-container google-fluentd-container earlyoom rubix miniconda3 docker-ce hive-webhcat-server zeppelin flink hudi pig ranger solr-server trino iceberg delta jupyter-kernel-gateway
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: +++ sort -u
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + echo 'PACKAGES_TO_UNINSTALL: hadoop-hdfs-datanode hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-yarn-nodemanager'
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: PACKAGES_TO_UNINSTALL: hadoop-hdfs-datanode hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-yarn-nodemanager
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + [[ standard == \s\t\a\n\d\a\r\d ]]
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + is_version_at_least 2.2 2.3
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + local reenable_x=false
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + [[ -o xtrace ]]
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + set +x
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + case ${compare_versions_result} in
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + return 1
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: ++ get_dataproc_property dataproc.logging.stackdriver.enable
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + STACKDRIVER_LOGGING_ENABLED=
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + [[ standard == \s\t\a\n\d\a\r\d ]]
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + [[ 2.2 == \2\.\2 ]]
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + is_legacy_containerized_agents_enabled
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: ++ get_dataproc_property_or_default dataproc.observability.containerised.legacy.agents.enable false
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + local -r legacy_containerised_agents_enabled=false
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + [[ false == \t\r\u\e ]]
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + uninstall_containerised_agents
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + source /usr/local/share/google/dataproc/bdutil/components/shared/containerized-obs-agents.sh
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: ++ set -x
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: ++ readonly LOGGING_AGENT_IMAGE=gcr.io/cloud-dataproc/observability/logging-agent:latest
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: ++ LOGGING_AGENT_IMAGE=gcr.io/cloud-dataproc/observability/logging-agent:latest
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: ++ readonly MONITORING_AGENT_IMAGE=gcr.io/cloud-dataproc/observability/monitoring-agent:latest
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: ++ MONITORING_AGENT_IMAGE=gcr.io/cloud-dataproc/observability/monitoring-agent:latest
<13>Apr 27 18:07:30 dataproc-startup-script[1265]: + systemctl start docker
<13>Apr 27 18:07:31 dataproc-startup-script[1265]: + docker image rm gcr.io/cloud-dataproc/observability/monitoring-agent:latest
<13>Apr 27 18:07:32 dataproc-startup-script[1265]: Untagged: gcr.io/cloud-dataproc/observability/monitoring-agent:latest
<13>Apr 27 18:07:32 dataproc-startup-script[1265]: Untagged: gcr.io/cloud-dataproc/observability/monitoring-agent@sha256:24d562eed77a6cad0edac4b3226d89d1611a49a48eb2c1217cb271d316bf0248
<13>Apr 27 18:07:32 dataproc-startup-script[1265]: Deleted: sha256:b6cc9be709f490d95853ab2de600041efea9f59a79f628e150155761d3eabbb6
<13>Apr 27 18:07:32 dataproc-startup-script[1265]: Deleted: sha256:1dc6ee519e55293f02ed83f5226dbc15bce079a89796ee5df018181b6405c8da
<13>Apr 27 18:07:32 dataproc-startup-script[1265]: Deleted: sha256:efd654cea983212c1142c0881a91d9fe2be6870f8a9cca54d672f220d0878302
<13>Apr 27 18:07:32 dataproc-startup-script[1265]: Deleted: sha256:62e5079e2bb8d6b2ff9e57f6784face341f0bf1a00003ccb281373cd8e2e57e4
<13>Apr 27 18:07:32 dataproc-startup-script[1265]: Deleted: sha256:0f79ed81670d2b1c482a7af0ccbe1b10bfe3ea2db26a036ff526d5f55274a036
<13>Apr 27 18:07:32 dataproc-startup-script[1265]: + docker image rm gcr.io/cloud-dataproc/observability/logging-agent:latest
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: Untagged: gcr.io/cloud-dataproc/observability/logging-agent:latest
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: Untagged: gcr.io/cloud-dataproc/observability/logging-agent@sha256:0da0e2bd764cc12f98a18df0502bc7869e169f70a56b02b3fbabc0d06c272484
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: Deleted: sha256:8928d5fa419eba3c42f8cfec245b2ec80e6c78d9c31e91f8c71f7dd0a35010f6
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: Deleted: sha256:67b21cf07df3a587c3dc94f2e11514d2994e29a2cbb44e1ab0055a5da1cc8af2
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: Deleted: sha256:da7310c09a42543b1f06a798bf7de5423fad0aca048f7d46154716a1d232d5c2
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: Deleted: sha256:476baebdfbf7a68c50e979971fcd47d799d1b194bcf1f03c1c979e9262bcd364
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + is_rm_image
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\r\a\y ]]
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + configure_conscrypt
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local conscrypt_enabled
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: ++ get_dataproc_property dataproc.conscrypt.provider.enable
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + conscrypt_enabled=true
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + [[ true == \t\r\u\e ]]
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + is_arm64
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: ++ uname -m
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + [[ x86_64 == \a\a\r\c\h\6\4 ]]
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local -r suffix=x86_64
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local -r conscrypt_shared_lib=/usr/local/share/google/dataproc/conscrypt/libconscrypt_openjdk_jni-linux-x86_64.so
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local conscrypt_jar
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: ++ compgen -G '/usr/local/share/google/dataproc/conscrypt/conscrypt-openjdk-*.jar'
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + conscrypt_jar=/usr/local/share/google/dataproc/conscrypt/conscrypt-openjdk-2.5.2-linux-x86_64.jar
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + is_java8
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + [[ /usr/lib/jvm/temurin-11-jdk-amd64 == *\-\8\-* ]]
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + ln -s /usr/local/share/google/dataproc/conscrypt/conscrypt-openjdk-2.5.2-linux-x86_64.jar /usr/local/share/google/dataproc/lib/conscrypt.jar
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + copy_conscript_security_config_java11_plus /usr/lib/jvm/temurin-11-jdk-amd64 /usr/lib/jvm/temurin-11-jdk-amd64/conf /usr/local/share/google/dataproc/conscrypt/libconscrypt_openjdk_jni-linux-x86_64.so
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local -r jdk_home=/usr/lib/jvm/temurin-11-jdk-amd64
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local -r jdk_config_dir=/usr/lib/jvm/temurin-11-jdk-amd64/conf
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local -r conscrypt_shared_lib=/usr/local/share/google/dataproc/conscrypt/libconscrypt_openjdk_jni-linux-x86_64.so
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + ln -s /usr/local/share/google/dataproc/conscrypt/libconscrypt_openjdk_jni-linux-x86_64.so /usr/lib/jvm/temurin-11-jdk-amd64/lib/libconscrypt_openjdk_jni.so
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + cp /usr/local/share/google/dataproc/java.security.conscrypt /usr/lib/jvm/temurin-11-jdk-amd64/conf/security/java.security.tmp
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + [[ -n /usr/lib/jvm/temurin-17-jdk-amd64 ]]
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + copy_conscript_security_config_java11_plus /usr/lib/jvm/temurin-17-jdk-amd64 /usr/lib/jvm/temurin-17-jdk-amd64/conf /usr/local/share/google/dataproc/conscrypt/libconscrypt_openjdk_jni-linux-x86_64.so
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local -r jdk_home=/usr/lib/jvm/temurin-17-jdk-amd64
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local -r jdk_config_dir=/usr/lib/jvm/temurin-17-jdk-amd64/conf
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local -r conscrypt_shared_lib=/usr/local/share/google/dataproc/conscrypt/libconscrypt_openjdk_jni-linux-x86_64.so
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + ln -s /usr/local/share/google/dataproc/conscrypt/libconscrypt_openjdk_jni-linux-x86_64.so /usr/lib/jvm/temurin-17-jdk-amd64/lib/libconscrypt_openjdk_jni.so
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + cp /usr/local/share/google/dataproc/java.security.conscrypt /usr/lib/jvm/temurin-17-jdk-amd64/conf/security/java.security.tmp
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + mv -f /usr/lib/jvm/temurin-11-jdk-amd64/conf/security/java.security.tmp /usr/lib/jvm/temurin-11-jdk-amd64/conf/security/java.security
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + [[ -n /usr/lib/jvm/temurin-17-jdk-amd64 ]]
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + mv -f /usr/lib/jvm/temurin-17-jdk-amd64/conf/security/java.security.tmp /usr/lib/jvm/temurin-17-jdk-amd64/conf/security/java.security
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + create_common_restart_drop_in_configs
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + create_restart_drop_in_config /etc/systemd/system/common/restart.conf on-failure 0
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local -r conf_path=/etc/systemd/system/common/restart.conf
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local -r restart_type=on-failure
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local -r interval_sec=0
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + [[ ! -f /etc/systemd/system/common/restart.conf ]]
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: ++ dirname /etc/systemd/system/common/restart.conf
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + mkdir -p /etc/systemd/system/common
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + cat
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + create_restart_drop_in_config /etc/systemd/system/common/worker-restart.conf always 0
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local -r conf_path=/etc/systemd/system/common/worker-restart.conf
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local -r restart_type=always
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local -r interval_sec=0
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + [[ ! -f /etc/systemd/system/common/worker-restart.conf ]]
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: ++ dirname /etc/systemd/system/common/worker-restart.conf
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + mkdir -p /etc/systemd/system/common
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + cat
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + [[ standard != \s\t\a\n\d\a\r\d ]]
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + loginfo 'Pre-activating components'
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + echo 'Pre-activating components'
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: Pre-activating components
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + pre_activate_components earlyoom fluentbit-ucp hdfs hive-metastore hive-server2 knox mapreduce miniconda3 mysql npd otel-ucp pig proxy-agent spark tez yarn
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + components=('earlyoom' 'fluentbit-ucp' 'hdfs' 'hive-metastore' 'hive-server2' 'knox' 'mapreduce' 'miniconda3' 'mysql' 'npd' 'otel-ucp' 'pig' 'proxy-agent' 'spark' 'tez' 'yarn')
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local components
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + run_components_scripts pre-activate earlyoom fluentbit-ucp hdfs hive-metastore hive-server2 knox mapreduce miniconda3 mysql npd otel-ucp pig proxy-agent spark tez yarn
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local -r script_type=pre-activate
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + all_components=('earlyoom' 'fluentbit-ucp' 'hdfs' 'hive-metastore' 'hive-server2' 'knox' 'mapreduce' 'miniconda3' 'mysql' 'npd' 'otel-ucp' 'pig' 'proxy-agent' 'spark' 'tez' 'yarn')
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local -r all_components
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + components=()
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local components
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + for component in "${all_components[@]}"
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/earlyoom.sh
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/earlyoom.sh ]]
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + for component in "${all_components[@]}"
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/fluentbit-ucp.sh
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/fluentbit-ucp.sh ]]
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + components+=("${component}")
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + for component in "${all_components[@]}"
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.sh
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.sh ]]
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + components+=("${component}")
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + for component in "${all_components[@]}"
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.sh
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.sh ]]
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + components+=("${component}")
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + for component in "${all_components[@]}"
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.sh
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.sh ]]
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + components+=("${component}")
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + for component in "${all_components[@]}"
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/knox.sh
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/knox.sh ]]
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + for component in "${all_components[@]}"
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/mapreduce.sh
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/mapreduce.sh ]]
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + for component in "${all_components[@]}"
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/miniconda3.sh
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/miniconda3.sh ]]
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + for component in "${all_components[@]}"
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/mysql.sh
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/mysql.sh ]]
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + components+=("${component}")
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + for component in "${all_components[@]}"
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/npd.sh
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/npd.sh ]]
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + for component in "${all_components[@]}"
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/otel-ucp.sh
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/otel-ucp.sh ]]
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + components+=("${component}")
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + for component in "${all_components[@]}"
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/pig.sh
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/pig.sh ]]
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + components+=("${component}")
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + for component in "${all_components[@]}"
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/proxy-agent.sh
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/proxy-agent.sh ]]
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + for component in "${all_components[@]}"
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh ]]
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + components+=("${component}")
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + for component in "${all_components[@]}"
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/tez.sh
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/tez.sh ]]
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + components+=("${component}")
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + for component in "${all_components[@]}"
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/yarn.sh
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/yarn.sh ]]
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + components+=("${component}")
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + echo 'Components with pre-activate script: fluentbit-ucp hdfs hive-metastore hive-server2 mysql otel-ucp pig spark tez yarn'
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: Components with pre-activate script: fluentbit-ucp hdfs hive-metastore hive-server2 mysql otel-ucp pig spark tez yarn
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local -r sentinel_dir=/tmp/dataproc/sentinel
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + mkdir -p /tmp/dataproc/sentinel
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + names=()
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local names
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + scripts=()
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local scripts
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + deps=()
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local deps
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + for cmp in "${components[@]}"
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + names+=("${cmp}.${script_type}")
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + scripts+=("${BDUTIL_DIR}/components/${script_type}/${cmp}.sh")
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + deps+=("${BDUTIL_DIR}/components/${script_type}/${cmp}.deps")
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + for cmp in "${components[@]}"
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + names+=("${cmp}.${script_type}")
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + scripts+=("${BDUTIL_DIR}/components/${script_type}/${cmp}.sh")
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + deps+=("${BDUTIL_DIR}/components/${script_type}/${cmp}.deps")
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + for cmp in "${components[@]}"
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + names+=("${cmp}.${script_type}")
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + scripts+=("${BDUTIL_DIR}/components/${script_type}/${cmp}.sh")
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + deps+=("${BDUTIL_DIR}/components/${script_type}/${cmp}.deps")
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + for cmp in "${components[@]}"
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + names+=("${cmp}.${script_type}")
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + scripts+=("${BDUTIL_DIR}/components/${script_type}/${cmp}.sh")
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + deps+=("${BDUTIL_DIR}/components/${script_type}/${cmp}.deps")
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + for cmp in "${components[@]}"
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + names+=("${cmp}.${script_type}")
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + scripts+=("${BDUTIL_DIR}/components/${script_type}/${cmp}.sh")
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + deps+=("${BDUTIL_DIR}/components/${script_type}/${cmp}.deps")
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + for cmp in "${components[@]}"
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + names+=("${cmp}.${script_type}")
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + scripts+=("${BDUTIL_DIR}/components/${script_type}/${cmp}.sh")
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + deps+=("${BDUTIL_DIR}/components/${script_type}/${cmp}.deps")
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + for cmp in "${components[@]}"
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + names+=("${cmp}.${script_type}")
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + scripts+=("${BDUTIL_DIR}/components/${script_type}/${cmp}.sh")
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + deps+=("${BDUTIL_DIR}/components/${script_type}/${cmp}.deps")
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + for cmp in "${components[@]}"
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + names+=("${cmp}.${script_type}")
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + scripts+=("${BDUTIL_DIR}/components/${script_type}/${cmp}.sh")
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + deps+=("${BDUTIL_DIR}/components/${script_type}/${cmp}.deps")
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + for cmp in "${components[@]}"
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + names+=("${cmp}.${script_type}")
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + scripts+=("${BDUTIL_DIR}/components/${script_type}/${cmp}.sh")
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + deps+=("${BDUTIL_DIR}/components/${script_type}/${cmp}.deps")
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + for cmp in "${components[@]}"
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + names+=("${cmp}.${script_type}")
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + scripts+=("${BDUTIL_DIR}/components/${script_type}/${cmp}.sh")
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + deps+=("${BDUTIL_DIR}/components/${script_type}/${cmp}.deps")
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + execute_task_graph 'fluentbit-ucp.pre-activate hdfs.pre-activate hive-metastore.pre-activate hive-server2.pre-activate mysql.pre-activate otel-ucp.pre-activate pig.pre-activate spark.pre-activate tez.pre-activate yarn.pre-activate' '/usr/local/share/google/dataproc/bdutil/components/pre-activate/fluentbit-ucp.sh /usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.sh /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.sh /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.sh /usr/local/share/google/dataproc/bdutil/components/pre-activate/mysql.sh /usr/local/share/google/dataproc/bdutil/components/pre-activate/otel-ucp.sh /usr/local/share/google/dataproc/bdutil/components/pre-activate/pig.sh /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh /usr/local/share/google/dataproc/bdutil/components/pre-activate/tez.sh /usr/local/share/google/dataproc/bdutil/components/pre-activate/yarn.sh' '/usr/local/share/google/dataproc/bdu
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: til/components/pre-activate/fluentbit-ucp.deps /usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.deps /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.deps /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.deps /usr/local/share/google/dataproc/bdutil/components/pre-activate/mysql.deps /usr/local/share/google/dataproc/bdutil/components/pre-activate/otel-ucp.deps /usr/local/share/google/dataproc/bdutil/components/pre-activate/pig.deps /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.deps /usr/local/share/google/dataproc/bdutil/components/pre-activate/tez.deps /usr/local/share/google/dataproc/bdutil/components/pre-activate/yarn.deps' /tmp/dataproc/sentinel 2.2
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + echo 'Generating makefile for the task graph'
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: Generating makefile for the task graph
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + mkdir -p /tmp/dataproc/make
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local makefile
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: ++ mktemp /tmp/dataproc/make/makefile.XXXXXX
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + makefile=/tmp/dataproc/make/makefile.datlvq
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + generate_task_graph_makefile 'fluentbit-ucp.pre-activate hdfs.pre-activate hive-metastore.pre-activate hive-server2.pre-activate mysql.pre-activate otel-ucp.pre-activate pig.pre-activate spark.pre-activate tez.pre-activate yarn.pre-activate' '/usr/local/share/google/dataproc/bdutil/components/pre-activate/fluentbit-ucp.sh /usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.sh /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.sh /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.sh /usr/local/share/google/dataproc/bdutil/components/pre-activate/mysql.sh /usr/local/share/google/dataproc/bdutil/components/pre-activate/otel-ucp.sh /usr/local/share/google/dataproc/bdutil/components/pre-activate/pig.sh /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh /usr/local/share/google/dataproc/bdutil/components/pre-activate/tez.sh /usr/local/share/google/dataproc/bdutil/components/pre-activate/yarn.sh' '/usr/local/share/google/da
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: taproc/bdutil/components/pre-activate/fluentbit-ucp.deps /usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.deps /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.deps /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.deps /usr/local/share/google/dataproc/bdutil/components/pre-activate/mysql.deps /usr/local/share/google/dataproc/bdutil/components/pre-activate/otel-ucp.deps /usr/local/share/google/dataproc/bdutil/components/pre-activate/pig.deps /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.deps /usr/local/share/google/dataproc/bdutil/components/pre-activate/tez.deps /usr/local/share/google/dataproc/bdutil/components/pre-activate/yarn.deps' /tmp/dataproc/sentinel 2.2
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + names=()
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local names
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + read -r -a names
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + scripts=()
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local scripts
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + read -r -a scripts
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + deps_manifests=()
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local deps_manifests
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + read -r -a deps_manifests
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local -r sentinel_dir=/tmp/dataproc/sentinel
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + task_args=('2.2')
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local -r task_args
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + targets=()
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local targets
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + (( i = 0 ))
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + (( i < 10 ))
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local name=fluentbit-ucp.pre-activate
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/fluentbit-ucp.sh
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local deps_manifest=/usr/local/share/google/dataproc/bdutil/components/pre-activate/fluentbit-ucp.deps
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + deps=()
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local deps
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/fluentbit-ucp.deps ]]
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local target=/tmp/dataproc/sentinel/fluentbit-ucp.pre-activate
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + targets+=("${target}")
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + echo -e '/tmp/dataproc/sentinel/fluentbit-ucp.pre-activate: '
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + echo -e '\t@echo '\''Running task: fluentbit-ucp.pre-activate'\'''
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + echo -e '\tbash -e /usr/local/share/google/dataproc/bdutil/components/pre-activate/fluentbit-ucp.sh 2.2 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/fluentbit-ucp.sh"; exit 1; }'
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + echo -e '\ttouch /tmp/dataproc/sentinel/fluentbit-ucp.pre-activate\n'
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + (( i++ ))
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + (( i < 10 ))
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local name=hdfs.pre-activate
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.sh
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local deps_manifest=/usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.deps
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + deps=()
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local deps
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.deps ]]
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local target=/tmp/dataproc/sentinel/hdfs.pre-activate
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + targets+=("${target}")
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + echo -e '/tmp/dataproc/sentinel/hdfs.pre-activate: '
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + echo -e '\t@echo '\''Running task: hdfs.pre-activate'\'''
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + echo -e '\tbash -e /usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.sh 2.2 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.sh"; exit 1; }'
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + echo -e '\ttouch /tmp/dataproc/sentinel/hdfs.pre-activate\n'
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + (( i++ ))
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + (( i < 10 ))
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local name=hive-metastore.pre-activate
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.sh
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local deps_manifest=/usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.deps
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + deps=()
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local deps
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.deps ]]
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local target=/tmp/dataproc/sentinel/hive-metastore.pre-activate
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + targets+=("${target}")
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + echo -e '/tmp/dataproc/sentinel/hive-metastore.pre-activate: '
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + echo -e '\t@echo '\''Running task: hive-metastore.pre-activate'\'''
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + echo -e '\tbash -e /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.sh 2.2 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.sh"; exit 1; }'
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + echo -e '\ttouch /tmp/dataproc/sentinel/hive-metastore.pre-activate\n'
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + (( i++ ))
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + (( i < 10 ))
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local name=hive-server2.pre-activate
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.sh
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local deps_manifest=/usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.deps
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + deps=()
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local deps
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.deps ]]
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: ++ cat /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.deps
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + dep_names=('mysql.pre-activate' 'hdfs.pre-activate' 'hive-metastore.pre-activate' 'tez.pre-activate')
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local dep_names
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + (( j = 0 ))
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + (( j < 4 ))
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local dep_name=mysql.pre-activate
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + [[  fluentbit-ucp.pre-activate hdfs.pre-activate hive-metastore.pre-activate hive-server2.pre-activate mysql.pre-activate otel-ucp.pre-activate pig.pre-activate spark.pre-activate tez.pre-activate yarn.pre-activate  =~  mysql\.pre-activate  ]]
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + deps+=("${sentinel_dir}/${dep_name}")
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + (( j++ ))
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + (( j < 4 ))
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local dep_name=hdfs.pre-activate
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + [[  fluentbit-ucp.pre-activate hdfs.pre-activate hive-metastore.pre-activate hive-server2.pre-activate mysql.pre-activate otel-ucp.pre-activate pig.pre-activate spark.pre-activate tez.pre-activate yarn.pre-activate  =~  hdfs\.pre-activate  ]]
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + deps+=("${sentinel_dir}/${dep_name}")
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + (( j++ ))
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + (( j < 4 ))
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local dep_name=hive-metastore.pre-activate
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + [[  fluentbit-ucp.pre-activate hdfs.pre-activate hive-metastore.pre-activate hive-server2.pre-activate mysql.pre-activate otel-ucp.pre-activate pig.pre-activate spark.pre-activate tez.pre-activate yarn.pre-activate  =~  hive-metastore\.pre-activate  ]]
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + deps+=("${sentinel_dir}/${dep_name}")
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + (( j++ ))
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + (( j < 4 ))
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local dep_name=tez.pre-activate
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + [[  fluentbit-ucp.pre-activate hdfs.pre-activate hive-metastore.pre-activate hive-server2.pre-activate mysql.pre-activate otel-ucp.pre-activate pig.pre-activate spark.pre-activate tez.pre-activate yarn.pre-activate  =~  tez\.pre-activate  ]]
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + deps+=("${sentinel_dir}/${dep_name}")
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + (( j++ ))
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + (( j < 4 ))
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local target=/tmp/dataproc/sentinel/hive-server2.pre-activate
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + targets+=("${target}")
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + echo -e '/tmp/dataproc/sentinel/hive-server2.pre-activate: /tmp/dataproc/sentinel/mysql.pre-activate /tmp/dataproc/sentinel/hdfs.pre-activate /tmp/dataproc/sentinel/hive-metastore.pre-activate /tmp/dataproc/sentinel/tez.pre-activate'
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + echo -e '\t@echo '\''Running task: hive-server2.pre-activate'\'''
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + echo -e '\tbash -e /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.sh 2.2 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.sh"; exit 1; }'
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + echo -e '\ttouch /tmp/dataproc/sentinel/hive-server2.pre-activate\n'
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + (( i++ ))
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + (( i < 10 ))
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local name=mysql.pre-activate
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/mysql.sh
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local deps_manifest=/usr/local/share/google/dataproc/bdutil/components/pre-activate/mysql.deps
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + deps=()
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local deps
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/mysql.deps ]]
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local target=/tmp/dataproc/sentinel/mysql.pre-activate
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + targets+=("${target}")
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + echo -e '/tmp/dataproc/sentinel/mysql.pre-activate: '
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + echo -e '\t@echo '\''Running task: mysql.pre-activate'\'''
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + echo -e '\tbash -e /usr/local/share/google/dataproc/bdutil/components/pre-activate/mysql.sh 2.2 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/mysql.sh"; exit 1; }'
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + echo -e '\ttouch /tmp/dataproc/sentinel/mysql.pre-activate\n'
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + (( i++ ))
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + (( i < 10 ))
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local name=otel-ucp.pre-activate
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/otel-ucp.sh
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local deps_manifest=/usr/local/share/google/dataproc/bdutil/components/pre-activate/otel-ucp.deps
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + deps=()
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local deps
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/otel-ucp.deps ]]
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local target=/tmp/dataproc/sentinel/otel-ucp.pre-activate
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + targets+=("${target}")
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + echo -e '/tmp/dataproc/sentinel/otel-ucp.pre-activate: '
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + echo -e '\t@echo '\''Running task: otel-ucp.pre-activate'\'''
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + echo -e '\tbash -e /usr/local/share/google/dataproc/bdutil/components/pre-activate/otel-ucp.sh 2.2 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/otel-ucp.sh"; exit 1; }'
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + echo -e '\ttouch /tmp/dataproc/sentinel/otel-ucp.pre-activate\n'
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + (( i++ ))
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + (( i < 10 ))
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local name=pig.pre-activate
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/pig.sh
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local deps_manifest=/usr/local/share/google/dataproc/bdutil/components/pre-activate/pig.deps
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + deps=()
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local deps
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/pig.deps ]]
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local target=/tmp/dataproc/sentinel/pig.pre-activate
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + targets+=("${target}")
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + echo -e '/tmp/dataproc/sentinel/pig.pre-activate: '
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + echo -e '\t@echo '\''Running task: pig.pre-activate'\'''
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + echo -e '\tbash -e /usr/local/share/google/dataproc/bdutil/components/pre-activate/pig.sh 2.2 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/pig.sh"; exit 1; }'
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + echo -e '\ttouch /tmp/dataproc/sentinel/pig.pre-activate\n'
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + (( i++ ))
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + (( i < 10 ))
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local name=spark.pre-activate
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local deps_manifest=/usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.deps
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + deps=()
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local deps
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.deps ]]
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local target=/tmp/dataproc/sentinel/spark.pre-activate
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + targets+=("${target}")
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + echo -e '/tmp/dataproc/sentinel/spark.pre-activate: '
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + echo -e '\t@echo '\''Running task: spark.pre-activate'\'''
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + echo -e '\tbash -e /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh 2.2 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh"; exit 1; }'
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + echo -e '\ttouch /tmp/dataproc/sentinel/spark.pre-activate\n'
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + (( i++ ))
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + (( i < 10 ))
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local name=tez.pre-activate
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/tez.sh
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local deps_manifest=/usr/local/share/google/dataproc/bdutil/components/pre-activate/tez.deps
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + deps=()
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local deps
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/tez.deps ]]
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local target=/tmp/dataproc/sentinel/tez.pre-activate
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + targets+=("${target}")
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + echo -e '/tmp/dataproc/sentinel/tez.pre-activate: '
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + echo -e '\t@echo '\''Running task: tez.pre-activate'\'''
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + echo -e '\tbash -e /usr/local/share/google/dataproc/bdutil/components/pre-activate/tez.sh 2.2 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/tez.sh"; exit 1; }'
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + echo -e '\ttouch /tmp/dataproc/sentinel/tez.pre-activate\n'
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + (( i++ ))
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + (( i < 10 ))
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local name=yarn.pre-activate
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/yarn.sh
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local deps_manifest=/usr/local/share/google/dataproc/bdutil/components/pre-activate/yarn.deps
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + deps=()
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local deps
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/yarn.deps ]]
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local target=/tmp/dataproc/sentinel/yarn.pre-activate
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + targets+=("${target}")
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + echo -e '/tmp/dataproc/sentinel/yarn.pre-activate: '
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + echo -e '\t@echo '\''Running task: yarn.pre-activate'\'''
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + echo -e '\tbash -e /usr/local/share/google/dataproc/bdutil/components/pre-activate/yarn.sh 2.2 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/yarn.sh"; exit 1; }'
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + echo -e '\ttouch /tmp/dataproc/sentinel/yarn.pre-activate\n'
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + (( i++ ))
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + (( i < 10 ))
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + echo -e 'all: /tmp/dataproc/sentinel/fluentbit-ucp.pre-activate /tmp/dataproc/sentinel/hdfs.pre-activate /tmp/dataproc/sentinel/hive-metastore.pre-activate /tmp/dataproc/sentinel/hive-server2.pre-activate /tmp/dataproc/sentinel/mysql.pre-activate /tmp/dataproc/sentinel/otel-ucp.pre-activate /tmp/dataproc/sentinel/pig.pre-activate /tmp/dataproc/sentinel/spark.pre-activate /tmp/dataproc/sentinel/tez.pre-activate /tmp/dataproc/sentinel/yarn.pre-activate'
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + echo 'Generated makefile:'
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: Generated makefile:
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + cat /tmp/dataproc/make/makefile.datlvq
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: /tmp/dataproc/sentinel/fluentbit-ucp.pre-activate: 
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: 	@echo 'Running task: fluentbit-ucp.pre-activate'
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: 	bash -e /usr/local/share/google/dataproc/bdutil/components/pre-activate/fluentbit-ucp.sh 2.2 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/fluentbit-ucp.sh"; exit 1; }
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: 	touch /tmp/dataproc/sentinel/fluentbit-ucp.pre-activate
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: 
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: /tmp/dataproc/sentinel/hdfs.pre-activate: 
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: 	@echo 'Running task: hdfs.pre-activate'
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: 	bash -e /usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.sh 2.2 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.sh"; exit 1; }
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: 	touch /tmp/dataproc/sentinel/hdfs.pre-activate
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: 
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: /tmp/dataproc/sentinel/hive-metastore.pre-activate: 
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: 	@echo 'Running task: hive-metastore.pre-activate'
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: 	bash -e /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.sh 2.2 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.sh"; exit 1; }
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: 	touch /tmp/dataproc/sentinel/hive-metastore.pre-activate
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: 
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: /tmp/dataproc/sentinel/hive-server2.pre-activate: /tmp/dataproc/sentinel/mysql.pre-activate /tmp/dataproc/sentinel/hdfs.pre-activate /tmp/dataproc/sentinel/hive-metastore.pre-activate /tmp/dataproc/sentinel/tez.pre-activate
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: 	@echo 'Running task: hive-server2.pre-activate'
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: 	bash -e /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.sh 2.2 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.sh"; exit 1; }
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: 	touch /tmp/dataproc/sentinel/hive-server2.pre-activate
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: 
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: /tmp/dataproc/sentinel/mysql.pre-activate: 
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: 	@echo 'Running task: mysql.pre-activate'
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: 	bash -e /usr/local/share/google/dataproc/bdutil/components/pre-activate/mysql.sh 2.2 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/mysql.sh"; exit 1; }
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: 	touch /tmp/dataproc/sentinel/mysql.pre-activate
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: 
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: /tmp/dataproc/sentinel/otel-ucp.pre-activate: 
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: 	@echo 'Running task: otel-ucp.pre-activate'
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: 	bash -e /usr/local/share/google/dataproc/bdutil/components/pre-activate/otel-ucp.sh 2.2 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/otel-ucp.sh"; exit 1; }
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: 	touch /tmp/dataproc/sentinel/otel-ucp.pre-activate
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: 
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: /tmp/dataproc/sentinel/pig.pre-activate: 
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: 	@echo 'Running task: pig.pre-activate'
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: 	bash -e /usr/local/share/google/dataproc/bdutil/components/pre-activate/pig.sh 2.2 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/pig.sh"; exit 1; }
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: 	touch /tmp/dataproc/sentinel/pig.pre-activate
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: 
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: /tmp/dataproc/sentinel/spark.pre-activate: 
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: 	@echo 'Running task: spark.pre-activate'
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: 	bash -e /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh 2.2 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh"; exit 1; }
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: 	touch /tmp/dataproc/sentinel/spark.pre-activate
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: 
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: /tmp/dataproc/sentinel/tez.pre-activate: 
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: 	@echo 'Running task: tez.pre-activate'
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: 	bash -e /usr/local/share/google/dataproc/bdutil/components/pre-activate/tez.sh 2.2 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/tez.sh"; exit 1; }
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: 	touch /tmp/dataproc/sentinel/tez.pre-activate
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: 
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: /tmp/dataproc/sentinel/yarn.pre-activate: 
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: 	@echo 'Running task: yarn.pre-activate'
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: 	bash -e /usr/local/share/google/dataproc/bdutil/components/pre-activate/yarn.sh 2.2 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/yarn.sh"; exit 1; }
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: 	touch /tmp/dataproc/sentinel/yarn.pre-activate
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: 
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: all: /tmp/dataproc/sentinel/fluentbit-ucp.pre-activate /tmp/dataproc/sentinel/hdfs.pre-activate /tmp/dataproc/sentinel/hive-metastore.pre-activate /tmp/dataproc/sentinel/hive-server2.pre-activate /tmp/dataproc/sentinel/mysql.pre-activate /tmp/dataproc/sentinel/otel-ucp.pre-activate /tmp/dataproc/sentinel/pig.pre-activate /tmp/dataproc/sentinel/spark.pre-activate /tmp/dataproc/sentinel/tez.pre-activate /tmp/dataproc/sentinel/yarn.pre-activate
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + echo 'Running task graph:'
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: Running task graph:
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + make -f /tmp/dataproc/make/makefile.datlvq all
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: Running task: fluentbit-ucp.pre-activate
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: bash -e /usr/local/share/google/dataproc/bdutil/components/pre-activate/fluentbit-ucp.sh 2.2 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/fluentbit-ucp.sh"; exit 1; }
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: ++ get_dataproc_property dataproc.logging.stackdriver.enable
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + STACKDRIVER_LOGGING_ENABLED=
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + [[ '' == \f\a\l\s\e ]]
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + is_legacy_containerized_agents_enabled
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: ++ get_dataproc_property_or_default dataproc.observability.containerised.legacy.agents.enable false
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local -r legacy_containerised_agents_enabled=false
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + [[ false == \t\r\u\e ]]
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + set_log_tag pre-activate-component-fluentbit-ucp
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + local -r tag=dataproc-pre-activate-component-fluentbit-ucp
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: + exec
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: ++ logger -s -t 'dataproc-pre-activate-component-fluentbit-ucp[2804]'
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: <13>Apr 27 18:07:33 dataproc-pre-activate-component-fluentbit-ucp[2804]: + UCP_OBSERVABILITY_BASE_PATH=/etc/ucp
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: <13>Apr 27 18:07:33 dataproc-pre-activate-component-fluentbit-ucp[2804]: + UCP_LOGGING_AGENT_PATH=/etc/ucp/logging
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: <13>Apr 27 18:07:33 dataproc-pre-activate-component-fluentbit-ucp[2804]: + FLUENTBIT_CONF_DIR=/etc/ucp/logging/configs
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: <13>Apr 27 18:07:33 dataproc-pre-activate-component-fluentbit-ucp[2804]: + configure_ucp_logging_agent
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: <13>Apr 27 18:07:33 dataproc-pre-activate-component-fluentbit-ucp[2804]: + local -r fluentbit_service_file=/etc/systemd/system/fluentbit-ucp.service
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: <13>Apr 27 18:07:33 dataproc-pre-activate-component-fluentbit-ucp[2804]: + cat
<13>Apr 27 18:07:33 dataproc-startup-script[1265]: <13>Apr 27 18:07:33 dataproc-pre-activate-component-fluentbit-ucp[2804]: + systemctl daemon-reload
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + systemctl enable fluentbit-ucp
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: Created symlink /etc/systemd/system/multi-user.target.wants/fluentbit-ucp.service → /etc/systemd/system/fluentbit-ucp.service.
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + setup_fluentbit_confs
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + mkdir -p /etc/ucp/logging/configs
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + cp -r /usr/local/share/google/dataproc/bdutil/fluentbit/append_resource_labels.lua /usr/local/share/google/dataproc/bdutil/fluentbit/dpms_proxy_fluentbit.conf /usr/local/share/google/dataproc/bdutil/fluentbit/fluentbit.conf /usr/local/share/google/dataproc/bdutil/fluentbit/hadoop_fluentbit.conf /usr/local/share/google/dataproc/bdutil/fluentbit/job-driverlogs-fluentbit.conf /usr/local/share/google/dataproc/bdutil/fluentbit/job-yarn-containerlogs-fluentbit.conf /usr/local/share/google/dataproc/bdutil/fluentbit/optional_logging /usr/local/share/google/dataproc/bdutil/fluentbit/parsers.conf /usr/local/share/google/dataproc/bdutil/fluentbit/yarn-userlogs-fluentbit.conf /etc/ucp/logging/configs
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + substitute_conf_placeholders
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: ++ get_dataproc_property dataproc.logging.stackdriver.job.driver.enable
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: ++ set +x
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + local -r job_driver_logging_enabled=
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + local include_job_driver_logs=
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + [[ '' == \t\r\u\e ]]
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + sed -i -e s#INCLUDE_JOB_DRIVER_LOGS_PLACEHOLDER## /etc/ucp/logging/configs/fluentbit.conf
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: ++ get_dataproc_property dataproc.logging.stackdriver.job.yarn.container.enable
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: ++ set +x
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + local -r container_logging_enabled=
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + local 'include_yarn_user_or_jobcontainer_logs=@INCLUDE yarn-userlogs-fluentbit.conf'
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + [[ '' == \t\r\u\e ]]
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + sed -i -e 's#INCLUDE_YARN_USER_OR_JOBCONTAINER_LOGS_PLACEHOLDER#@INCLUDE yarn-userlogs-fluentbit.conf#' /etc/ucp/logging/configs/fluentbit.conf
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + local include_dpms_proxy_logs=
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + is_component_selected dpms-proxy
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + local -r component=dpms-proxy
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + local activated_components
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: ++ get_components_to_activate
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: ++ get_dataproc_property dataproc.components.activate
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: ++ set +x
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: ++ tr '[:upper:]' '[:lower:]'
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + activated_components='knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3'
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + [[ knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3 == *dpms-proxy* ]]
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + sed -i -e s#INCLUDE_DPMS_PROXY_LOGS_PLACEHOLDER## /etc/ucp/logging/configs/fluentbit.conf
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: ++ get_dataproc_property_or_default dataproc.cluster.caching.enabled false
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: ++ set +x
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + local -r is_caching_enabled=false
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + local include_cluster_caching_logs=
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + [[ false == \t\r\u\e ]]
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + sed -i -e s#INCLUDE_CLUSTER_CACHING_LOGS_PLACEHOLDER## /etc/ucp/logging/configs/fluentbit.conf
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: ++ get_dataproc_property_or_default internal.node.main.memory-protection-worker.enabled false
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: ++ set +x
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + local -r worker_earlyoom_enabled=true
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + local include_earlyoom_logs=
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + [[ Master == \W\o\r\k\e\r ]]
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + sed -i -e s#INCLUDE_EARLYOOM_LOGS_PLACEHOLDER## /etc/ucp/logging/configs/fluentbit.conf
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + substitute_optional_hadoop_dirs
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + local optional_components_logs=
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + is_component_selected presto
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + local -r component=presto
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + local activated_components
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: ++ get_components_to_activate
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: ++ get_dataproc_property dataproc.components.activate
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: ++ set +x
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: ++ tr '[:upper:]' '[:lower:]'
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + activated_components='knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3'
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + [[ knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3 == *presto* ]]
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + is_component_selected trino
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + local -r component=trino
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + local activated_components
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: ++ get_components_to_activate
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: ++ get_dataproc_property dataproc.components.activate
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: ++ tr '[:upper:]' '[:lower:]'
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: ++ set +x
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + activated_components='knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3'
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + [[ knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3 == *trino* ]]
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + is_component_selected flink
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + local -r component=flink
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + local activated_components
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: ++ get_components_to_activate
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: ++ get_dataproc_property dataproc.components.activate
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: ++ tr '[:upper:]' '[:lower:]'
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: ++ set +x
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + activated_components='knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3'
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + [[ knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3 == *flink* ]]
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + is_component_selected druid
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + local -r component=druid
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + local activated_components
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: ++ get_components_to_activate
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: ++ get_dataproc_property dataproc.components.activate
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: ++ tr '[:upper:]' '[:lower:]'
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: ++ set +x
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + activated_components='knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3'
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + [[ knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3 == *druid* ]]
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + is_component_selected hbase
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + local -r component=hbase
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + local activated_components
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: ++ get_components_to_activate
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: ++ get_dataproc_property dataproc.components.activate
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: ++ set +x
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: ++ tr '[:upper:]' '[:lower:]'
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + activated_components='knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3'
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + [[ knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3 == *hbase* ]]
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + is_component_selected ranger
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + local -r component=ranger
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + local activated_components
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: ++ get_components_to_activate
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: ++ get_dataproc_property dataproc.components.activate
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: ++ set +x
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: ++ tr '[:upper:]' '[:lower:]'
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + activated_components='knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3'
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + [[ knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3 == *ranger* ]]
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + is_component_selected solr
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + local -r component=solr
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + local activated_components
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: ++ get_components_to_activate
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: ++ get_dataproc_property dataproc.components.activate
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: ++ set +x
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: ++ tr '[:upper:]' '[:lower:]'
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + activated_components='knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3'
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + [[ knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3 == *solr* ]]
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + sed -i -e s#OPTIONAL_COMPONENTS_PLACEHOLDER## /etc/ucp/logging/configs/hadoop_fluentbit.conf
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + mkdir -p /var/log/google-dataproc-job
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + mkdir -p /var/log/cloud-sql-proxy
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + mkdir -p /var/log/hadoop-yarn
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + chown yarn:hadoop -R /var/log/hadoop-yarn
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + mkdir -p /var/log/hadoop-yarn/userlogs
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + chown yarn:yarn -R /var/log/hadoop-yarn/userlogs
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + configure_extended_logging
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + local include_extended_logs=
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + is_extended_logging_enabled
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + local extended_logging_enabled=
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + [[ standard == \s\t\a\n\d\a\r\d ]]
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: ++ get_dataproc_property_or_default dataproc.logging.extended.enabled false
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: ++ set +x
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + extended_logging_enabled=false
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + [[ false == \t\r\u\e ]]
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + sed -i -e s#INCLUDE_EXTENDED_LOGS_PLACEHOLDER## /etc/ucp/logging/configs/fluentbit.conf
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + configure_syslog
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + local include_syslogs=
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + is_syslog_logging_enabled
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + local syslog_enabled
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + [[ standard == \s\t\a\n\d\a\r\d ]]
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: ++ get_dataproc_property_or_default dataproc.logging.syslog.enabled false
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: ++ set +x
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + syslog_enabled=false
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + [[ false == \t\r\u\e ]]
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + sed -i -e s#INCLUDE_SYSLOG_PLACEHOLDER## /etc/ucp/logging/configs/fluentbit.conf
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + configure_cloud_logging_base_url
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + cloud_logging_base_url=
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: ++ is_tpc
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: +++ get_universe_domain
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: ++++ gcloud config get core/universe_domain
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: +++ local -r universe_domain=googleapis.com
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: +++ echo googleapis.com
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: ++ [[ googleapis.com == \g\o\o\g\l\e\a\p\i\s\.\c\o\m ]]
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: ++ return 0
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + [[ -n '' ]]
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + sed -i -e s#CLOUD_LOGGING_BASE_URL_PLACEHOLDER## /etc/ucp/logging/configs/fluentbit.conf
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-fluentbit-ucp[2804]: + sed -i -e s#CLOUD_LOGGING_BASE_URL_PLACEHOLDER## /etc/ucp/logging/configs/optional_logging/dataproc-syslog.conf
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: touch /tmp/dataproc/sentinel/fluentbit-ucp.pre-activate
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: Running task: hdfs.pre-activate
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: bash -e /usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.sh 2.2 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.sh"; exit 1; }
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: + set_log_tag pre-activate-component-hdfs
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: + local -r tag=dataproc-pre-activate-component-hdfs
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: + exec
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: ++ logger -s -t 'dataproc-pre-activate-component-hdfs[3032]'
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + readonly HDFS_ADMIN=hdfs
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + HDFS_ADMIN=hdfs
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + readonly HDFS_SITE_XML=/etc/hadoop/conf/hdfs-site.xml
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + HDFS_SITE_XML=/etc/hadoop/conf/hdfs-site.xml
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + readonly CLUSTER_PROPS_DIR=/tmp/cluster/properties
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + CLUSTER_PROPS_DIR=/tmp/cluster/properties
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + export HDFS_NAME_DIR=/hadoop/dfs/name
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + HDFS_NAME_DIR=/hadoop/dfs/name
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + export HDFS_SECONDARY_NAME_DIR=/hadoop/dfs/namesecondary
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + HDFS_SECONDARY_NAME_DIR=/hadoop/dfs/namesecondary
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: ++ get_metadata_cluster_name
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: ++ get_dataproc_metadata DATAPROC_METADATA_CLUSTER_NAME attributes/dataproc-cluster-name
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: ++ set +x
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + CLUSTER_NAME=cluster-dip-01
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + export CLUSTER_NAME
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: ++ get_metadata_master
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: ++ get_dataproc_metadata DATAPROC_METADATA_MASTER attributes/dataproc-master
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: ++ set +x
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + DATAPROC_MASTER=cluster-dip-01-m
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: ++ get_metadata_master_additional
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: ++ get_dataproc_metadata DATAPROC_METADATA_MASTER_ADDITIONAL attributes/dataproc-master-additional
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: ++ set +x
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + DATAPROC_MASTER_ADDITIONAL=
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + MASTER_HOSTNAMES=($DATAPROC_MASTER ${DATAPROC_MASTER_ADDITIONAL//,/ })
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + NUM_MASTERS=1
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: ++ get_metadata_worker_count
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: ++ get_dataproc_metadata DATAPROC_METADATA_WORKER_COUNT attributes/dataproc-worker-count
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: ++ set +x
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + NUM_WORKERS=3
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + DATANODE_PACKAGES=('hadoop-hdfs-datanode')
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + THIRD_MASTER_OPTIONAL_PACKAGES=('hadoop-hdfs-namenode' 'hadoop-hdfs-zkfc')
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + (( i = 0 ))
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + (( i < 1 ))
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + declare MASTER_HOSTNAME_0=cluster-dip-01-m
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + export MASTER_HOSTNAME_0
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + (( i++ ))
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + (( i < 1 ))
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: ++ is_component_selected kerberos
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: ++ local -r component=kerberos
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: ++ local activated_components
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: +++ get_components_to_activate
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: +++ get_dataproc_property dataproc.components.activate
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: +++ set +x
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: +++ tr '[:upper:]' '[:lower:]'
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: ++ activated_components='knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3'
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: ++ [[ knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3 == *kerberos* ]]
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: ++ echo false
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + KERBEROS_ENABLED=false
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + export ENABLE_HDFS_PERMISSIONS=false
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + ENABLE_HDFS_PERMISSIONS=false
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + DATA_DIRS_ARRAY=($(get_data_dirs))
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: ++ get_data_dirs
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: ++ local -a mount_points
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: ++ mapfile -t mount_points
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: +++ find '/mnt/[0-9]*/' -maxdepth 0
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: find: ‘/mnt/[0-9]*/’: No such file or directory
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: +++ true
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: ++ (( 0 ))
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: ++ echo /
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: ++ return
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + HDFS_DIRS=/hadoop/dfs
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + HDFS_DIRS_ARRAY=(${HDFS_DIRS})
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + HDFS_DATA_DIRS=/hadoop/dfs/data
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + HDFS_DATA_DIRS_ARRAY=(${HDFS_DATA_DIRS})
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + mkdir -p /hadoop/dfs /hadoop/dfs/data
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + chown -L -R hdfs:hadoop /hadoop/dfs /hadoop/dfs
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + chmod -R 700 /hadoop/dfs
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: ++ free -m
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: ++ awk '/^Mem:/{print $2}'
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + TOTAL_MEM=7949
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: ++ python -c 'print(int(7949 * 0.4 / 2))'
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + NAMENODE_MEM_MB=1589
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + SECONDARYNAMENODE_MEM_MB=1589
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + is_version_at_least 2.2 2.1
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + local reenable_x=false
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + [[ -o xtrace ]]
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + set +x
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + case ${compare_versions_result} in
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + return 0
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + GC_LOG_OPTS='-Xlog:gc*:stdout:time,level,tags'
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: ++ grep GC_OPTS /tmp/cluster/properties/hadoop-env.sh
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: ++ cut -d= -f2
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: ++ head -n1
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: ++ :
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + USER_SUPPLIED_GC_OPTS=
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + [[ -z '' ]]
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + is_version_at_least 2.2 3.0
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + local reenable_x=false
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + [[ -o xtrace ]]
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + set +x
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + case ${compare_versions_result} in
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + return 1
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + GC_OPTS=-XX:+UseConcMarkSweepGC
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + GC_JDK17_OPTS=
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + is_version_at_least 2.2 3.0
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + local reenable_x=false
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + [[ -o xtrace ]]
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + set +x
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + case ${compare_versions_result} in
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + return 1
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + cat
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + export HDFS_DATA_DIRS=/hadoop/dfs/data
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + HDFS_DATA_DIRS=/hadoop/dfs/data
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + [[ 1 -gt 1 ]]
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + readonly HDFS_TEMPLATE=hdfs-template.xml
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + HDFS_TEMPLATE=hdfs-template.xml
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/hdfs-site.xml --source_configuration_file hdfs-template.xml --resolve_environment_variables --create_if_absent --clobber
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + (( NUM_WORKERS == 0 ))
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + [[ 1 -gt 1 ]]
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + readonly HDFS_SIMPLIFICATION_MIXINS=hdfs-simplification-mixins.xml
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + HDFS_SIMPLIFICATION_MIXINS=hdfs-simplification-mixins.xml
<13>Apr 27 18:07:34 dataproc-startup-script[1265]: <13>Apr 27 18:07:34 dataproc-pre-activate-component-hdfs[3032]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/hdfs-site.xml --source_configuration_file /usr/local/share/google/dataproc/bdutil/conf/hdfs-simplification-mixins.xml --resolve_environment_variables --create_if_absent --clobber
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + update_hostname_in_xml
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + local host_name
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + local domain
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: ++ dnsdomainname
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + domain=c.euphoric-coral-451717-v8.internal
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + host_name=cluster-dip-01-m.c.euphoric-coral-451717-v8.internal
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + update_hostname_for_property /etc/hadoop/conf/hdfs-site.xml dfs.namenode.http-address cluster-dip-01-m.c.euphoric-coral-451717-v8.internal
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + local xml_file=/etc/hadoop/conf/hdfs-site.xml
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + local prop_name=dfs.namenode.http-address
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + local host_name=cluster-dip-01-m.c.euphoric-coral-451717-v8.internal
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + local updated_value
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + local prop_value
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: ++ bdconfig get_property_value --configuration_file /etc/hadoop/conf/hdfs-site.xml --name dfs.namenode.http-address
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: I0427 18:07:35.124855 139813224993472 xml_config_commands.py:181] Property value is: "cluster-dip-01-m:50070"
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + prop_value=cluster-dip-01-m:50070
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + [[ cluster-dip-01-m:50070 != \N\o\n\e ]]
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + loginfo 'Reading dfs.namenode.http-address in /etc/hadoop/conf/hdfs-site.xml'
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + echo 'Reading dfs.namenode.http-address in /etc/hadoop/conf/hdfs-site.xml'
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: Reading dfs.namenode.http-address in /etc/hadoop/conf/hdfs-site.xml
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: ++ echo cluster-dip-01-m:50070
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: ++ sed -e s/0.0.0.0/cluster-dip-01-m.c.euphoric-coral-451717-v8.internal/g
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + updated_value=cluster-dip-01-m:50070
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + [[ cluster-dip-01-m:50070 != \c\l\u\s\t\e\r\-\d\i\p\-\0\1\-\m\:\5\0\0\7\0 ]]
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + update_hostname_for_property /etc/hadoop/conf/hdfs-site.xml dfs.namenode.https-address cluster-dip-01-m.c.euphoric-coral-451717-v8.internal
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + local xml_file=/etc/hadoop/conf/hdfs-site.xml
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + local prop_name=dfs.namenode.https-address
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + local host_name=cluster-dip-01-m.c.euphoric-coral-451717-v8.internal
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + local updated_value
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + local prop_value
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: ++ bdconfig get_property_value --configuration_file /etc/hadoop/conf/hdfs-site.xml --name dfs.namenode.https-address
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: I0427 18:07:35.208065 139621026243264 xml_config_commands.py:181] Property value is: "None"
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + prop_value=None
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + [[ None != \N\o\n\e ]]
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + update_hostname_for_property /etc/hadoop/conf/hdfs-site.xml dfs.namenode.secondary.http-address cluster-dip-01-m.c.euphoric-coral-451717-v8.internal
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + local xml_file=/etc/hadoop/conf/hdfs-site.xml
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + local prop_name=dfs.namenode.secondary.http-address
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + local host_name=cluster-dip-01-m.c.euphoric-coral-451717-v8.internal
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + local updated_value
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + local prop_value
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: ++ bdconfig get_property_value --configuration_file /etc/hadoop/conf/hdfs-site.xml --name dfs.namenode.secondary.http-address
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: I0427 18:07:35.289584 140390005203648 xml_config_commands.py:181] Property value is: "cluster-dip-01-m:50090"
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + prop_value=cluster-dip-01-m:50090
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + [[ cluster-dip-01-m:50090 != \N\o\n\e ]]
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + loginfo 'Reading dfs.namenode.secondary.http-address in /etc/hadoop/conf/hdfs-site.xml'
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + echo 'Reading dfs.namenode.secondary.http-address in /etc/hadoop/conf/hdfs-site.xml'
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: Reading dfs.namenode.secondary.http-address in /etc/hadoop/conf/hdfs-site.xml
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: ++ echo cluster-dip-01-m:50090
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: ++ sed -e s/0.0.0.0/cluster-dip-01-m.c.euphoric-coral-451717-v8.internal/g
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + updated_value=cluster-dip-01-m:50090
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + [[ cluster-dip-01-m:50090 != \c\l\u\s\t\e\r\-\d\i\p\-\0\1\-\m\:\5\0\0\9\0 ]]
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + update_hostname_for_property /etc/hadoop/conf/hdfs-site.xml dfs.namenode.secondary.https-address cluster-dip-01-m.c.euphoric-coral-451717-v8.internal
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + local xml_file=/etc/hadoop/conf/hdfs-site.xml
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + local prop_name=dfs.namenode.secondary.https-address
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + local host_name=cluster-dip-01-m.c.euphoric-coral-451717-v8.internal
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + local updated_value
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + local prop_value
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: ++ bdconfig get_property_value --configuration_file /etc/hadoop/conf/hdfs-site.xml --name dfs.namenode.secondary.https-address
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: I0427 18:07:35.373970 140170152694464 xml_config_commands.py:181] Property value is: "None"
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + prop_value=None
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + [[ None != \N\o\n\e ]]
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: ++ hostname -f
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + host_name=cluster-dip-01-m.c.euphoric-coral-451717-v8.internal
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + update_hostname_for_property /etc/hadoop/conf/hdfs-site.xml dfs.datanode.address cluster-dip-01-m.c.euphoric-coral-451717-v8.internal
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + local xml_file=/etc/hadoop/conf/hdfs-site.xml
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + local prop_name=dfs.datanode.address
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + local host_name=cluster-dip-01-m.c.euphoric-coral-451717-v8.internal
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + local updated_value
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + local prop_value
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: ++ bdconfig get_property_value --configuration_file /etc/hadoop/conf/hdfs-site.xml --name dfs.datanode.address
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: I0427 18:07:35.456805 139938400506560 xml_config_commands.py:181] Property value is: "None"
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + prop_value=None
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + [[ None != \N\o\n\e ]]
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + update_hostname_for_property /etc/hadoop/conf/hdfs-site.xml dfs.datanode.http.address cluster-dip-01-m.c.euphoric-coral-451717-v8.internal
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + local xml_file=/etc/hadoop/conf/hdfs-site.xml
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + local prop_name=dfs.datanode.http.address
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + local host_name=cluster-dip-01-m.c.euphoric-coral-451717-v8.internal
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + local updated_value
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + local prop_value
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: ++ bdconfig get_property_value --configuration_file /etc/hadoop/conf/hdfs-site.xml --name dfs.datanode.http.address
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: I0427 18:07:35.539279 140130749014720 xml_config_commands.py:181] Property value is: "None"
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + prop_value=None
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + [[ None != \N\o\n\e ]]
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + update_hostname_for_property /etc/hadoop/conf/hdfs-site.xml dfs.datanode.https.address cluster-dip-01-m.c.euphoric-coral-451717-v8.internal
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + local xml_file=/etc/hadoop/conf/hdfs-site.xml
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + local prop_name=dfs.datanode.https.address
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + local host_name=cluster-dip-01-m.c.euphoric-coral-451717-v8.internal
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + local updated_value
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + local prop_value
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: ++ bdconfig get_property_value --configuration_file /etc/hadoop/conf/hdfs-site.xml --name dfs.datanode.https.address
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: I0427 18:07:35.621216 139802632041152 xml_config_commands.py:181] Property value is: "None"
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + prop_value=None
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + [[ None != \N\o\n\e ]]
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + update_hostname_for_property /etc/hadoop/conf/hdfs-site.xml dfs.datanode.ipc.address cluster-dip-01-m.c.euphoric-coral-451717-v8.internal
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + local xml_file=/etc/hadoop/conf/hdfs-site.xml
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + local prop_name=dfs.datanode.ipc.address
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + local host_name=cluster-dip-01-m.c.euphoric-coral-451717-v8.internal
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + local updated_value
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + local prop_value
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: ++ bdconfig get_property_value --configuration_file /etc/hadoop/conf/hdfs-site.xml --name dfs.datanode.ipc.address
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: I0427 18:07:35.701777 140696517198528 xml_config_commands.py:181] Property value is: "None"
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + prop_value=None
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + [[ None != \N\o\n\e ]]
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + update_hdfs_param_values
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + bdconfig set_property --configuration_file /etc/hadoop/conf/hdfs-site.xml --name dfs.namenode.replication.work.multiplier.per.iteration --value 20 --clobber
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + bdconfig set_property --configuration_file /etc/hadoop/conf/hdfs-site.xml --name dfs.namenode.replication.max-streams --value 20 --clobber
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + bdconfig set_property --configuration_file /etc/hadoop/conf/hdfs-site.xml --name dfs.namenode.replication.max-streams-hard-limit --value 40 --clobber
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + merge_xml_properties /tmp/cluster/properties/hdfs.xml /etc/hadoop/conf/hdfs-site.xml
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + local src=/tmp/cluster/properties/hdfs.xml
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + local dest=/etc/hadoop/conf/hdfs-site.xml
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + [[ ! -f /tmp/cluster/properties/hdfs.xml ]]
<13>Apr 27 18:07:35 dataproc-startup-script[1265]: <13>Apr 27 18:07:35 dataproc-pre-activate-component-hdfs[3032]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/hdfs-site.xml --source_configuration_file /tmp/cluster/properties/hdfs.xml --resolve_environment_variables --create_if_absent --clobber
<13>Apr 27 18:07:36 dataproc-startup-script[1265]: <13>Apr 27 18:07:36 dataproc-pre-activate-component-hdfs[3032]: + loginfo 'Merged /tmp/cluster/properties/hdfs.xml.'
<13>Apr 27 18:07:36 dataproc-startup-script[1265]: <13>Apr 27 18:07:36 dataproc-pre-activate-component-hdfs[3032]: + echo 'Merged /tmp/cluster/properties/hdfs.xml.'
<13>Apr 27 18:07:36 dataproc-startup-script[1265]: <13>Apr 27 18:07:36 dataproc-pre-activate-component-hdfs[3032]: Merged /tmp/cluster/properties/hdfs.xml.
<13>Apr 27 18:07:36 dataproc-startup-script[1265]: <13>Apr 27 18:07:36 dataproc-pre-activate-component-hdfs[3032]: ++ create_or_validate_include_file_path
<13>Apr 27 18:07:36 dataproc-startup-script[1265]: <13>Apr 27 18:07:36 dataproc-pre-activate-component-hdfs[3032]: ++ local include_path
<13>Apr 27 18:07:36 dataproc-startup-script[1265]: <13>Apr 27 18:07:36 dataproc-pre-activate-component-hdfs[3032]: +++ get_include_file_path
<13>Apr 27 18:07:36 dataproc-startup-script[1265]: <13>Apr 27 18:07:36 dataproc-pre-activate-component-hdfs[3032]: +++ local include_path
<13>Apr 27 18:07:36 dataproc-startup-script[1265]: <13>Apr 27 18:07:36 dataproc-pre-activate-component-hdfs[3032]: ++++ /usr/share/google/get_metadata_value attributes/dataproc-include-file-location
<13>Apr 27 18:07:36 dataproc-startup-script[1265]: <13>Apr 27 18:07:36 dataproc-pre-activate-component-hdfs[3032]: +++ include_path=gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_include
<13>Apr 27 18:07:36 dataproc-startup-script[1265]: <13>Apr 27 18:07:36 dataproc-pre-activate-component-hdfs[3032]: +++ [[ -z gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_include ]]
<13>Apr 27 18:07:36 dataproc-startup-script[1265]: <13>Apr 27 18:07:36 dataproc-pre-activate-component-hdfs[3032]: +++ echo gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_include
<13>Apr 27 18:07:36 dataproc-startup-script[1265]: <13>Apr 27 18:07:36 dataproc-pre-activate-component-hdfs[3032]: ++ include_path=gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_include
<13>Apr 27 18:07:36 dataproc-startup-script[1265]: <13>Apr 27 18:07:36 dataproc-pre-activate-component-hdfs[3032]: ++ [[ gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_include == gs://* ]]
<13>Apr 27 18:07:36 dataproc-startup-script[1265]: <13>Apr 27 18:07:36 dataproc-pre-activate-component-hdfs[3032]: ++ loginfo 'Checking if include membership files exist in GCS: gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_include'
<13>Apr 27 18:07:36 dataproc-startup-script[1265]: <13>Apr 27 18:07:36 dataproc-pre-activate-component-hdfs[3032]: ++ echo 'Checking if include membership files exist in GCS: gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_include'
<13>Apr 27 18:07:36 dataproc-startup-script[1265]: <13>Apr 27 18:07:36 dataproc-pre-activate-component-hdfs[3032]: Checking if include membership files exist in GCS: gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_include
<13>Apr 27 18:07:36 dataproc-startup-script[1265]: <13>Apr 27 18:07:36 dataproc-pre-activate-component-hdfs[3032]: ++ retry_constant_custom 10 3 gcs_file_exists gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_include
<13>Apr 27 18:07:36 dataproc-startup-script[1265]: <13>Apr 27 18:07:36 dataproc-pre-activate-component-hdfs[3032]: ++ local -r max_retry_time=10
<13>Apr 27 18:07:36 dataproc-startup-script[1265]: <13>Apr 27 18:07:36 dataproc-pre-activate-component-hdfs[3032]: ++ local -r retry_delay=3
<13>Apr 27 18:07:36 dataproc-startup-script[1265]: <13>Apr 27 18:07:36 dataproc-pre-activate-component-hdfs[3032]: ++ cmd=('gcs_file_exists' 'gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_include')
<13>Apr 27 18:07:36 dataproc-startup-script[1265]: <13>Apr 27 18:07:36 dataproc-pre-activate-component-hdfs[3032]: ++ local -r cmd
<13>Apr 27 18:07:36 dataproc-startup-script[1265]: <13>Apr 27 18:07:36 dataproc-pre-activate-component-hdfs[3032]: ++ local -r max_retries=3
<13>Apr 27 18:07:36 dataproc-startup-script[1265]: <13>Apr 27 18:07:36 dataproc-pre-activate-component-hdfs[3032]: ++ local reenable_x=false
<13>Apr 27 18:07:36 dataproc-startup-script[1265]: <13>Apr 27 18:07:36 dataproc-pre-activate-component-hdfs[3032]: ++ [[ -o xtrace ]]
<13>Apr 27 18:07:36 dataproc-startup-script[1265]: <13>Apr 27 18:07:36 dataproc-pre-activate-component-hdfs[3032]: ++ set +x
<13>Apr 27 18:07:36 dataproc-startup-script[1265]: <13>Apr 27 18:07:36 dataproc-pre-activate-component-hdfs[3032]: About to run 'gcs_file_exists gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_include' with retries...
<13>Apr 27 18:07:37 dataproc-startup-script[1265]: <13>Apr 27 18:07:37 dataproc-pre-activate-component-hdfs[3032]: 'gcs_file_exists gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_include' succeeded after 1 execution(s).
<13>Apr 27 18:07:37 dataproc-startup-script[1265]: <13>Apr 27 18:07:37 dataproc-pre-activate-component-hdfs[3032]: ++ return 0
<13>Apr 27 18:07:37 dataproc-startup-script[1265]: <13>Apr 27 18:07:37 dataproc-pre-activate-component-hdfs[3032]: ++ echo gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_include
<13>Apr 27 18:07:37 dataproc-startup-script[1265]: <13>Apr 27 18:07:37 dataproc-pre-activate-component-hdfs[3032]: + INCLUDE_PATH=gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_include
<13>Apr 27 18:07:37 dataproc-startup-script[1265]: <13>Apr 27 18:07:37 dataproc-pre-activate-component-hdfs[3032]: ++ create_or_validate_exclude_file_path
<13>Apr 27 18:07:37 dataproc-startup-script[1265]: <13>Apr 27 18:07:37 dataproc-pre-activate-component-hdfs[3032]: ++ local exclude_path
<13>Apr 27 18:07:37 dataproc-startup-script[1265]: <13>Apr 27 18:07:37 dataproc-pre-activate-component-hdfs[3032]: +++ get_exclude_file_path
<13>Apr 27 18:07:37 dataproc-startup-script[1265]: <13>Apr 27 18:07:37 dataproc-pre-activate-component-hdfs[3032]: +++ local exclude_path
<13>Apr 27 18:07:37 dataproc-startup-script[1265]: <13>Apr 27 18:07:37 dataproc-pre-activate-component-hdfs[3032]: ++++ /usr/share/google/get_metadata_value attributes/dataproc-exclude-file-location
<13>Apr 27 18:07:37 dataproc-startup-script[1265]: <13>Apr 27 18:07:37 dataproc-pre-activate-component-hdfs[3032]: +++ exclude_path=gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_exclude.xml
<13>Apr 27 18:07:37 dataproc-startup-script[1265]: <13>Apr 27 18:07:37 dataproc-pre-activate-component-hdfs[3032]: +++ [[ -z gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_exclude.xml ]]
<13>Apr 27 18:07:37 dataproc-startup-script[1265]: <13>Apr 27 18:07:37 dataproc-pre-activate-component-hdfs[3032]: +++ echo gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_exclude.xml
<13>Apr 27 18:07:37 dataproc-startup-script[1265]: <13>Apr 27 18:07:37 dataproc-pre-activate-component-hdfs[3032]: ++ exclude_path=gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_exclude.xml
<13>Apr 27 18:07:37 dataproc-startup-script[1265]: <13>Apr 27 18:07:37 dataproc-pre-activate-component-hdfs[3032]: ++ [[ gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_exclude.xml == gs://* ]]
<13>Apr 27 18:07:37 dataproc-startup-script[1265]: <13>Apr 27 18:07:37 dataproc-pre-activate-component-hdfs[3032]: ++ loginfo 'Checking if exclude membership files exist in GCS: gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_exclude.xml'
<13>Apr 27 18:07:37 dataproc-startup-script[1265]: <13>Apr 27 18:07:37 dataproc-pre-activate-component-hdfs[3032]: ++ echo 'Checking if exclude membership files exist in GCS: gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_exclude.xml'
<13>Apr 27 18:07:37 dataproc-startup-script[1265]: <13>Apr 27 18:07:37 dataproc-pre-activate-component-hdfs[3032]: Checking if exclude membership files exist in GCS: gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_exclude.xml
<13>Apr 27 18:07:37 dataproc-startup-script[1265]: <13>Apr 27 18:07:37 dataproc-pre-activate-component-hdfs[3032]: ++ retry_constant_custom 10 3 gcs_file_exists gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_exclude.xml
<13>Apr 27 18:07:37 dataproc-startup-script[1265]: <13>Apr 27 18:07:37 dataproc-pre-activate-component-hdfs[3032]: ++ local -r max_retry_time=10
<13>Apr 27 18:07:37 dataproc-startup-script[1265]: <13>Apr 27 18:07:37 dataproc-pre-activate-component-hdfs[3032]: ++ local -r retry_delay=3
<13>Apr 27 18:07:37 dataproc-startup-script[1265]: <13>Apr 27 18:07:37 dataproc-pre-activate-component-hdfs[3032]: ++ cmd=('gcs_file_exists' 'gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_exclude.xml')
<13>Apr 27 18:07:37 dataproc-startup-script[1265]: <13>Apr 27 18:07:37 dataproc-pre-activate-component-hdfs[3032]: ++ local -r cmd
<13>Apr 27 18:07:37 dataproc-startup-script[1265]: <13>Apr 27 18:07:37 dataproc-pre-activate-component-hdfs[3032]: ++ local -r max_retries=3
<13>Apr 27 18:07:37 dataproc-startup-script[1265]: <13>Apr 27 18:07:37 dataproc-pre-activate-component-hdfs[3032]: ++ local reenable_x=false
<13>Apr 27 18:07:37 dataproc-startup-script[1265]: <13>Apr 27 18:07:37 dataproc-pre-activate-component-hdfs[3032]: ++ [[ -o xtrace ]]
<13>Apr 27 18:07:37 dataproc-startup-script[1265]: <13>Apr 27 18:07:37 dataproc-pre-activate-component-hdfs[3032]: ++ set +x
<13>Apr 27 18:07:37 dataproc-startup-script[1265]: <13>Apr 27 18:07:37 dataproc-pre-activate-component-hdfs[3032]: About to run 'gcs_file_exists gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_exclude.xml' with retries...
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-hdfs[3032]: 'gcs_file_exists gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_exclude.xml' succeeded after 1 execution(s).
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-hdfs[3032]: ++ return 0
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-hdfs[3032]: ++ echo gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_exclude.xml
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-hdfs[3032]: + EXCLUDE_PATH=gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_exclude.xml
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-hdfs[3032]: + bdconfig set_property --configuration_file /etc/hadoop/conf/hdfs-site.xml --name dfs.hosts --value gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_include --clobber
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-hdfs[3032]: + bdconfig set_property --configuration_file /etc/hadoop/conf/hdfs-site.xml --name dfs.hosts.exclude --value gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/nodes_exclude.xml --clobber
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-hdfs[3032]: + [[ false == \t\r\u\e ]]
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-hdfs[3032]: ++ get_metadata_datanode_enabled
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-hdfs[3032]: ++ get_dataproc_metadata DATAPROC_METADATA_DATANODE_ENABLED attributes/dataproc-datanode-enabled
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-hdfs[3032]: ++ set +x
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-hdfs[3032]: + [[ false != \t\r\u\e ]]
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-hdfs[3032]: + mark_packages_to_uninstall hadoop-hdfs-datanode
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-hdfs[3032]: + for package in "$@"
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-hdfs[3032]: + touch /tmp/dataproc/uninstall/hadoop-hdfs-datanode
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-hdfs[3032]: ++ get_metadata_role
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-hdfs[3032]: ++ get_dataproc_metadata DATAPROC_METADATA_ROLE attributes/dataproc-role
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-hdfs[3032]: ++ set +x
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-hdfs[3032]: + [[ Master == \M\a\s\t\e\r ]]
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: touch /tmp/dataproc/sentinel/hdfs.pre-activate
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-hdfs[3032]: + [[ 1 == \3 ]]
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: Running task: hive-metastore.pre-activate
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: bash -e /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.sh 2.2 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.sh"; exit 1; }
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: +++ get_metadata_master
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: +++ get_dataproc_metadata DATAPROC_METADATA_MASTER attributes/dataproc-master
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: +++ set +x
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: ++ DATAPROC_MASTER=cluster-dip-01-m
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: +++ get_metadata_master_additional
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: +++ get_dataproc_metadata DATAPROC_METADATA_MASTER_ADDITIONAL attributes/dataproc-master-additional
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: +++ set +x
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: ++ DATAPROC_MASTER_ADDITIONAL=
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: ++ MASTER_HOSTNAMES=($DATAPROC_MASTER ${DATAPROC_MASTER_ADDITIONAL//,/ })
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: ++ NUM_MASTERS=1
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: +++ get_metadata_role
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: +++ get_dataproc_metadata DATAPROC_METADATA_ROLE attributes/dataproc-role
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: +++ set +x
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: ++ ROLE=Master
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: ++ [[ 1 -gt 1 ]]
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: ++ CLUSTER_MASTER_METASTORE_URIS=thrift://cluster-dip-01-m:9083
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: + set -x
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: + set_log_tag pre-activate-component-hive-metastore
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: + local -r tag=dataproc-pre-activate-component-hive-metastore
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: + exec
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: ++ logger -s -t 'dataproc-pre-activate-component-hive-metastore[3462]'
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-hive-metastore[3462]: + HIVE_CONF_DIR=/etc/hive/conf
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-hive-metastore[3462]: + CLUSTER_PROPERTIES_DIR=/tmp/cluster/properties
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-hive-metastore[3462]: + bdconfig set_property --configuration_file /etc/hive/conf/hive-site.xml --name hive.metastore.uris --value thrift://cluster-dip-01-m:9083 --clobber
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-hive-metastore[3462]: + is_component_selected hdfs
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-hive-metastore[3462]: + local -r component=hdfs
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-hive-metastore[3462]: + local activated_components
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-hive-metastore[3462]: ++ get_components_to_activate
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-hive-metastore[3462]: ++ tr '[:upper:]' '[:lower:]'
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-hive-metastore[3462]: ++ get_dataproc_property dataproc.components.activate
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-hive-metastore[3462]: ++ set +x
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-hive-metastore[3462]: + activated_components='knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3'
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-hive-metastore[3462]: + [[ knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3 == *hdfs* ]]
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-hive-metastore[3462]: ++ get_property_in_xml /etc/hive/conf/hive-site.xml hive.metastore.warehouse.dir
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-hive-metastore[3462]: ++ set +x
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-hive-metastore[3462]: + HIVE_WAREHOUSE_DIR=
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-hive-metastore[3462]: + [[ '' == \g\s\:\/\/* ]]
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-hive-metastore[3462]: + METADATASTORE_JDBC_URI=jdbc:mysql://cluster-dip-01-m/metastore
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-hive-metastore[3462]: + bdconfig set_property --configuration_file /etc/hive/conf/hive-site.xml --name javax.jdo.option.ConnectionURL --value jdbc:mysql://cluster-dip-01-m/metastore --clobber
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: touch /tmp/dataproc/sentinel/hive-metastore.pre-activate
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: Running task: mysql.pre-activate
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: bash -e /usr/local/share/google/dataproc/bdutil/components/pre-activate/mysql.sh 2.2 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/mysql.sh"; exit 1; }
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: ++ is_version_at_least 2.2 2.2
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: ++ local reenable_x=false
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: ++ [[ -o xtrace ]]
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: ++ case ${compare_versions_result} in
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: ++ return 0
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: ++ MYSQL_VERSION=8.0
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: ++ MYSQL_EL_VERSION=8
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: + source /usr/local/share/google/dataproc/bdutil/os/debian/components/pre-activate/mysql.sh
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: + set -x
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: + set_log_tag pre-activate-component-mysql
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: + local -r tag=dataproc-pre-activate-component-mysql
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: + exec
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: ++ logger -s -t 'dataproc-pre-activate-component-mysql[3545]'
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-mysql[3545]: + [[ cluster-dip-01-m != \c\l\u\s\t\e\r\-\d\i\p\-\0\1\-\m ]]
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-mysql[3545]: + start_mysql
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-mysql[3545]: + local service_name
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-mysql[3545]: + is_rocky
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-mysql[3545]: ++ os_id
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-mysql[3545]: ++ cut -d= -f2
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-mysql[3545]: ++ grep '^ID=' /etc/os-release
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-mysql[3545]: ++ xargs
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-mysql[3545]: + [[ debian == \r\o\c\k\y ]]
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-mysql[3545]: + service_name=mysql
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-mysql[3545]: + systemctl unmask mysql
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-mysql[3545]: + enable_service mysql
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-mysql[3545]: + local -r service=mysql
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-mysql[3545]: + local -r unit=mysql.service
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-mysql[3545]: + retry_constant_short systemctl enable mysql.service
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-mysql[3545]: + retry_constant_custom 30 1 systemctl enable mysql.service
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-mysql[3545]: + local -r max_retry_time=30
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-mysql[3545]: + local -r retry_delay=1
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-mysql[3545]: + cmd=('systemctl' 'enable' 'mysql.service')
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-mysql[3545]: + local -r cmd
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-mysql[3545]: + local -r max_retries=30
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-mysql[3545]: + local reenable_x=false
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-mysql[3545]: + [[ -o xtrace ]]
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-mysql[3545]: + set +x
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-mysql[3545]: About to run 'systemctl enable mysql.service' with retries...
<13>Apr 27 18:07:38 dataproc-startup-script[1265]: <13>Apr 27 18:07:38 dataproc-pre-activate-component-mysql[3545]: Created symlink /etc/systemd/system/multi-user.target.wants/mysql.service → /lib/systemd/system/mysql.service.
<13>Apr 27 18:07:39 dataproc-startup-script[1265]: <13>Apr 27 18:07:39 dataproc-pre-activate-component-mysql[3545]: 'systemctl enable mysql.service' succeeded after 1 execution(s).
<13>Apr 27 18:07:39 dataproc-startup-script[1265]: <13>Apr 27 18:07:39 dataproc-pre-activate-component-mysql[3545]: + return 0
<13>Apr 27 18:07:39 dataproc-startup-script[1265]: <13>Apr 27 18:07:39 dataproc-pre-activate-component-mysql[3545]: + local -r drop_in_dir=/etc/systemd/system/mysql.service.d
<13>Apr 27 18:07:39 dataproc-startup-script[1265]: <13>Apr 27 18:07:39 dataproc-pre-activate-component-mysql[3545]: + mkdir -p /etc/systemd/system/mysql.service.d
<13>Apr 27 18:07:39 dataproc-startup-script[1265]: <13>Apr 27 18:07:39 dataproc-pre-activate-component-mysql[3545]: + local props
<13>Apr 27 18:07:39 dataproc-startup-script[1265]: <13>Apr 27 18:07:39 dataproc-pre-activate-component-mysql[3545]: ++ retry_constant_short systemctl show mysql.service -p Restart,RemainAfterExit
<13>Apr 27 18:07:39 dataproc-startup-script[1265]: <13>Apr 27 18:07:39 dataproc-pre-activate-component-mysql[3545]: ++ retry_constant_custom 30 1 systemctl show mysql.service -p Restart,RemainAfterExit
<13>Apr 27 18:07:39 dataproc-startup-script[1265]: <13>Apr 27 18:07:39 dataproc-pre-activate-component-mysql[3545]: ++ local -r max_retry_time=30
<13>Apr 27 18:07:39 dataproc-startup-script[1265]: <13>Apr 27 18:07:39 dataproc-pre-activate-component-mysql[3545]: ++ local -r retry_delay=1
<13>Apr 27 18:07:39 dataproc-startup-script[1265]: <13>Apr 27 18:07:39 dataproc-pre-activate-component-mysql[3545]: ++ cmd=('systemctl' 'show' 'mysql.service' '-p' 'Restart,RemainAfterExit')
<13>Apr 27 18:07:39 dataproc-startup-script[1265]: <13>Apr 27 18:07:39 dataproc-pre-activate-component-mysql[3545]: ++ local -r cmd
<13>Apr 27 18:07:39 dataproc-startup-script[1265]: <13>Apr 27 18:07:39 dataproc-pre-activate-component-mysql[3545]: ++ local -r max_retries=30
<13>Apr 27 18:07:39 dataproc-startup-script[1265]: <13>Apr 27 18:07:39 dataproc-pre-activate-component-mysql[3545]: ++ local reenable_x=false
<13>Apr 27 18:07:39 dataproc-startup-script[1265]: <13>Apr 27 18:07:39 dataproc-pre-activate-component-mysql[3545]: ++ [[ -o xtrace ]]
<13>Apr 27 18:07:39 dataproc-startup-script[1265]: <13>Apr 27 18:07:39 dataproc-pre-activate-component-mysql[3545]: ++ set +x
<13>Apr 27 18:07:39 dataproc-startup-script[1265]: <13>Apr 27 18:07:39 dataproc-pre-activate-component-mysql[3545]: About to run 'systemctl show mysql.service -p Restart,RemainAfterExit' with retries...
<13>Apr 27 18:07:39 dataproc-startup-script[1265]: <13>Apr 27 18:07:39 dataproc-pre-activate-component-mysql[3545]: 'systemctl show mysql.service -p Restart,RemainAfterExit' succeeded after 1 execution(s).
<13>Apr 27 18:07:39 dataproc-startup-script[1265]: <13>Apr 27 18:07:39 dataproc-pre-activate-component-mysql[3545]: ++ return 0
<13>Apr 27 18:07:39 dataproc-startup-script[1265]: <13>Apr 27 18:07:39 dataproc-pre-activate-component-mysql[3545]: + props='Restart=on-failure
<13>Apr 27 18:07:39 dataproc-startup-script[1265]: <13>Apr 27 18:07:39 dataproc-pre-activate-component-mysql[3545]: RemainAfterExit=no'
<13>Apr 27 18:07:39 dataproc-startup-script[1265]: <13>Apr 27 18:07:39 dataproc-pre-activate-component-mysql[3545]: + [[ mysql != \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Apr 27 18:07:39 dataproc-startup-script[1265]: <13>Apr 27 18:07:39 dataproc-pre-activate-component-mysql[3545]: + [[ mysql != \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Apr 27 18:07:39 dataproc-startup-script[1265]: <13>Apr 27 18:07:39 dataproc-pre-activate-component-mysql[3545]: + [[ Restart=on-failure
<13>Apr 27 18:07:39 dataproc-startup-script[1265]: <13>Apr 27 18:07:39 dataproc-pre-activate-component-mysql[3545]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Apr 27 18:07:39 dataproc-startup-script[1265]: <13>Apr 27 18:07:39 dataproc-pre-activate-component-mysql[3545]: + local -r gate_start_on_agent_success_drop_in=/etc/systemd/system/common/agent-gate.conf
<13>Apr 27 18:07:39 dataproc-startup-script[1265]: <13>Apr 27 18:07:39 dataproc-pre-activate-component-mysql[3545]: + [[ ! -f /etc/systemd/system/common/agent-gate.conf ]]
<13>Apr 27 18:07:39 dataproc-startup-script[1265]: <13>Apr 27 18:07:39 dataproc-pre-activate-component-mysql[3545]: + [[ mysql == \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Apr 27 18:07:39 dataproc-startup-script[1265]: <13>Apr 27 18:07:39 dataproc-pre-activate-component-mysql[3545]: + [[ mysql == \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Apr 27 18:07:39 dataproc-startup-script[1265]: <13>Apr 27 18:07:39 dataproc-pre-activate-component-mysql[3545]: + retry_constant systemctl start mysql
<13>Apr 27 18:07:39 dataproc-startup-script[1265]: <13>Apr 27 18:07:39 dataproc-pre-activate-component-mysql[3545]: + retry_constant_custom 300 1 systemctl start mysql
<13>Apr 27 18:07:39 dataproc-startup-script[1265]: <13>Apr 27 18:07:39 dataproc-pre-activate-component-mysql[3545]: + local -r max_retry_time=300
<13>Apr 27 18:07:39 dataproc-startup-script[1265]: <13>Apr 27 18:07:39 dataproc-pre-activate-component-mysql[3545]: + local -r retry_delay=1
<13>Apr 27 18:07:39 dataproc-startup-script[1265]: <13>Apr 27 18:07:39 dataproc-pre-activate-component-mysql[3545]: + cmd=('systemctl' 'start' 'mysql')
<13>Apr 27 18:07:39 dataproc-startup-script[1265]: <13>Apr 27 18:07:39 dataproc-pre-activate-component-mysql[3545]: + local -r cmd
<13>Apr 27 18:07:39 dataproc-startup-script[1265]: <13>Apr 27 18:07:39 dataproc-pre-activate-component-mysql[3545]: + local -r max_retries=300
<13>Apr 27 18:07:39 dataproc-startup-script[1265]: <13>Apr 27 18:07:39 dataproc-pre-activate-component-mysql[3545]: + local reenable_x=false
<13>Apr 27 18:07:39 dataproc-startup-script[1265]: <13>Apr 27 18:07:39 dataproc-pre-activate-component-mysql[3545]: + [[ -o xtrace ]]
<13>Apr 27 18:07:39 dataproc-startup-script[1265]: <13>Apr 27 18:07:39 dataproc-pre-activate-component-mysql[3545]: + set +x
<13>Apr 27 18:07:39 dataproc-startup-script[1265]: <13>Apr 27 18:07:39 dataproc-pre-activate-component-mysql[3545]: About to run 'systemctl start mysql' with retries...
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-mysql[3545]: 'systemctl start mysql' succeeded after 1 execution(s).
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-mysql[3545]: + return 0
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-mysql[3545]: + change_file_permission
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-mysql[3545]: + echo 'Changing file permissions on my.cnf'
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-mysql[3545]: Changing file permissions on my.cnf
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-mysql[3545]: + chmod 600 /etc/mysql/my.cnf
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-mysql[3545]: + ensure_myconf_link_correct
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-mysql[3545]: + is_debian12
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-mysql[3545]: + is_debian
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-mysql[3545]: ++ os_id
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-mysql[3545]: ++ grep '^ID=' /etc/os-release
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-mysql[3545]: ++ xargs
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-mysql[3545]: ++ cut -d= -f2
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-mysql[3545]: + [[ debian == \d\e\b\i\a\n ]]
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-mysql[3545]: ++ os_version
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-mysql[3545]: ++ grep '^VERSION_ID=' /etc/os-release
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-mysql[3545]: ++ xargs
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-mysql[3545]: ++ cut -d= -f2
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-mysql[3545]: + [[ 12 == \1\2* ]]
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-mysql[3545]: + [[ standard == \s\t\a\n\d\a\r\d ]]
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-mysql[3545]: + is_version_at_least 2.2 2.2
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-mysql[3545]: + local reenable_x=false
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-mysql[3545]: + [[ -o xtrace ]]
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-mysql[3545]: + set +x
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-mysql[3545]: + case ${compare_versions_result} in
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-mysql[3545]: + return 0
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-mysql[3545]: + update-alternatives --set my.cnf /etc/mysql/mysql.cnf
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-mysql[3545]: update-alternatives: using /etc/mysql/mysql.cnf to provide /etc/mysql/my.cnf (my.cnf) in manual mode
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-mysql[3545]: + reset_mysql_password
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-mysql[3545]: + set +x
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-mysql[3545]: Connecting to mysql to reset password
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-mysql[3545]: mysql: [Warning] Using a password on the command line interface can be insecure.
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-mysql[3545]: Mysql password reset.
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-mysql[3545]: Stored mysql information in a safe place
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: touch /tmp/dataproc/sentinel/mysql.pre-activate
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: Running task: tez.pre-activate
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: bash -e /usr/local/share/google/dataproc/bdutil/components/pre-activate/tez.sh 2.2 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/tez.sh"; exit 1; }
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: + set_log_tag pre-activate-component-tez
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: + local -r tag=dataproc-pre-activate-component-tez
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: + exec
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: ++ logger -s -t 'dataproc-pre-activate-component-tez[3699]'
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-tez[3699]: ++ ls /usr/lib/tez/tez-ui-0.10.2.war
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-tez[3699]: + readonly TEZ_UI_WAR=/usr/lib/tez/tez-ui-0.10.2.war
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-tez[3699]: + TEZ_UI_WAR=/usr/lib/tez/tez-ui-0.10.2.war
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-tez[3699]: + pre_activate_tez
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-tez[3699]: + configure_ui_war
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-tez[3699]: + local tmp_dir
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-tez[3699]: ++ mktemp -d
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-tez[3699]: + tmp_dir=/tmp/tmp.0Fe2Ga44TS
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-tez[3699]: + unzip -q /usr/lib/tez/tez-ui-0.10.2.war -d /tmp/tmp.0Fe2Ga44TS
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-tez[3699]: + local -r tez_configs=/tmp/tmp.0Fe2Ga44TS/config/configs.env
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-tez[3699]: + local -r tez_ui_js=/tmp/tmp.0Fe2Ga44TS/assets/tez-ui.js
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-tez[3699]: + local ats_v2_port=8192
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-tez[3699]: + is_component_selected kerberos
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-tez[3699]: + local -r component=kerberos
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-tez[3699]: + local activated_components
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-tez[3699]: ++ get_components_to_activate
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-tez[3699]: ++ get_dataproc_property dataproc.components.activate
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-tez[3699]: ++ tr '[:upper:]' '[:lower:]'
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-tez[3699]: ++ set +x
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-tez[3699]: + activated_components='knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3'
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-tez[3699]: + [[ knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3 == *kerberos* ]]
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-tez[3699]: + local ats_v2_enabled=false
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-tez[3699]: ++ get_dataproc_property yarn.atsv2.bigtable.instance
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-tez[3699]: ++ set +x
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-tez[3699]: + [[ -n '' ]]
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-tez[3699]: + [[ -f /tmp/tmp.0Fe2Ga44TS/config/configs.env ]]
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-tez[3699]: + [[ -f /tmp/tmp.0Fe2Ga44TS/assets/tez-ui.js ]]
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-tez[3699]: + sed -i 's#"timeline":"localhost:8188"#"timeline":"cluster-dip-01-m:8188"#' /tmp/tmp.0Fe2Ga44TS/assets/tez-ui.js
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-tez[3699]: + sed -i 's#"timelineV2":"localhost:8192"#"timelineV2":"cluster-dip-01-m:8192"#' /tmp/tmp.0Fe2Ga44TS/assets/tez-ui.js
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-tez[3699]: + sed -i 's#"rm":"localhost:8088"#"rm":"cluster-dip-01-m:8088"#' /tmp/tmp.0Fe2Ga44TS/assets/tez-ui.js
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-tez[3699]: + is_component_selected knox
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-tez[3699]: + local -r component=knox
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-tez[3699]: + local activated_components
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-tez[3699]: ++ get_components_to_activate
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-tez[3699]: ++ get_dataproc_property dataproc.components.activate
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-tez[3699]: ++ tr '[:upper:]' '[:lower:]'
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-tez[3699]: ++ set +x
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-tez[3699]: + activated_components='knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3'
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-tez[3699]: + [[ knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3 == *knox* ]]
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-tez[3699]: + local cluster_ui_hostname
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-tez[3699]: ++ get_dataproc_property dataproc.proxy.public.hostname
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-tez[3699]: ++ set +x
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-tez[3699]: ++ sed 's/\\:/:/'
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-tez[3699]: + cluster_ui_hostname=https://hai26gzvrngzjp6hwhycsnr4ly-dot-us-east1.dataproc.googleusercontent.com
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-tez[3699]: + cat
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-tez[3699]: + cd /tmp/tmp.0Fe2Ga44TS
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-tez[3699]: + zip -q /usr/lib/tez/tez-ui-0.10.2.war -r ./META-INF ./WEB-INF ./assets ./config ./fonts ./index.html
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-tez[3699]: + cd ..
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-tez[3699]: + rm -rf /tmp/tmp.0Fe2Ga44TS
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-tez[3699]: ++ stat /usr/lib/tez/tez-common-0.10.2.jar --format=%Y
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-tez[3699]: + touch -d @1579705815 /usr/lib/tez/tez-ui-0.10.2.war
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-tez[3699]: + configure_yarn_for_tez
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-tez[3699]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.ui-names --value tez --clobber
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-tez[3699]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.ui-on-disk-path.tez --value /usr/lib/tez/tez-ui-0.10.2.war --clobber
<13>Apr 27 18:07:40 dataproc-startup-script[1265]: <13>Apr 27 18:07:40 dataproc-pre-activate-component-tez[3699]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.ui-web-path.tez --value /tez-ui --clobber
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-tez[3699]: + configure_tez
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-tez[3699]: + local history_logging_service_class=ATSHistoryLoggingService
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-tez[3699]: ++ get_dataproc_property yarn.atsv2.bigtable.instance
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-tez[3699]: ++ set +x
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-tez[3699]: + [[ -n '' ]]
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-tez[3699]: + bdconfig set_property --configuration_file /etc/tez/conf/tez-site.xml --name tez.history.logging.service.class --value org.apache.tez.dag.history.logging.ats.ATSHistoryLoggingService --clobber
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-tez[3699]: + bdconfig set_property --configuration_file /etc/tez/conf/tez-site.xml --name tez.tez-ui.history-url.base --value http://cluster-dip-01-m:8188/tez-ui/ --clobber
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-tez[3699]: + bdconfig set_property --configuration_file /etc/tez/conf/tez-site.xml --name tez.am.node-blacklisting.enabled --value false --clobber
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-tez[3699]: + configure_tez_for_jdk17
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-tez[3699]: + is_version_at_least 2.2 3.0
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-tez[3699]: + local reenable_x=false
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-tez[3699]: + [[ -o xtrace ]]
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-tez[3699]: + set +x
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-tez[3699]: + case ${compare_versions_result} in
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-tez[3699]: + return 1
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-tez[3699]: + return
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-tez[3699]: + merge_user_properties
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-tez[3699]: + [[ -f /etc/tez/conf/tez-site.xml ]]
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-tez[3699]: + merge_xml_properties /tmp/cluster/properties/tez.xml /etc/tez/conf/tez-site.xml
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-tez[3699]: + local src=/tmp/cluster/properties/tez.xml
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-tez[3699]: + local dest=/etc/tez/conf/tez-site.xml
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-tez[3699]: + [[ ! -f /tmp/cluster/properties/tez.xml ]]
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-tez[3699]: + bdconfig merge_configurations --configuration_file /etc/tez/conf/tez-site.xml --source_configuration_file /tmp/cluster/properties/tez.xml --resolve_environment_variables --create_if_absent --clobber
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-tez[3699]: + loginfo 'Merged /tmp/cluster/properties/tez.xml.'
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-tez[3699]: + echo 'Merged /tmp/cluster/properties/tez.xml.'
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-tez[3699]: Merged /tmp/cluster/properties/tez.xml.
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: touch /tmp/dataproc/sentinel/tez.pre-activate
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: Running task: hive-server2.pre-activate
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: bash -e /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.sh 2.2 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.sh"; exit 1; }
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: + set_log_tag pre-activate-component-hive-server2
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: + local -r tag=dataproc-pre-activate-component-hive-server2
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: + exec
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: ++ logger -s -t 'dataproc-pre-activate-component-hive-server2[3770]'
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + HIVE_CONF_DIR=/etc/hive/conf
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + CLUSTER_PROPERTIES_DIR=/tmp/cluster/properties
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: ++ get_metadata_bucket
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: ++ get_dataproc_metadata DATAPROC_METADATA_BUCKET attributes/dataproc-bucket
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: ++ set +x
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + CONFIGBUCKET=dataproc-staging-us-east1-679657336577-kutp8kah
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: ++ get_metadata_cluster_uuid
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: ++ get_dataproc_metadata DATAPROC_METADATA_CLUSTER_UUID attributes/dataproc-cluster-uuid
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: ++ set +x
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + CLUSTER_UUID=0b63cfa9-1f8a-481e-87c8-a0cf661f3625
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: ++ get_metadata_master
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: ++ get_dataproc_metadata DATAPROC_METADATA_MASTER attributes/dataproc-master
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: ++ set +x
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + DATAPROC_MASTER=cluster-dip-01-m
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: ++ get_metadata_master_additional
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: ++ get_dataproc_metadata DATAPROC_METADATA_MASTER_ADDITIONAL attributes/dataproc-master-additional
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: ++ set +x
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + DATAPROC_MASTER_ADDITIONAL=
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + MASTER_HOSTNAMES=($DATAPROC_MASTER ${DATAPROC_MASTER_ADDITIONAL//,/ })
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + NUM_MASTERS=1
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + [[ 1 -gt 1 ]]
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + bdconfig set_property --configuration_file /etc/hive/conf/hive-site.xml --name hive.user.install.directory --value '${hadoop.tmp.dir}/hive/user-install-dir' --clobber
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + is_version_at_least 2.2 2.1
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + local reenable_x=false
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + [[ -o xtrace ]]
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + set +x
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + case ${compare_versions_result} in
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + return 0
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + set_property_hive_site hive.exec.input.listing.max.threads 20
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + local -r name=hive.exec.input.listing.max.threads
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + local -r value=20
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + set_property_in_xml /etc/hive/conf/hive-site.xml hive.exec.input.listing.max.threads 20
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + local -r xml_file=/etc/hive/conf/hive-site.xml
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + local -r name=hive.exec.input.listing.max.threads
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + local -r value=20
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + local -r mode=overwrite
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + local -r delimiter=,
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + local skip=false
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + local old_value
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + local new_value
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + [[ overwrite == \o\v\e\r\w\r\i\t\e ]]
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + new_value=20
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + [[ false == \f\a\l\s\e ]]
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + bdconfig set_property --configuration_file /etc/hive/conf/hive-site.xml --name hive.exec.input.listing.max.threads --value 20 --create_if_absent --clobber
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + set_property_hive_site hive.fetch.task.conversion minimal
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + local -r name=hive.fetch.task.conversion
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + local -r value=minimal
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + set_property_in_xml /etc/hive/conf/hive-site.xml hive.fetch.task.conversion minimal
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + local -r xml_file=/etc/hive/conf/hive-site.xml
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + local -r name=hive.fetch.task.conversion
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + local -r value=minimal
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + local -r mode=overwrite
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + local -r delimiter=,
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + local skip=false
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + local old_value
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + local new_value
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + [[ overwrite == \o\v\e\r\w\r\i\t\e ]]
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + new_value=minimal
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + [[ false == \f\a\l\s\e ]]
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + bdconfig set_property --configuration_file /etc/hive/conf/hive-site.xml --name hive.fetch.task.conversion --value minimal --create_if_absent --clobber
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + is_component_selected trino
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + local -r component=trino
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + local activated_components
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: ++ get_components_to_activate
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: ++ get_dataproc_property dataproc.components.activate
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: ++ set +x
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: ++ tr '[:upper:]' '[:lower:]'
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + activated_components='knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3'
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + [[ knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3 == *trino* ]]
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + merge_xml_properties /tmp/cluster/properties/hive.xml /etc/hive/conf/hive-site.xml
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + local src=/tmp/cluster/properties/hive.xml
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + local dest=/etc/hive/conf/hive-site.xml
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + [[ ! -f /tmp/cluster/properties/hive.xml ]]
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + bdconfig merge_configurations --configuration_file /etc/hive/conf/hive-site.xml --source_configuration_file /tmp/cluster/properties/hive.xml --resolve_environment_variables --create_if_absent --clobber
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + loginfo 'Merged /tmp/cluster/properties/hive.xml.'
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + echo 'Merged /tmp/cluster/properties/hive.xml.'
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: Merged /tmp/cluster/properties/hive.xml.
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + merge_java_properties /tmp/cluster/properties/hive-log4j2.properties /etc/hive/conf/hive-log4j2.properties
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + local -r src=/tmp/cluster/properties/hive-log4j2.properties
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + local -r dest=/etc/hive/conf/hive-log4j2.properties
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + local -r 'header=\n# User-supplied properties.'
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + [[ ! -f /tmp/cluster/properties/hive-log4j2.properties ]]
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + echo -e '\n# User-supplied properties.'
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + cat /tmp/cluster/properties/hive-log4j2.properties
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + loginfo 'Merged /tmp/cluster/properties/hive-log4j2.properties.'
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: + echo 'Merged /tmp/cluster/properties/hive-log4j2.properties.'
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: <13>Apr 27 18:07:41 dataproc-pre-activate-component-hive-server2[3770]: Merged /tmp/cluster/properties/hive-log4j2.properties.
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: touch /tmp/dataproc/sentinel/hive-server2.pre-activate
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: Running task: otel-ucp.pre-activate
<13>Apr 27 18:07:41 dataproc-startup-script[1265]: bash -e /usr/local/share/google/dataproc/bdutil/components/pre-activate/otel-ucp.sh 2.2 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/otel-ucp.sh"; exit 1; }
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: 0
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: 60
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: 300
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: 1440
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: 0
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + set_log_tag pre-activate-component-otel-ucp
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + local -r tag=dataproc-pre-activate-component-otel-ucp
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + exec
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ logger -s -t 'dataproc-pre-activate-component-otel-ucp[3892]'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: ++ get_dataproc_property dataproc.observability.containerised.legacy.agents.enable
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: ++ set +x
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: + CONTAINERISED_AGENTS_ENABLED=
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: + [[ '' == \t\r\u\e ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: ++ get_metadata_project_number
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: ++ get_dataproc_metadata DATAPROC_METADATA_PROJECT_NUMBER ../project/numeric-project-id
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: ++ set +x
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: + PROJECT_NUMBER=679657336577
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: + readonly OTEL_S8S_DIR=/usr/local/share/google/dataproc/bdutil/conf/otel
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: + OTEL_S8S_DIR=/usr/local/share/google/dataproc/bdutil/conf/otel
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\r\a\y ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: + [[ standard == \s\t\a\n\d\a\r\d ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: + is_any_metric_source_enabled
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: + local stack_driver_monitoring_enabled
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: ++ get_dataproc_property dataproc.monitoring.stackdriver.enable
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: ++ set +x
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: + stack_driver_monitoring_enabled=false
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: + [[ false == \t\r\u\e ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: + local metric_sources_enabled_array
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: + get_enabled_metric_sources_array metric_sources_enabled_array
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: + local -n ms_array=metric_sources_enabled_array
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: + local metric_sources_enabled_delimited_by_comma
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: ++ get_dataproc_property dataproc.monitoring.metric.sources
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: ++ set +x
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: + metric_sources_enabled_delimited_by_comma=
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: + use_ucp_observability_agents
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: + [[ standard != \s\t\a\n\d\a\r\d ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: + is_containerised_legacy_agents_supported
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: + [[ standard == \s\t\a\n\d\a\r\d ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: + is_version_at_least 2.2 2.2
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: + local reenable_x=false
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: + [[ -o xtrace ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: + set +x
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: + case ${compare_versions_result} in
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: + return 0
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: + return 0
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: + is_legacy_containerized_agents_enabled
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: ++ get_dataproc_property_or_default dataproc.observability.containerised.legacy.agents.enable false
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: ++ set +x
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: + local -r legacy_containerised_agents_enabled=false
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: + [[ false == \t\r\u\e ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: + return 0
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: + metric_sources_enabled_delimited_by_comma=
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: + ms_array=(${metric_sources_enabled_delimited_by_comma//,/ })
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: + [[ 0 -gt 0 ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: + return 1
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: + cp /usr/local/share/google/dataproc/bdutil/monitoring_plugin_generator/dataproc_otel_base.yaml /etc/ucp/monitoring/configs/otel_base_config.yaml
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: + cp /usr/local/share/google/dataproc/bdutil/conf/otel_spark_default_metrics.yaml /etc/ucp/monitoring/configs/spark_default_metrics.yaml
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: + cp /usr/local/share/google/dataproc/bdutil/conf/otel_gcs_connector_metrics.yaml /etc/ucp/monitoring/configs/gcs_connector_metrics.yaml
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: + configure_ucp_monitoring_agent
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: + otel_args=()
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: + local otel_args
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: ++ find /etc/ucp/monitoring/configs/ -type f -name '*.yaml'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: ++ sort
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: + local -r 'config_files=/etc/ucp/monitoring/configs/gcs_connector_metrics.yaml
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: /etc/ucp/monitoring/configs/otel_base_config.yaml
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: /etc/ucp/monitoring/configs/spark_default_metrics.yaml'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: + for yaml_file in $config_files
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: + otel_args+=("--config ${yaml_file}")
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: + for yaml_file in $config_files
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: + otel_args+=("--config ${yaml_file}")
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: + for yaml_file in $config_files
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: + otel_args+=("--config ${yaml_file}")
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: + local -r monitoring_agent_service_file=/etc/systemd/system/otel-metrics-agent.service
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: + cat
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: + systemctl daemon-reload
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: + systemctl enable otel-metrics-agent
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-otel-ucp[3892]: Created symlink /etc/systemd/system/multi-user.target.wants/otel-metrics-agent.service → /etc/systemd/system/otel-metrics-agent.service.
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: touch /tmp/dataproc/sentinel/otel-ucp.pre-activate
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: Running task: pig.pre-activate
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: bash -e /usr/local/share/google/dataproc/bdutil/components/pre-activate/pig.sh 2.2 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/pig.sh"; exit 1; }
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + set_log_tag pre-activate-component-pig
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + local -r tag=dataproc-pre-activate-component-pig
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + exec
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ logger -s -t 'dataproc-pre-activate-component-pig[3980]'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-pig[3980]: + merge_user_properties
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-pig[3980]: + merge_java_properties /tmp/cluster/properties/pig.properties /etc/pig/conf/pig.properties
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-pig[3980]: + local -r src=/tmp/cluster/properties/pig.properties
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-pig[3980]: + local -r dest=/etc/pig/conf/pig.properties
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-pig[3980]: + local -r 'header=\n# User-supplied properties.'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-pig[3980]: + [[ ! -f /tmp/cluster/properties/pig.properties ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-pig[3980]: + echo -e '\n# User-supplied properties.'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-pig[3980]: + cat /tmp/cluster/properties/pig.properties
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-pig[3980]: + loginfo 'Merged /tmp/cluster/properties/pig.properties.'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-pig[3980]: + echo 'Merged /tmp/cluster/properties/pig.properties.'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-pig[3980]: Merged /tmp/cluster/properties/pig.properties.
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-pig[3980]: + merge_java_properties /tmp/cluster/properties/pig-log4j.properties /etc/pig/conf/log4j.properties
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-pig[3980]: + local -r src=/tmp/cluster/properties/pig-log4j.properties
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-pig[3980]: + local -r dest=/etc/pig/conf/log4j.properties
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-pig[3980]: + local -r 'header=\n# User-supplied properties.'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-pig[3980]: + [[ ! -f /tmp/cluster/properties/pig-log4j.properties ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-pig[3980]: + echo -e '\n# User-supplied properties.'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-pig[3980]: + cat /tmp/cluster/properties/pig-log4j.properties
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: touch /tmp/dataproc/sentinel/pig.pre-activate
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-pig[3980]: + loginfo 'Merged /tmp/cluster/properties/pig-log4j.properties.'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-pig[3980]: + echo 'Merged /tmp/cluster/properties/pig-log4j.properties.'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-pig[3980]: Merged /tmp/cluster/properties/pig-log4j.properties.
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: Running task: spark.pre-activate
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: bash -e /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh 2.2 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh"; exit 1; }
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: +++ get_metadata_master
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: +++ get_dataproc_metadata DATAPROC_METADATA_MASTER attributes/dataproc-master
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: +++ set +x
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ DATAPROC_MASTER=cluster-dip-01-m
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: +++ get_metadata_master_additional
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: +++ get_dataproc_metadata DATAPROC_METADATA_MASTER_ADDITIONAL attributes/dataproc-master-additional
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: +++ set +x
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ DATAPROC_MASTER_ADDITIONAL=
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ MASTER_HOSTNAMES=($DATAPROC_MASTER ${DATAPROC_MASTER_ADDITIONAL//,/ })
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ NUM_MASTERS=1
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: +++ get_metadata_role
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: +++ get_dataproc_metadata DATAPROC_METADATA_ROLE attributes/dataproc-role
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: +++ set +x
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ ROLE=Master
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ [[ 1 -gt 1 ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ CLUSTER_MASTER_METASTORE_URIS=thrift://cluster-dip-01-m:9083
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../shared/nvidia-drivers.sh
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../shared/spark.sh
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ export SPARK_HOME=/usr/lib/spark
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ SPARK_HOME=/usr/lib/spark
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ export SPARK_JARS_DIR=/usr/lib/spark/jars
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ SPARK_JARS_DIR=/usr/lib/spark/jars
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ [[ standard == \s\t\a\n\d\a\r\d ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ [[ 2.2 =~ ^2\.[01]$ ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ export SPARK_CONNECTOR_DIR=/usr/lib/spark/connector
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ SPARK_CONNECTOR_DIR=/usr/lib/spark/connector
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ export SPARK_CONF_DIR=/etc/spark/conf
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ export SPARK_AUTH_SECRET_FILE=/tmp/cluster/spark.auth.secret
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ SPARK_AUTH_SECRET_FILE=/tmp/cluster/spark.auth.secret
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ export LINEAGE_LIB_DIR=/usr/local/share/google/dataproc/lineage
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ LINEAGE_LIB_DIR=/usr/local/share/google/dataproc/lineage
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ CLUSTER_PROPERTIES_DIR=/tmp/cluster/properties
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ export SPARK_DATA_DIR=/hadoop/spark
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ SPARK_DATA_DIR=/hadoop/spark
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ export SPARK_LOG_DIR=/var/log/spark
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ SPARK_LOG_DIR=/var/log/spark
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ export SPARK_TMP_DIR=/hadoop/spark/tmp
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ SPARK_TMP_DIR=/hadoop/spark/tmp
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ export SPARK_WORK_DIR=/hadoop/spark/work
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ SPARK_WORK_DIR=/hadoop/spark/work
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ export SPARK_CONNECT_PORT=15001
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ SPARK_CONNECT_PORT=15001
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ export SPARK_CONNECT_PROXY_PORT=8443
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ SPARK_CONNECT_PROXY_PORT=8443
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ export SPARK_CONNECT_SYSTEMD_UNIT_PATH=/usr/lib/systemd/system/spark-connect.service
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ SPARK_CONNECT_SYSTEMD_UNIT_PATH=/usr/lib/systemd/system/spark-connect.service
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ export SPARK_CONNECT_PROXY_SYSTEMD_UNIT_PATH=/usr/lib/systemd/system/spark-connect-proxy.service
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ SPARK_CONNECT_PROXY_SYSTEMD_UNIT_PATH=/usr/lib/systemd/system/spark-connect-proxy.service
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ export SPARK_NATIVE_HOME_DIR=/usr/lib/spark/native
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ SPARK_NATIVE_HOME_DIR=/usr/lib/spark/native
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../shared/delta.sh
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ export DELTA_HOME=/usr/lib/delta/lib
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ DELTA_HOME=/usr/lib/delta/lib
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ export DELTA_SPARK_JAR=delta-spark_2.12-3.2.0.jar
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ DELTA_SPARK_JAR=delta-spark_2.12-3.2.0.jar
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ export DELTA_STORAGE_JAR=delta-storage-3.2.0.jar
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ DELTA_STORAGE_JAR=delta-storage-3.2.0.jar
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ export DELTA_HIVE_JAR=delta-hive-assembly_2.12-3.2.0.jar
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ DELTA_HIVE_JAR=delta-hive-assembly_2.12-3.2.0.jar
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ export DELTA_SPARK_JAR_PATH=/usr/lib/delta/lib/delta-spark_2.12-3.2.0.jar
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ DELTA_SPARK_JAR_PATH=/usr/lib/delta/lib/delta-spark_2.12-3.2.0.jar
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ export DELTA_STORAGE_JAR_PATH=/usr/lib/delta/lib/delta-storage-3.2.0.jar
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ DELTA_STORAGE_JAR_PATH=/usr/lib/delta/lib/delta-storage-3.2.0.jar
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ export DELTA_HIVE_JAR_PATH=/usr/lib/delta/lib/delta-hive-assembly_2.12-3.2.0.jar
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ DELTA_HIVE_JAR_PATH=/usr/lib/delta/lib/delta-hive-assembly_2.12-3.2.0.jar
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + source /usr/local/share/google/dataproc/dataproc_env.sh
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ CLUSTER_NAME=cluster-dip-01
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ CLUSTER_UUID=0b63cfa9-1f8a-481e-87c8-a0cf661f3625
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ CONFIGBUCKET=dataproc-staging-us-east1-679657336577-kutp8kah
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ TEMP_BUCKET=dataproc-temp-us-east1-679657336577-juievcnm
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ HCFS_ROOT_URI=hdfs://cluster-dip-01-m
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ MASTER_HOSTNAME_0=cluster-dip-01-m
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ DATAPROC_MASTER=cluster-dip-01-m
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ DATAPROC_MASTER_FQDN=cluster-dip-01-m.c.euphoric-coral-451717-v8.internal
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ MASTER_HOSTNAMES=(cluster-dip-01-m)
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ NUM_MASTERS=1
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ NUM_WORKERS=3
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ PROJECT=euphoric-coral-451717-v8
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ ROLE=Master
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + set -x
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + set_log_tag pre-activate-component-spark
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + local -r tag=dataproc-pre-activate-component-spark
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + exec
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ logger -s -t 'dataproc-pre-activate-component-spark[4101]'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + readonly SPARK_STANDALONE_LOCAL_DATA_DIR=/hadoop/spark/local-dir
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + SPARK_STANDALONE_LOCAL_DATA_DIR=/hadoop/spark/local-dir
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + pre_activate_spark
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + update_bq_connector
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + local connector_version
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: ++ get_metadata_spark_bq_connector_version
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: ++ get_dataproc_metadata DATAPROC_METADATA_SPARK_BQ_CONNECTOR_VERSION attributes/SPARK_BQ_CONNECTOR_VERSION
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: ++ set +x
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + connector_version=
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + local connector_url
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: ++ get_metadata_spark_bq_connector_url
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: ++ get_dataproc_metadata DATAPROC_METADATA_SPARK_BQ_CONNECTOR_URL attributes/SPARK_BQ_CONNECTOR_URL
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: ++ set +x
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + connector_url=
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + local jar_name
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + [[ -z '' ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + [[ -n '' ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + [[ -n '' ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + init_local_dirs_common
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + mkdir -p /hadoop/spark/tmp /hadoop/spark/work /var/log/spark
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + chown spark:spark -R /hadoop/spark /var/log/spark
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + chmod 1777 -R /hadoop/spark /var/log/spark
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + ln -sf /hadoop/spark/work /usr/lib/spark/work
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + configure_env_common
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + cat
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + configure_event_log_dir
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + local persist_history_to_gcs
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: ++ get_dataproc_property_or_default job.history.to-gcs.enabled false
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: ++ set +x
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + persist_history_to_gcs=true
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + local event_log_dir=hdfs://cluster-dip-01-m/user/spark/eventlog
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + [[ true == \t\r\u\e ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + loginfo 'Enabling persisting Spark job history files to GCS.'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + echo 'Enabling persisting Spark job history files to GCS.'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: Enabling persisting Spark job history files to GCS.
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + [[ -n dataproc-temp-us-east1-679657336577-juievcnm ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + local gcs_history_dir_path=0b63cfa9-1f8a-481e-87c8-a0cf661f3625/spark-job-history
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + event_log_dir=gs://dataproc-temp-us-east1-679657336577-juievcnm/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/spark-job-history
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + cat
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + configure_arrow
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + [[ 2.2 == \1\.\5 ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + configure_env_yarn
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + cat
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + is_version_at_least 2.2 2.1
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + local reenable_x=false
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + [[ -o xtrace ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + set +x
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + case ${compare_versions_result} in
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + return 0
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + cat
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + cat
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + configure_defaults_yarn
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + cat
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + is_version_at_least 2.2 2.0
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + local reenable_x=false
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + [[ -o xtrace ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + set +x
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + case ${compare_versions_result} in
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + return 0
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + is_component_selected hive-server2
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + local -r component=hive-server2
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + local activated_components
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: ++ get_components_to_activate
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: ++ get_dataproc_property dataproc.components.activate
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: ++ tr '[:upper:]' '[:lower:]'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: ++ set +x
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + activated_components='knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + [[ knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3 == *hive-server2* ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + cat
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + is_version_at_least 2.2 2.1
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + local reenable_x=false
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + [[ -o xtrace ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + set +x
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + case ${compare_versions_result} in
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + return 0
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + cat
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + local enable_docker_yarn
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: ++ get_dataproc_property_or_default yarn.docker.enable ''
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: ++ set +x
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + enable_docker_yarn=
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + [[ -z '' ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: ++ get_dataproc_property_or_default docker.yarn.enable ''
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: ++ set +x
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + enable_docker_yarn=
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + [[ '' == \t\r\u\e ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + cat
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + configure_efm
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + local spark_efm_property
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: ++ get_dataproc_property efm.spark.shuffle
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: ++ set +x
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + spark_efm_property=
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + [[ '' == \p\r\i\m\a\r\y\-\w\o\r\k\e\r ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + is_component_selected kerberos
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + local -r component=kerberos
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + local activated_components
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: ++ get_components_to_activate
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: ++ tr '[:upper:]' '[:lower:]'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: ++ get_dataproc_property dataproc.components.activate
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: ++ set +x
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + activated_components='knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + [[ knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3 == *kerberos* ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + merge_user_properties
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + merge_java_properties /tmp/cluster/properties/spark.properties /etc/spark/conf/spark-defaults.conf
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + local -r src=/tmp/cluster/properties/spark.properties
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + local -r dest=/etc/spark/conf/spark-defaults.conf
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + local -r 'header=\n# User-supplied properties.'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + [[ ! -f /tmp/cluster/properties/spark.properties ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + echo -e '\n# User-supplied properties.'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + cat /tmp/cluster/properties/spark.properties
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + loginfo 'Merged /tmp/cluster/properties/spark.properties.'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + echo 'Merged /tmp/cluster/properties/spark.properties.'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: Merged /tmp/cluster/properties/spark.properties.
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + merge_sh_env_vars /tmp/cluster/properties/spark-env.sh /etc/spark/conf/spark-env.sh
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + local src=/tmp/cluster/properties/spark-env.sh
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + local dest=/etc/spark/conf/spark-env.sh
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + [[ ! -f /tmp/cluster/properties/spark-env.sh ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + echo -e '\n# User-supplied properties.'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + cat /tmp/cluster/properties/spark-env.sh
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + loginfo 'Merged /tmp/cluster/properties/spark-env.sh.'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + echo 'Merged /tmp/cluster/properties/spark-env.sh.'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: Merged /tmp/cluster/properties/spark-env.sh.
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + [[ standard == \s\t\a\n\d\a\r\d ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + is_version_at_least 2.2 2.1
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + local reenable_x=false
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + [[ -o xtrace ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + set +x
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + case ${compare_versions_result} in
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + return 0
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + merge_java_properties /tmp/cluster/properties/spark-log4j2.properties /etc/spark/conf/log4j2.properties
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + local -r src=/tmp/cluster/properties/spark-log4j2.properties
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + local -r dest=/etc/spark/conf/log4j2.properties
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + local -r 'header=\n# User-supplied properties.'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + [[ ! -f /tmp/cluster/properties/spark-log4j2.properties ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + echo -e '\n# User-supplied properties.'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + cat /tmp/cluster/properties/spark-log4j2.properties
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + loginfo 'Merged /tmp/cluster/properties/spark-log4j2.properties.'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + echo 'Merged /tmp/cluster/properties/spark-log4j2.properties.'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: Merged /tmp/cluster/properties/spark-log4j2.properties.
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + configure_lineage
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + [[ ! -v OPENLINEAGE_VERSION ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + local lineage_enabled=false
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + is_rm_image
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\r\a\y ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: ++ get_metadata_dataproc_lineage_enabled
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: ++ get_dataproc_metadata DATAPROC_METADATA_LINEAGE_ENABLED attributes/DATAPROC_LINEAGE_ENABLED
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: ++ set +x
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + lineage_enabled=
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: ++ get_dataproc_property_or_default dataproc.lineage.enabled ''
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: ++ set +x
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + lineage_enabled=
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + [[ '' == \t\r\u\e ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + is_version_at_least 2.2 2.0
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + local reenable_x=false
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + [[ -o xtrace ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + set +x
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + case ${compare_versions_result} in
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + return 0
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + configure_broadcast_join 0.0075 200
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + local -r fraction=0.0075
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + local -r max_mb=200
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + local user_broadcast_join_threshold_mb
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: ++ get_java_property /etc/spark/conf/spark-defaults.conf spark.sql.autoBroadcastJoinThreshold
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: ++ set +x
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + user_broadcast_join_threshold_mb=
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + [[ -n '' ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + local spark_executor_memory
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: ++ get_java_property /etc/spark/conf/spark-defaults.conf spark.executor.memory
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: ++ set +x
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + spark_executor_memory=2893m
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + local spark_executor_memory_bytes
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: ++ get_size_in_bytes 2893m
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: ++ local size=2893M
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: ++ size=2893M
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: ++ size=2893M
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: ++ numfmt --from=iec 2893M
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + spark_executor_memory_bytes=3033530368
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + local broadcast_join_threshold_mb
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: ++ python -c 'print(min(max(int(3033530368 * 0.0075 / 1024 / 1024), 10), 200))'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + broadcast_join_threshold_mb=21
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + cat
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + is_rm_image
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\r\a\y ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + [[ Master == \M\a\s\t\e\r ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + [[ 0 == \0 ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + run_in_background --tag spark-create-event-log-dir create_event_log_dir
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + local -r pid=4233
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + [[ --tag == \-\-\t\a\g ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + shift 2
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + [[ ! -f /tmp/dataproc/commands/4233.running ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + echo create_event_log_dir
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + echo 'Started background process [create_event_log_dir] as pid 4233'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: Started background process [create_event_log_dir] as pid 4233
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + local cohort
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + trap 'echo "$?" >"${COMMANDS_TMP_DIR}/${BASHPID}.exitcode"' EXIT
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + run_with_logger --tag spark-create-event-log-dir create_event_log_dir
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + local tag=dataproc-script
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + local pid=4233
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + [[ --tag == \-\-\t\a\g ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + tag=dataproc-spark-create-event-log-dir
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + shift 2
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + [[ 1 -eq 0 ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + create_event_log_dir
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: ++ get_dataproc_property internal.cohort
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: ++ set +x
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: ++ logger -s -t 'dataproc-spark-create-event-log-dir[4233]'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: <13>Apr 27 18:07:42 dataproc-spark-create-event-log-dir[4233]: + local event_log_dir
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: <13>Apr 27 18:07:42 dataproc-spark-create-event-log-dir[4233]: ++ get_java_property /etc/spark/conf/spark-defaults.conf spark.eventLog.dir
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: <13>Apr 27 18:07:42 dataproc-spark-create-event-log-dir[4233]: ++ set +x
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: <13>Apr 27 18:07:42 dataproc-spark-create-event-log-dir[4233]: + event_log_dir=gs://dataproc-temp-us-east1-679657336577-juievcnm/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/spark-job-history
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: <13>Apr 27 18:07:42 dataproc-spark-create-event-log-dir[4233]: + [[ gs://dataproc-temp-us-east1-679657336577-juievcnm/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/spark-job-history == gs://* ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: <13>Apr 27 18:07:42 dataproc-spark-create-event-log-dir[4233]: + loginfo 'Creating Spark event log dir in GCS'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: <13>Apr 27 18:07:42 dataproc-spark-create-event-log-dir[4233]: + echo 'Creating Spark event log dir in GCS'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: <13>Apr 27 18:07:42 dataproc-spark-create-event-log-dir[4233]: Creating Spark event log dir in GCS
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: <13>Apr 27 18:07:42 dataproc-spark-create-event-log-dir[4233]: + event_log_dir=dataproc-temp-us-east1-679657336577-juievcnm/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/spark-job-history
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: <13>Apr 27 18:07:42 dataproc-spark-create-event-log-dir[4233]: + event_log_dir=dataproc-temp-us-east1-679657336577-juievcnm/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/spark-job-history
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: <13>Apr 27 18:07:42 dataproc-spark-create-event-log-dir[4233]: + local event_log_dir_bucket=dataproc-temp-us-east1-679657336577-juievcnm
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: <13>Apr 27 18:07:42 dataproc-spark-create-event-log-dir[4233]: + local event_log_dir_path=0b63cfa9-1f8a-481e-87c8-a0cf661f3625/spark-job-history
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: <13>Apr 27 18:07:42 dataproc-spark-create-event-log-dir[4233]: + set +xe
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + cohort=
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + is_version_at_least 2.2 3.0
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + local reenable_x=false
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + [[ -o xtrace ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + set +x
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + case ${compare_versions_result} in
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: touch /tmp/dataproc/sentinel/spark.pre-activate
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + return 1
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: + [[ -n '' ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: Running task: yarn.pre-activate
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: bash -e /usr/local/share/google/dataproc/bdutil/components/pre-activate/yarn.sh 2.2 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/yarn.sh"; exit 1; }
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: <13>Apr 27 18:07:42 dataproc-spark-create-event-log-dir[4233]: + [[ 0 -ne 0 ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-spark[4101]: <13>Apr 27 18:07:42 dataproc-spark-create-event-log-dir[4233]: + set +x
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + set_log_tag pre-activate-component-yarn
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + local -r tag=dataproc-pre-activate-component-yarn
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + exec
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ logger -s -t 'dataproc-pre-activate-component-yarn[4344]'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-yarn[4344]: ++ get_dataproc_property yarn.atsv2.bigtable.instance
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-yarn[4344]: ++ set +x
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-yarn[4344]: + readonly ATSV2_BIGTABLE_RESOURCE=
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-yarn[4344]: + ATSV2_BIGTABLE_RESOURCE=
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-yarn[4344]: + [[ -n '' ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-yarn[4344]: + rm -Rf /usr/local/share/google/dataproc/lib/bigtable-hbase-2.x-hadoop-1.26.2.jar
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-yarn[4344]: + is_component_selected kerberos
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-yarn[4344]: + local -r component=kerberos
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-yarn[4344]: + local activated_components
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-yarn[4344]: ++ get_components_to_activate
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-yarn[4344]: ++ tr '[:upper:]' '[:lower:]'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-yarn[4344]: ++ get_dataproc_property dataproc.components.activate
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-yarn[4344]: ++ set +x
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-yarn[4344]: + activated_components='knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-activate-component-yarn[4344]: + [[ knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3 == *kerberos* ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: touch /tmp/dataproc/sentinel/yarn.pre-activate
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + echo 'All pre-activate scripts done'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: All pre-activate scripts done
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + loginfo 'Pre-uninstalling unselected components'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + echo 'Pre-uninstalling unselected components'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: Pre-uninstalling unselected components
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + pre_uninstall_components delta docker-ce dpms-proxy flink google-fluentd-container hive-webhcat-server hudi iceberg jupyter jupyter-kernel-gateway kerberos ranger rubix solr-server stackdriver-agent-container trino zeppelin zookeeper-server
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + components=('delta' 'docker-ce' 'dpms-proxy' 'flink' 'google-fluentd-container' 'hive-webhcat-server' 'hudi' 'iceberg' 'jupyter' 'jupyter-kernel-gateway' 'kerberos' 'ranger' 'rubix' 'solr-server' 'stackdriver-agent-container' 'trino' 'zeppelin' 'zookeeper-server')
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + local components
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + mkdir -p /tmp/dataproc/components/pre-uninstall
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + for component in "${components[@]}"
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + loginfo 'Pre-uninstalling component delta'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + echo 'Pre-uninstalling component delta'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: Pre-uninstalling component delta
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + pre_uninstall_component delta
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + local -r component=delta
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/delta.sh
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/delta.sh ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + echo 'Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/delta.sh'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/delta.sh
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + touch /tmp/dataproc/components/pre-uninstall/delta.running
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + local exit_code=0
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + bash -e /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/delta.sh
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + set_log_tag pre-uninstall-component-delta
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + local -r tag=dataproc-pre-uninstall-component-delta
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + exec
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + [[ 0 -ne 0 ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + touch /tmp/dataproc/components/pre-uninstall/delta.done
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ logger -s -t 'dataproc-pre-uninstall-component-delta[4426]'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + for component in "${components[@]}"
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + loginfo 'Pre-uninstalling component docker-ce'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + echo 'Pre-uninstalling component docker-ce'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: Pre-uninstalling component docker-ce
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + pre_uninstall_component docker-ce
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + local -r component=docker-ce
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/docker-ce.sh
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/docker-ce.sh ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + echo 'Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/docker-ce.sh'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/docker-ce.sh
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + touch /tmp/dataproc/components/pre-uninstall/docker-ce.running
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + local exit_code=0
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + bash -e /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/docker-ce.sh
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + set_log_tag pre-uninstall-component-docker-ce
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + local -r tag=dataproc-pre-uninstall-component-docker-ce
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + exec
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + [[ 0 -ne 0 ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + touch /tmp/dataproc/components/pre-uninstall/docker-ce.done
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ logger -s -t 'dataproc-pre-uninstall-component-docker-ce[4434]'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-uninstall-component-docker-ce[4434]: + mark_packages_to_uninstall docker-ce
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-uninstall-component-docker-ce[4434]: + for package in "$@"
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-uninstall-component-docker-ce[4434]: + touch /tmp/dataproc/uninstall/docker-ce
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + for component in "${components[@]}"
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + loginfo 'Pre-uninstalling component dpms-proxy'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + echo 'Pre-uninstalling component dpms-proxy'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: Pre-uninstalling component dpms-proxy
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + pre_uninstall_component dpms-proxy
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + local -r component=dpms-proxy
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/dpms-proxy.sh
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/dpms-proxy.sh ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + echo 'Component dpms-proxy doesn'\''t have a pre-uninstall script'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: Component dpms-proxy doesn't have a pre-uninstall script
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + for component in "${components[@]}"
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + loginfo 'Pre-uninstalling component flink'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + echo 'Pre-uninstalling component flink'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: Pre-uninstalling component flink
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + pre_uninstall_component flink
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + local -r component=flink
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/flink.sh
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/flink.sh ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + echo 'Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/flink.sh'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/flink.sh
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + touch /tmp/dataproc/components/pre-uninstall/flink.running
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + local exit_code=0
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + bash -e /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/flink.sh
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + set_log_tag pre-uninstall-component-flink
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + local -r tag=dataproc-pre-uninstall-component-flink
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + exec
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + [[ 0 -ne 0 ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + touch /tmp/dataproc/components/pre-uninstall/flink.done
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ logger -s -t 'dataproc-pre-uninstall-component-flink[4442]'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-uninstall-component-flink[4442]: + mark_packages_to_uninstall flink
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-uninstall-component-flink[4442]: + for package in "$@"
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-uninstall-component-flink[4442]: + touch /tmp/dataproc/uninstall/flink
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + for component in "${components[@]}"
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + loginfo 'Pre-uninstalling component google-fluentd-container'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + echo 'Pre-uninstalling component google-fluentd-container'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: Pre-uninstalling component google-fluentd-container
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + pre_uninstall_component google-fluentd-container
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + local -r component=google-fluentd-container
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/google-fluentd-container.sh
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/google-fluentd-container.sh ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + echo 'Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/google-fluentd-container.sh'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/google-fluentd-container.sh
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + touch /tmp/dataproc/components/pre-uninstall/google-fluentd-container.running
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + local exit_code=0
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + bash -e /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/google-fluentd-container.sh
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + [[ 0 -ne 0 ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + touch /tmp/dataproc/components/pre-uninstall/google-fluentd-container.done
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + for component in "${components[@]}"
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + loginfo 'Pre-uninstalling component hive-webhcat-server'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + echo 'Pre-uninstalling component hive-webhcat-server'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: Pre-uninstalling component hive-webhcat-server
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + pre_uninstall_component hive-webhcat-server
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + local -r component=hive-webhcat-server
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/hive-webhcat-server.sh
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/hive-webhcat-server.sh ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + echo 'Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/hive-webhcat-server.sh'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/hive-webhcat-server.sh
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + touch /tmp/dataproc/components/pre-uninstall/hive-webhcat-server.running
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + local exit_code=0
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + bash -e /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/hive-webhcat-server.sh
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + set_log_tag pre-uninstall-component-hive-webhcat-server
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + local -r tag=dataproc-pre-uninstall-component-hive-webhcat-server
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + exec
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + [[ 0 -ne 0 ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + touch /tmp/dataproc/components/pre-uninstall/hive-webhcat-server.done
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: ++ logger -s -t 'dataproc-pre-uninstall-component-hive-webhcat-server[4510]'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-uninstall-component-hive-webhcat-server[4510]: + mark_packages_to_uninstall hive-webhcat-server
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-uninstall-component-hive-webhcat-server[4510]: + for package in "$@"
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: <13>Apr 27 18:07:42 dataproc-pre-uninstall-component-hive-webhcat-server[4510]: + touch /tmp/dataproc/uninstall/hive-webhcat-server
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + for component in "${components[@]}"
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + loginfo 'Pre-uninstalling component hudi'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + echo 'Pre-uninstalling component hudi'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: Pre-uninstalling component hudi
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + pre_uninstall_component hudi
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + local -r component=hudi
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/hudi.sh
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/hudi.sh ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + echo 'Component hudi doesn'\''t have a pre-uninstall script'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: Component hudi doesn't have a pre-uninstall script
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + for component in "${components[@]}"
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + loginfo 'Pre-uninstalling component iceberg'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + echo 'Pre-uninstalling component iceberg'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: Pre-uninstalling component iceberg
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + pre_uninstall_component iceberg
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + local -r component=iceberg
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/iceberg.sh
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/iceberg.sh ]]
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + echo 'Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/iceberg.sh'
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/iceberg.sh
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + touch /tmp/dataproc/components/pre-uninstall/iceberg.running
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + local exit_code=0
<13>Apr 27 18:07:42 dataproc-startup-script[1265]: + bash -e /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/iceberg.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + set_log_tag pre-uninstall-component-iceberg
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r tag=dataproc-pre-uninstall-component-iceberg
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + exec
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ 0 -ne 0 ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + touch /tmp/dataproc/components/pre-uninstall/iceberg.done
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: ++ logger -s -t 'dataproc-pre-uninstall-component-iceberg[4575]'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-pre-uninstall-component-iceberg[4575]: + mark_packages_to_uninstall iceberg
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-pre-uninstall-component-iceberg[4575]: + for package in "$@"
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-pre-uninstall-component-iceberg[4575]: + touch /tmp/dataproc/uninstall/iceberg
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + for component in "${components[@]}"
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + loginfo 'Pre-uninstalling component jupyter'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'Pre-uninstalling component jupyter'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: Pre-uninstalling component jupyter
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + pre_uninstall_component jupyter
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r component=jupyter
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/jupyter.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/jupyter.sh ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'Component jupyter doesn'\''t have a pre-uninstall script'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: Component jupyter doesn't have a pre-uninstall script
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + for component in "${components[@]}"
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + loginfo 'Pre-uninstalling component jupyter-kernel-gateway'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'Pre-uninstalling component jupyter-kernel-gateway'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: Pre-uninstalling component jupyter-kernel-gateway
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + pre_uninstall_component jupyter-kernel-gateway
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r component=jupyter-kernel-gateway
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/jupyter-kernel-gateway.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/jupyter-kernel-gateway.sh ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'Component jupyter-kernel-gateway doesn'\''t have a pre-uninstall script'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: Component jupyter-kernel-gateway doesn't have a pre-uninstall script
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + for component in "${components[@]}"
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + loginfo 'Pre-uninstalling component kerberos'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'Pre-uninstalling component kerberos'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: Pre-uninstalling component kerberos
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + pre_uninstall_component kerberos
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r component=kerberos
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/kerberos.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/kerberos.sh ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/kerberos.sh'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/kerberos.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + touch /tmp/dataproc/components/pre-uninstall/kerberos.running
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local exit_code=0
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + bash -e /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/kerberos.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + set_log_tag pre-uninstall-component-kerberos
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r tag=dataproc-pre-uninstall-component-kerberos
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + exec
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: ++ logger -s -t 'dataproc-pre-uninstall-component-kerberos[4641]'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-pre-uninstall-component-kerberos[4641]: + is_rocky
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-pre-uninstall-component-kerberos[4641]: ++ os_id
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-pre-uninstall-component-kerberos[4641]: ++ grep '^ID=' /etc/os-release
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-pre-uninstall-component-kerberos[4641]: ++ cut -d= -f2
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-pre-uninstall-component-kerberos[4641]: ++ xargs
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-pre-uninstall-component-kerberos[4641]: + [[ debian == \r\o\c\k\y ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-pre-uninstall-component-kerberos[4641]: + mark_packages_to_uninstall krb5-user krb5-config krb5-kdc krb5-admin-server krb5-kpropd xinetd
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-pre-uninstall-component-kerberos[4641]: + for package in "$@"
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-pre-uninstall-component-kerberos[4641]: + touch /tmp/dataproc/uninstall/krb5-user
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-pre-uninstall-component-kerberos[4641]: + for package in "$@"
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-pre-uninstall-component-kerberos[4641]: + touch /tmp/dataproc/uninstall/krb5-config
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-pre-uninstall-component-kerberos[4641]: + for package in "$@"
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-pre-uninstall-component-kerberos[4641]: + touch /tmp/dataproc/uninstall/krb5-kdc
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-pre-uninstall-component-kerberos[4641]: + for package in "$@"
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-pre-uninstall-component-kerberos[4641]: + touch /tmp/dataproc/uninstall/krb5-admin-server
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-pre-uninstall-component-kerberos[4641]: + for package in "$@"
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-pre-uninstall-component-kerberos[4641]: + touch /tmp/dataproc/uninstall/krb5-kpropd
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-pre-uninstall-component-kerberos[4641]: + for package in "$@"
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-pre-uninstall-component-kerberos[4641]: + touch /tmp/dataproc/uninstall/xinetd
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ 0 -ne 0 ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + touch /tmp/dataproc/components/pre-uninstall/kerberos.done
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + for component in "${components[@]}"
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + loginfo 'Pre-uninstalling component ranger'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'Pre-uninstalling component ranger'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: Pre-uninstalling component ranger
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + pre_uninstall_component ranger
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r component=ranger
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/ranger.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/ranger.sh ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/ranger.sh'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/ranger.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + touch /tmp/dataproc/components/pre-uninstall/ranger.running
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local exit_code=0
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + bash -e /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/ranger.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + set_log_tag pre-uninstall-component-ranger
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r tag=dataproc-pre-uninstall-component-ranger
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + exec
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ 0 -ne 0 ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + touch /tmp/dataproc/components/pre-uninstall/ranger.done
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: ++ logger -s -t 'dataproc-pre-uninstall-component-ranger[4716]'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-pre-uninstall-component-ranger[4716]: + mark_packages_to_uninstall ranger
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-pre-uninstall-component-ranger[4716]: + for package in "$@"
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-pre-uninstall-component-ranger[4716]: + touch /tmp/dataproc/uninstall/ranger
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + for component in "${components[@]}"
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + loginfo 'Pre-uninstalling component rubix'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'Pre-uninstalling component rubix'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: Pre-uninstalling component rubix
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + pre_uninstall_component rubix
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r component=rubix
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/rubix.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/rubix.sh ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/rubix.sh'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/rubix.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + touch /tmp/dataproc/components/pre-uninstall/rubix.running
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local exit_code=0
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + bash -e /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/rubix.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + set_log_tag pre-uninstall-component-rubix
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r tag=dataproc-pre-uninstall-component-rubix
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + exec
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ 0 -ne 0 ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + touch /tmp/dataproc/components/pre-uninstall/rubix.done
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: ++ logger -s -t 'dataproc-pre-uninstall-component-rubix[4781]'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-pre-uninstall-component-rubix[4781]: + mark_packages_to_uninstall rubix
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-pre-uninstall-component-rubix[4781]: + for package in "$@"
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-pre-uninstall-component-rubix[4781]: + touch /tmp/dataproc/uninstall/rubix
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + for component in "${components[@]}"
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + loginfo 'Pre-uninstalling component solr-server'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'Pre-uninstalling component solr-server'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: Pre-uninstalling component solr-server
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + pre_uninstall_component solr-server
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r component=solr-server
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/solr-server.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/solr-server.sh ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/solr-server.sh'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/solr-server.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + touch /tmp/dataproc/components/pre-uninstall/solr-server.running
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local exit_code=0
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + bash -e /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/solr-server.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + set_log_tag pre-uninstall-component-solr-server
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r tag=dataproc-pre-uninstall-component-solr-server
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + exec
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ 0 -ne 0 ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + touch /tmp/dataproc/components/pre-uninstall/solr-server.done
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: ++ logger -s -t 'dataproc-pre-uninstall-component-solr-server[4846]'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-pre-uninstall-component-solr-server[4846]: + mark_packages_to_uninstall solr-server
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-pre-uninstall-component-solr-server[4846]: + for package in "$@"
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-pre-uninstall-component-solr-server[4846]: + touch /tmp/dataproc/uninstall/solr-server
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + for component in "${components[@]}"
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + loginfo 'Pre-uninstalling component stackdriver-agent-container'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'Pre-uninstalling component stackdriver-agent-container'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: Pre-uninstalling component stackdriver-agent-container
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + pre_uninstall_component stackdriver-agent-container
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r component=stackdriver-agent-container
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/stackdriver-agent-container.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/stackdriver-agent-container.sh ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/stackdriver-agent-container.sh'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/stackdriver-agent-container.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + touch /tmp/dataproc/components/pre-uninstall/stackdriver-agent-container.running
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local exit_code=0
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + bash -e /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/stackdriver-agent-container.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ 0 -ne 0 ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + touch /tmp/dataproc/components/pre-uninstall/stackdriver-agent-container.done
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + for component in "${components[@]}"
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + loginfo 'Pre-uninstalling component trino'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'Pre-uninstalling component trino'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: Pre-uninstalling component trino
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + pre_uninstall_component trino
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r component=trino
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/trino.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/trino.sh ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/trino.sh'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/trino.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + touch /tmp/dataproc/components/pre-uninstall/trino.running
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local exit_code=0
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + bash -e /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/trino.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + set_log_tag pre-uninstall-component-trino
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r tag=dataproc-pre-uninstall-component-trino
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + exec
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: ++ logger -s -t 'dataproc-pre-uninstall-component-trino[4918]'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ 0 -ne 0 ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + touch /tmp/dataproc/components/pre-uninstall/trino.done
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-pre-uninstall-component-trino[4918]: + mark_packages_to_uninstall trino
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-pre-uninstall-component-trino[4918]: + for package in "$@"
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-pre-uninstall-component-trino[4918]: + touch /tmp/dataproc/uninstall/trino
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + for component in "${components[@]}"
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + loginfo 'Pre-uninstalling component zeppelin'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'Pre-uninstalling component zeppelin'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: Pre-uninstalling component zeppelin
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + pre_uninstall_component zeppelin
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r component=zeppelin
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/zeppelin.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/zeppelin.sh ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/zeppelin.sh'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/zeppelin.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + touch /tmp/dataproc/components/pre-uninstall/zeppelin.running
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local exit_code=0
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + bash -e /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/zeppelin.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + set_log_tag pre-uninstall-component-zeppelin
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r tag=dataproc-pre-uninstall-component-zeppelin
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + exec
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: ++ logger -s -t 'dataproc-pre-uninstall-component-zeppelin[4983]'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ 0 -ne 0 ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + touch /tmp/dataproc/components/pre-uninstall/zeppelin.done
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-pre-uninstall-component-zeppelin[4983]: + mark_packages_to_uninstall zeppelin
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-pre-uninstall-component-zeppelin[4983]: + for package in "$@"
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-pre-uninstall-component-zeppelin[4983]: + touch /tmp/dataproc/uninstall/zeppelin
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + for component in "${components[@]}"
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + loginfo 'Pre-uninstalling component zookeeper-server'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'Pre-uninstalling component zookeeper-server'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: Pre-uninstalling component zookeeper-server
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + pre_uninstall_component zookeeper-server
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r component=zookeeper-server
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/zookeeper-server.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/zookeeper-server.sh ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/zookeeper-server.sh'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/zookeeper-server.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + touch /tmp/dataproc/components/pre-uninstall/zookeeper-server.running
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local exit_code=0
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + bash -e /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/zookeeper-server.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-pre-activate-component-spark[4101]: <13>Apr 27 18:07:43 dataproc-spark-create-event-log-dir[4233]: + [[ 200 -ge 200 ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-pre-activate-component-spark[4101]: <13>Apr 27 18:07:43 dataproc-spark-create-event-log-dir[4233]: + [[ 200 -le 299 ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-pre-activate-component-spark[4101]: <13>Apr 27 18:07:43 dataproc-spark-create-event-log-dir[4233]: + return 0
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-pre-activate-component-spark[4101]: ++ echo 0
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + set_log_tag pre-uninstall-component-zookeper
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r tag=dataproc-pre-uninstall-component-zookeper
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + exec
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: ++ logger -s -t 'dataproc-pre-uninstall-component-zookeper[5048]'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ 0 -ne 0 ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + touch /tmp/dataproc/components/pre-uninstall/zookeeper-server.done
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-pre-uninstall-component-zookeper[5048]: + mark_packages_to_uninstall zookeeper-server
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-pre-uninstall-component-zookeper[5048]: + for package in "$@"
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-pre-uninstall-component-zookeper[5048]: + touch /tmp/dataproc/uninstall/zookeeper-server
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + loginfo 'Uninstalling packages which must be uninstalled before activating components'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'Uninstalling packages which must be uninstalled before activating components'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: Uninstalling packages which must be uninstalled before activating components
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + uninstall_packages_pre_activate
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local packages_to_uninstall
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + packages_to_uninstall=($(list_packages_to_uninstall_pre_activate))
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: ++ list_packages_to_uninstall_pre_activate
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: ++ find /tmp/dataproc/uninstall-pre-activate/ -type f -exec basename '{}' ';'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ 0 -gt 0 ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + loginfo 'Starting to uninstall artifacts'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'Starting to uninstall artifacts'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: Starting to uninstall artifacts
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + start_uninstall_artifacts
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local blocking_default=
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ standard == \s\t\a\n\d\a\r\d ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + is_version_at_least 2.2 2.0
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local reenable_x=false
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ -o xtrace ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + set +x
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + case ${compare_versions_result} in
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + return 0
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local blocking
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: ++ get_dataproc_property_or_default dataproc.uninstall.packages.blocking ''
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + blocking=
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ standard == \s\t\a\n\d\a\r\d ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ -z '' ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local init_action_count
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: ++ /usr/share/google/get_metadata_value attributes/dataproc-initialization-script-count
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + init_action_count=0
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ 0 == \0 ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + blocking=false
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ false == \t\r\u\e ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + disable_unattended_upgrades_shutdown
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + run_with_logger --tag delayed_uninstall_artifacts delayed_uninstall_artifacts
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + is_rocky
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local tag=dataproc-script
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local pid=5070
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ --tag == \-\-\t\a\g ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + tag=dataproc-delayed_uninstall_artifacts
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + shift 2
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ 1 -eq 0 ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + delayed_uninstall_artifacts
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: ++ os_id
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: ++ cut -d= -f2
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: ++ xargs
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: ++ grep '^ID=' /etc/os-release
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: ++ logger -s -t 'dataproc-delayed_uninstall_artifacts[5070]'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-delayed_uninstall_artifacts[5070]: + sleep 60
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ debian == \r\o\c\k\y ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: ++ get_metadata_value scheduling/preemptible
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ FALSE = \T\R\U\E ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: ++ get_dataproc_property_or_default dataproc.multi.user.metadata.proxy.enabled false
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + MULTI_USER_METADATA_PROXY_ENABLED=false
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + readonly MULTI_USER_METADATA_PROXY_ENABLED
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ false == \t\r\u\e ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + loginfo 'Starting services'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'Starting services'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: Starting services
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + start_component_services dpms-proxy hadoop-hdfs-namenode hive-metastore hive-server2 knox proxy-agent spark-history-server hadoop-yarn-resourcemanager hive-webhcat-server solr-server jupyter-kernel-gateway hadoop-mapreduce-historyserver mysql-server hadoop-yarn-timelineserver jupyter zeppelin hadoop-hdfs-secondarynamenode
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + services=('dpms-proxy' 'hadoop-hdfs-namenode' 'hive-metastore' 'hive-server2' 'knox' 'proxy-agent' 'spark-history-server' 'hadoop-yarn-resourcemanager' 'hive-webhcat-server' 'solr-server' 'jupyter-kernel-gateway' 'hadoop-mapreduce-historyserver' 'mysql-server' 'hadoop-yarn-timelineserver' 'jupyter' 'zeppelin' 'hadoop-hdfs-secondarynamenode')
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r services
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local service
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + for service in "${services[@]}"
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + in_array dpms-proxy DATAPROC_COMPONENTS
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r value=dpms-proxy
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -n values=DATAPROC_COMPONENTS
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ !  dpms-proxy hdfs hive-metastore hive-server2 kerberos knox mapreduce mysql npd proxy-agent spark tez yarn zookeeper-server jupyter fluentbit-ucp otel-ucp stackdriver-agent-container google-fluentd-container earlyoom rubix miniconda3 docker-ce hive-webhcat-server zeppelin flink hudi pig ranger solr-server trino iceberg delta jupyter-kernel-gateway  == *\ \d\p\m\s\-\p\r\o\x\y\ * ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + continue
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + for service in "${services[@]}"
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + in_array hadoop-hdfs-namenode DATAPROC_COMPONENTS
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r value=hadoop-hdfs-namenode
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -n values=DATAPROC_COMPONENTS
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ !  dpms-proxy hdfs hive-metastore hive-server2 kerberos knox mapreduce mysql npd proxy-agent spark tez yarn zookeeper-server jupyter fluentbit-ucp otel-ucp stackdriver-agent-container google-fluentd-container earlyoom rubix miniconda3 docker-ce hive-webhcat-server zeppelin flink hudi pig ranger solr-server trino iceberg delta jupyter-kernel-gateway  == *\ \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e\ * ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + return 1
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + in_array hadoop-hdfs-namenode COMPONENT_SERVICES
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r value=hadoop-hdfs-namenode
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -n values=COMPONENT_SERVICES
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ !  hadoop-hdfs-namenode hadoop-hdfs-datanode hadoop-hdfs-zkfc hadoop-hdfs-secondarynamenode hadoop-hdfs-journalnode hive-metastore hive-server2 mysql-server spark-history-server  == *\ \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e\ * ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + continue
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + for service in "${services[@]}"
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + in_array hive-metastore DATAPROC_COMPONENTS
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r value=hive-metastore
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -n values=DATAPROC_COMPONENTS
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ !  dpms-proxy hdfs hive-metastore hive-server2 kerberos knox mapreduce mysql npd proxy-agent spark tez yarn zookeeper-server jupyter fluentbit-ucp otel-ucp stackdriver-agent-container google-fluentd-container earlyoom rubix miniconda3 docker-ce hive-webhcat-server zeppelin flink hudi pig ranger solr-server trino iceberg delta jupyter-kernel-gateway  == *\ \h\i\v\e\-\m\e\t\a\s\t\o\r\e\ * ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + continue
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + for service in "${services[@]}"
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + in_array hive-server2 DATAPROC_COMPONENTS
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r value=hive-server2
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -n values=DATAPROC_COMPONENTS
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ !  dpms-proxy hdfs hive-metastore hive-server2 kerberos knox mapreduce mysql npd proxy-agent spark tez yarn zookeeper-server jupyter fluentbit-ucp otel-ucp stackdriver-agent-container google-fluentd-container earlyoom rubix miniconda3 docker-ce hive-webhcat-server zeppelin flink hudi pig ranger solr-server trino iceberg delta jupyter-kernel-gateway  == *\ \h\i\v\e\-\s\e\r\v\e\r\2\ * ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + continue
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + for service in "${services[@]}"
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + in_array knox DATAPROC_COMPONENTS
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r value=knox
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -n values=DATAPROC_COMPONENTS
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ !  dpms-proxy hdfs hive-metastore hive-server2 kerberos knox mapreduce mysql npd proxy-agent spark tez yarn zookeeper-server jupyter fluentbit-ucp otel-ucp stackdriver-agent-container google-fluentd-container earlyoom rubix miniconda3 docker-ce hive-webhcat-server zeppelin flink hudi pig ranger solr-server trino iceberg delta jupyter-kernel-gateway  == *\ \k\n\o\x\ * ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + continue
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + for service in "${services[@]}"
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + in_array proxy-agent DATAPROC_COMPONENTS
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r value=proxy-agent
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -n values=DATAPROC_COMPONENTS
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ !  dpms-proxy hdfs hive-metastore hive-server2 kerberos knox mapreduce mysql npd proxy-agent spark tez yarn zookeeper-server jupyter fluentbit-ucp otel-ucp stackdriver-agent-container google-fluentd-container earlyoom rubix miniconda3 docker-ce hive-webhcat-server zeppelin flink hudi pig ranger solr-server trino iceberg delta jupyter-kernel-gateway  == *\ \p\r\o\x\y\-\a\g\e\n\t\ * ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + continue
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + for service in "${services[@]}"
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + in_array spark-history-server DATAPROC_COMPONENTS
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r value=spark-history-server
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -n values=DATAPROC_COMPONENTS
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ !  dpms-proxy hdfs hive-metastore hive-server2 kerberos knox mapreduce mysql npd proxy-agent spark tez yarn zookeeper-server jupyter fluentbit-ucp otel-ucp stackdriver-agent-container google-fluentd-container earlyoom rubix miniconda3 docker-ce hive-webhcat-server zeppelin flink hudi pig ranger solr-server trino iceberg delta jupyter-kernel-gateway  == *\ \s\p\a\r\k\-\h\i\s\t\o\r\y\-\s\e\r\v\e\r\ * ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + return 1
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + in_array spark-history-server COMPONENT_SERVICES
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r value=spark-history-server
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -n values=COMPONENT_SERVICES
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ !  hadoop-hdfs-namenode hadoop-hdfs-datanode hadoop-hdfs-zkfc hadoop-hdfs-secondarynamenode hadoop-hdfs-journalnode hive-metastore hive-server2 mysql-server spark-history-server  == *\ \s\p\a\r\k\-\h\i\s\t\o\r\y\-\s\e\r\v\e\r\ * ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + continue
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + for service in "${services[@]}"
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + in_array hadoop-yarn-resourcemanager DATAPROC_COMPONENTS
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r value=hadoop-yarn-resourcemanager
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -n values=DATAPROC_COMPONENTS
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ !  dpms-proxy hdfs hive-metastore hive-server2 kerberos knox mapreduce mysql npd proxy-agent spark tez yarn zookeeper-server jupyter fluentbit-ucp otel-ucp stackdriver-agent-container google-fluentd-container earlyoom rubix miniconda3 docker-ce hive-webhcat-server zeppelin flink hudi pig ranger solr-server trino iceberg delta jupyter-kernel-gateway  == *\ \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r\ * ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + return 1
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + in_array hadoop-yarn-resourcemanager COMPONENT_SERVICES
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r value=hadoop-yarn-resourcemanager
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -n values=COMPONENT_SERVICES
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ !  hadoop-hdfs-namenode hadoop-hdfs-datanode hadoop-hdfs-zkfc hadoop-hdfs-secondarynamenode hadoop-hdfs-journalnode hive-metastore hive-server2 mysql-server spark-history-server  == *\ \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r\ * ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + return 1
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + run_in_background --tag setup-hadoop-yarn-resourcemanager setup_service hadoop-yarn-resourcemanager
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r pid=5090
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ --tag == \-\-\t\a\g ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + shift 2
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5090.running ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'setup_service hadoop-yarn-resourcemanager'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'Started background process [setup_service hadoop-yarn-resourcemanager] as pid 5090'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: Started background process [setup_service hadoop-yarn-resourcemanager] as pid 5090
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + for service in "${services[@]}"
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + in_array hive-webhcat-server DATAPROC_COMPONENTS
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r value=hive-webhcat-server
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -n values=DATAPROC_COMPONENTS
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ !  dpms-proxy hdfs hive-metastore hive-server2 kerberos knox mapreduce mysql npd proxy-agent spark tez yarn zookeeper-server jupyter fluentbit-ucp otel-ucp stackdriver-agent-container google-fluentd-container earlyoom rubix miniconda3 docker-ce hive-webhcat-server zeppelin flink hudi pig ranger solr-server trino iceberg delta jupyter-kernel-gateway  == *\ \h\i\v\e\-\w\e\b\h\c\a\t\-\s\e\r\v\e\r\ * ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + continue
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + for service in "${services[@]}"
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + in_array solr-server DATAPROC_COMPONENTS
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + trap 'echo "$?" >"${COMMANDS_TMP_DIR}/${BASHPID}.exitcode"' EXIT
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r value=solr-server
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -n values=DATAPROC_COMPONENTS
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ !  dpms-proxy hdfs hive-metastore hive-server2 kerberos knox mapreduce mysql npd proxy-agent spark tez yarn zookeeper-server jupyter fluentbit-ucp otel-ucp stackdriver-agent-container google-fluentd-container earlyoom rubix miniconda3 docker-ce hive-webhcat-server zeppelin flink hudi pig ranger solr-server trino iceberg delta jupyter-kernel-gateway  == *\ \s\o\l\r\-\s\e\r\v\e\r\ * ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + continue
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + for service in "${services[@]}"
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + run_with_logger --tag setup-hadoop-yarn-resourcemanager setup_service hadoop-yarn-resourcemanager
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + in_array jupyter-kernel-gateway DATAPROC_COMPONENTS
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r value=jupyter-kernel-gateway
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -n values=DATAPROC_COMPONENTS
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local tag=dataproc-script
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ !  dpms-proxy hdfs hive-metastore hive-server2 kerberos knox mapreduce mysql npd proxy-agent spark tez yarn zookeeper-server jupyter fluentbit-ucp otel-ucp stackdriver-agent-container google-fluentd-container earlyoom rubix miniconda3 docker-ce hive-webhcat-server zeppelin flink hudi pig ranger solr-server trino iceberg delta jupyter-kernel-gateway  == *\ \j\u\p\y\t\e\r\-\k\e\r\n\e\l\-\g\a\t\e\w\a\y\ * ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local pid=5090
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + continue
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ --tag == \-\-\t\a\g ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + for service in "${services[@]}"
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + tag=dataproc-setup-hadoop-yarn-resourcemanager
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + in_array hadoop-mapreduce-historyserver DATAPROC_COMPONENTS
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + shift 2
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r value=hadoop-mapreduce-historyserver
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ 2 -eq 0 ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -n values=DATAPROC_COMPONENTS
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + setup_service hadoop-yarn-resourcemanager
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ !  dpms-proxy hdfs hive-metastore hive-server2 kerberos knox mapreduce mysql npd proxy-agent spark tez yarn zookeeper-server jupyter fluentbit-ucp otel-ucp stackdriver-agent-container google-fluentd-container earlyoom rubix miniconda3 docker-ce hive-webhcat-server zeppelin flink hudi pig ranger solr-server trino iceberg delta jupyter-kernel-gateway  == *\ \h\a\d\o\o\p\-\m\a\p\r\e\d\u\c\e\-\h\i\s\t\o\r\y\s\e\r\v\e\r\ * ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + return 1
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + in_array hadoop-mapreduce-historyserver COMPONENT_SERVICES
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r value=hadoop-mapreduce-historyserver
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -n values=COMPONENT_SERVICES
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ !  hadoop-hdfs-namenode hadoop-hdfs-datanode hadoop-hdfs-zkfc hadoop-hdfs-secondarynamenode hadoop-hdfs-journalnode hive-metastore hive-server2 mysql-server spark-history-server  == *\ \h\a\d\o\o\p\-\m\a\p\r\e\d\u\c\e\-\h\i\s\t\o\r\y\s\e\r\v\e\r\ * ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + return 1
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + run_in_background --tag setup-hadoop-mapreduce-historyserver setup_service hadoop-mapreduce-historyserver
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r pid=5092
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ --tag == \-\-\t\a\g ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + shift 2
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5092.running ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'setup_service hadoop-mapreduce-historyserver'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'Started background process [setup_service hadoop-mapreduce-historyserver] as pid 5092'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: Started background process [setup_service hadoop-mapreduce-historyserver] as pid 5092
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + for service in "${services[@]}"
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + in_array mysql-server DATAPROC_COMPONENTS
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r value=mysql-server
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -n values=DATAPROC_COMPONENTS
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ !  dpms-proxy hdfs hive-metastore hive-server2 kerberos knox mapreduce mysql npd proxy-agent spark tez yarn zookeeper-server jupyter fluentbit-ucp otel-ucp stackdriver-agent-container google-fluentd-container earlyoom rubix miniconda3 docker-ce hive-webhcat-server zeppelin flink hudi pig ranger solr-server trino iceberg delta jupyter-kernel-gateway  == *\ \m\y\s\q\l\-\s\e\r\v\e\r\ * ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + return 1
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + in_array mysql-server COMPONENT_SERVICES
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r value=mysql-server
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -n values=COMPONENT_SERVICES
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ !  hadoop-hdfs-namenode hadoop-hdfs-datanode hadoop-hdfs-zkfc hadoop-hdfs-secondarynamenode hadoop-hdfs-journalnode hive-metastore hive-server2 mysql-server spark-history-server  == *\ \m\y\s\q\l\-\s\e\r\v\e\r\ * ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + continue
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + for service in "${services[@]}"
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + in_array hadoop-yarn-timelineserver DATAPROC_COMPONENTS
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r value=hadoop-yarn-timelineserver
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -n values=DATAPROC_COMPONENTS
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ !  dpms-proxy hdfs hive-metastore hive-server2 kerberos knox mapreduce mysql npd proxy-agent spark tez yarn zookeeper-server jupyter fluentbit-ucp otel-ucp stackdriver-agent-container google-fluentd-container earlyoom rubix miniconda3 docker-ce hive-webhcat-server zeppelin flink hudi pig ranger solr-server trino iceberg delta jupyter-kernel-gateway  == *\ \h\a\d\o\o\p\-\y\a\r\n\-\t\i\m\e\l\i\n\e\s\e\r\v\e\r\ * ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + return 1
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + in_array hadoop-yarn-timelineserver COMPONENT_SERVICES
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r value=hadoop-yarn-timelineserver
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -n values=COMPONENT_SERVICES
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ !  hadoop-hdfs-namenode hadoop-hdfs-datanode hadoop-hdfs-zkfc hadoop-hdfs-secondarynamenode hadoop-hdfs-journalnode hive-metastore hive-server2 mysql-server spark-history-server  == *\ \h\a\d\o\o\p\-\y\a\r\n\-\t\i\m\e\l\i\n\e\s\e\r\v\e\r\ * ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + return 1
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + run_in_background --tag setup-hadoop-yarn-timelineserver setup_service hadoop-yarn-timelineserver
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r pid=5093
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ --tag == \-\-\t\a\g ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + shift 2
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5093.running ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'setup_service hadoop-yarn-timelineserver'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'Started background process [setup_service hadoop-yarn-timelineserver] as pid 5093'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: Started background process [setup_service hadoop-yarn-timelineserver] as pid 5093
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + for service in "${services[@]}"
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + in_array jupyter DATAPROC_COMPONENTS
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r value=jupyter
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -n values=DATAPROC_COMPONENTS
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ !  dpms-proxy hdfs hive-metastore hive-server2 kerberos knox mapreduce mysql npd proxy-agent spark tez yarn zookeeper-server jupyter fluentbit-ucp otel-ucp stackdriver-agent-container google-fluentd-container earlyoom rubix miniconda3 docker-ce hive-webhcat-server zeppelin flink hudi pig ranger solr-server trino iceberg delta jupyter-kernel-gateway  == *\ \j\u\p\y\t\e\r\ * ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + continue
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + for service in "${services[@]}"
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + in_array zeppelin DATAPROC_COMPONENTS
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r value=zeppelin
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -n values=DATAPROC_COMPONENTS
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ !  dpms-proxy hdfs hive-metastore hive-server2 kerberos knox mapreduce mysql npd proxy-agent spark tez yarn zookeeper-server jupyter fluentbit-ucp otel-ucp stackdriver-agent-container google-fluentd-container earlyoom rubix miniconda3 docker-ce hive-webhcat-server zeppelin flink hudi pig ranger solr-server trino iceberg delta jupyter-kernel-gateway  == *\ \z\e\p\p\e\l\i\n\ * ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + continue
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + for service in "${services[@]}"
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + in_array hadoop-hdfs-secondarynamenode DATAPROC_COMPONENTS
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r value=hadoop-hdfs-secondarynamenode
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -n values=DATAPROC_COMPONENTS
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ !  dpms-proxy hdfs hive-metastore hive-server2 kerberos knox mapreduce mysql npd proxy-agent spark tez yarn zookeeper-server jupyter fluentbit-ucp otel-ucp stackdriver-agent-container google-fluentd-container earlyoom rubix miniconda3 docker-ce hive-webhcat-server zeppelin flink hudi pig ranger solr-server trino iceberg delta jupyter-kernel-gateway  == *\ \h\a\d\o\o\p\-\h\d\f\s\-\s\e\c\o\n\d\a\r\y\n\a\m\e\n\o\d\e\ * ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + return 1
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + in_array hadoop-hdfs-secondarynamenode COMPONENT_SERVICES
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r value=hadoop-hdfs-secondarynamenode
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -n values=COMPONENT_SERVICES
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ !  hadoop-hdfs-namenode hadoop-hdfs-datanode hadoop-hdfs-zkfc hadoop-hdfs-secondarynamenode hadoop-hdfs-journalnode hive-metastore hive-server2 mysql-server spark-history-server  == *\ \h\a\d\o\o\p\-\h\d\f\s\-\s\e\c\o\n\d\a\r\y\n\a\m\e\n\o\d\e\ * ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + continue
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + loginfo 'Activating components'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'Activating components'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: Activating components
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + activate_components earlyoom fluentbit-ucp hdfs hive-metastore hive-server2 knox mapreduce miniconda3 mysql npd otel-ucp pig proxy-agent spark tez yarn
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + components=('earlyoom' 'fluentbit-ucp' 'hdfs' 'hive-metastore' 'hive-server2' 'knox' 'mapreduce' 'miniconda3' 'mysql' 'npd' 'otel-ucp' 'pig' 'proxy-agent' 'spark' 'tez' 'yarn')
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local components
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + mkdir -p /tmp/dataproc/components/activate
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + for component in "${components[@]}"
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ earlyoom != \k\e\r\b\e\r\o\s ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + loginfo 'Activating: earlyoom'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'Activating: earlyoom'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: Activating: earlyoom
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + run_in_background --tag activate-component-earlyoom activate_component earlyoom
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r pid=5096
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ --tag == \-\-\t\a\g ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + shift 2
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5096.running ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'activate_component earlyoom'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'Started background process [activate_component earlyoom] as pid 5096'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: Started background process [activate_component earlyoom] as pid 5096
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + for component in "${components[@]}"
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ fluentbit-ucp != \k\e\r\b\e\r\o\s ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + loginfo 'Activating: fluentbit-ucp'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'Activating: fluentbit-ucp'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: Activating: fluentbit-ucp
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + run_in_background --tag activate-component-fluentbit-ucp activate_component fluentbit-ucp
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r pid=5097
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ --tag == \-\-\t\a\g ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + shift 2
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5097.running ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'activate_component fluentbit-ucp'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'Started background process [activate_component fluentbit-ucp] as pid 5097'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: Started background process [activate_component fluentbit-ucp] as pid 5097
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + for component in "${components[@]}"
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ hdfs != \k\e\r\b\e\r\o\s ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + loginfo 'Activating: hdfs'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'Activating: hdfs'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: Activating: hdfs
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + run_in_background --tag activate-component-hdfs activate_component hdfs
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r pid=5098
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ --tag == \-\-\t\a\g ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + shift 2
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5098.running ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'activate_component hdfs'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'Started background process [activate_component hdfs] as pid 5098'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: Started background process [activate_component hdfs] as pid 5098
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + for component in "${components[@]}"
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ hive-metastore != \k\e\r\b\e\r\o\s ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + loginfo 'Activating: hive-metastore'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'Activating: hive-metastore'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: Activating: hive-metastore
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + run_in_background --tag activate-component-hive-metastore activate_component hive-metastore
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r pid=5099
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ --tag == \-\-\t\a\g ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + shift 2
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5099.running ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'activate_component hive-metastore'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'Started background process [activate_component hive-metastore] as pid 5099'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: Started background process [activate_component hive-metastore] as pid 5099
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + for component in "${components[@]}"
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ hive-server2 != \k\e\r\b\e\r\o\s ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + loginfo 'Activating: hive-server2'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'Activating: hive-server2'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: Activating: hive-server2
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + run_in_background --tag activate-component-hive-server2 activate_component hive-server2
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r pid=5100
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ --tag == \-\-\t\a\g ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + shift 2
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5100.running ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'activate_component hive-server2'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'Started background process [activate_component hive-server2] as pid 5100'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: Started background process [activate_component hive-server2] as pid 5100
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + for component in "${components[@]}"
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ knox != \k\e\r\b\e\r\o\s ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + loginfo 'Activating: knox'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'Activating: knox'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: Activating: knox
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + run_in_background --tag activate-component-knox activate_component knox
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r pid=5101
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ --tag == \-\-\t\a\g ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + shift 2
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5101.running ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'activate_component knox'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'Started background process [activate_component knox] as pid 5101'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: Started background process [activate_component knox] as pid 5101
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + for component in "${components[@]}"
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ mapreduce != \k\e\r\b\e\r\o\s ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + loginfo 'Activating: mapreduce'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'Activating: mapreduce'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: Activating: mapreduce
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + run_in_background --tag activate-component-mapreduce activate_component mapreduce
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r pid=5103
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + trap 'echo "$?" >"${COMMANDS_TMP_DIR}/${BASHPID}.exitcode"' EXIT
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ --tag == \-\-\t\a\g ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + shift 2
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5103.running ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'activate_component mapreduce'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + run_with_logger --tag activate-component-earlyoom activate_component earlyoom
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local tag=dataproc-script
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'Started background process [activate_component mapreduce] as pid 5103'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: Started background process [activate_component mapreduce] as pid 5103
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local pid=5096
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ --tag == \-\-\t\a\g ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + tag=dataproc-activate-component-earlyoom
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + shift 2
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + for component in "${components[@]}"
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ 2 -eq 0 ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ miniconda3 != \k\e\r\b\e\r\o\s ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + loginfo 'Activating: miniconda3'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + activate_component earlyoom
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'Activating: miniconda3'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: Activating: miniconda3
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + run_in_background --tag activate-component-miniconda3 activate_component miniconda3
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r pid=5105
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ --tag == \-\-\t\a\g ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + shift 2
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5105.running ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'activate_component miniconda3'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'Started background process [activate_component miniconda3] as pid 5105'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: Started background process [activate_component miniconda3] as pid 5105
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + for component in "${components[@]}"
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ mysql != \k\e\r\b\e\r\o\s ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + loginfo 'Activating: mysql'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'Activating: mysql'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: Activating: mysql
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + run_in_background --tag activate-component-mysql activate_component mysql
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + trap 'echo "$?" >"${COMMANDS_TMP_DIR}/${BASHPID}.exitcode"' EXIT
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r pid=5106
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + run_with_logger --tag activate-component-mapreduce activate_component mapreduce
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ --tag == \-\-\t\a\g ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + shift 2
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5106.running ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local tag=dataproc-script
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'activate_component mysql'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local pid=5103
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ --tag == \-\-\t\a\g ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + tag=dataproc-activate-component-mapreduce
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + shift 2
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ 2 -eq 0 ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'Started background process [activate_component mysql] as pid 5106'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + activate_component mapreduce
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: Started background process [activate_component mysql] as pid 5106
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + for component in "${components[@]}"
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ npd != \k\e\r\b\e\r\o\s ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + loginfo 'Activating: npd'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'Activating: npd'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: Activating: npd
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + run_in_background --tag activate-component-npd activate_component npd
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r pid=5107
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ --tag == \-\-\t\a\g ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + shift 2
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5107.running ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'activate_component npd'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'Started background process [activate_component npd] as pid 5107'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: Started background process [activate_component npd] as pid 5107
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + for component in "${components[@]}"
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ otel-ucp != \k\e\r\b\e\r\o\s ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + loginfo 'Activating: otel-ucp'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'Activating: otel-ucp'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: Activating: otel-ucp
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + run_in_background --tag activate-component-otel-ucp activate_component otel-ucp
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r pid=5108
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ --tag == \-\-\t\a\g ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + shift 2
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5108.running ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'activate_component otel-ucp'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'Started background process [activate_component otel-ucp] as pid 5108'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: Started background process [activate_component otel-ucp] as pid 5108
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + for component in "${components[@]}"
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ pig != \k\e\r\b\e\r\o\s ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + loginfo 'Activating: pig'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'Activating: pig'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: Activating: pig
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + run_in_background --tag activate-component-pig activate_component pig
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r pid=5109
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ --tag == \-\-\t\a\g ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + shift 2
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5109.running ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'activate_component pig'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'Started background process [activate_component pig] as pid 5109'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: Started background process [activate_component pig] as pid 5109
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + for component in "${components[@]}"
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ proxy-agent != \k\e\r\b\e\r\o\s ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + loginfo 'Activating: proxy-agent'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'Activating: proxy-agent'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: Activating: proxy-agent
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + run_in_background --tag activate-component-proxy-agent activate_component proxy-agent
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r pid=5110
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ --tag == \-\-\t\a\g ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + shift 2
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5110.running ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'activate_component proxy-agent'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'Started background process [activate_component proxy-agent] as pid 5110'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: Started background process [activate_component proxy-agent] as pid 5110
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + for component in "${components[@]}"
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ spark != \k\e\r\b\e\r\o\s ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + loginfo 'Activating: spark'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'Activating: spark'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: Activating: spark
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + run_in_background --tag activate-component-spark activate_component spark
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r pid=5111
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ --tag == \-\-\t\a\g ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + shift 2
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5111.running ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'activate_component spark'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'Started background process [activate_component spark] as pid 5111'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: Started background process [activate_component spark] as pid 5111
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + for component in "${components[@]}"
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ tez != \k\e\r\b\e\r\o\s ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + loginfo 'Activating: tez'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'Activating: tez'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: Activating: tez
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + run_in_background --tag activate-component-tez activate_component tez
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r pid=5113
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ --tag == \-\-\t\a\g ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + shift 2
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5113.running ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'activate_component tez'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'Started background process [activate_component tez] as pid 5113'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: Started background process [activate_component tez] as pid 5113
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + for component in "${components[@]}"
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ yarn != \k\e\r\b\e\r\o\s ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + loginfo 'Activating: yarn'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'Activating: yarn'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: Activating: yarn
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + run_in_background --tag activate-component-yarn activate_component yarn
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + trap 'echo "$?" >"${COMMANDS_TMP_DIR}/${BASHPID}.exitcode"' EXIT
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r pid=5114
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ --tag == \-\-\t\a\g ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + shift 2
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + trap 'echo "$?" >"${COMMANDS_TMP_DIR}/${BASHPID}.exitcode"' EXIT
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5114.running ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'activate_component yarn'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + run_with_logger --tag activate-component-miniconda3 activate_component miniconda3
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + echo 'Started background process [activate_component yarn] as pid 5114'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: Started background process [activate_component yarn] as pid 5114
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local tag=dataproc-script
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local pid=5105
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + run_with_logger --tag setup-hadoop-yarn-timelineserver setup_service hadoop-yarn-timelineserver
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ --tag == \-\-\t\a\g ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + is_hermetic_vm
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local tag=dataproc-script
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r sentinel_file=/etc/google-dataproc/hermetic_vm
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local pid=5093
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + tag=dataproc-activate-component-miniconda3
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ --tag == \-\-\t\a\g ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ -f /etc/google-dataproc/hermetic_vm ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local hermetic_vm
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + tag=dataproc-setup-hadoop-yarn-timelineserver
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + shift 2
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + shift 2
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ 2 -eq 0 ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ 2 -eq 0 ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + setup_service hadoop-yarn-timelineserver
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + activate_component miniconda3
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: ++ logger -s -t 'dataproc-activate-component-earlyoom[5096]'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + trap 'echo "$?" >"${COMMANDS_TMP_DIR}/${BASHPID}.exitcode"' EXIT
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + run_with_logger --tag setup-hadoop-mapreduce-historyserver setup_service hadoop-mapreduce-historyserver
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local tag=dataproc-script
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local pid=5092
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ --tag == \-\-\t\a\g ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + tag=dataproc-setup-hadoop-mapreduce-historyserver
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + shift 2
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ 2 -eq 0 ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + setup_service hadoop-mapreduce-historyserver
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + trap 'echo "$?" >"${COMMANDS_TMP_DIR}/${BASHPID}.exitcode"' EXIT
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + run_with_logger --tag activate-component-yarn activate_component yarn
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local tag=dataproc-script
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local pid=5114
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ --tag == \-\-\t\a\g ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + tag=dataproc-activate-component-yarn
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + shift 2
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ 2 -eq 0 ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + activate_component yarn
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + trap 'echo "$?" >"${COMMANDS_TMP_DIR}/${BASHPID}.exitcode"' EXIT
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + trap 'echo "$?" >"${COMMANDS_TMP_DIR}/${BASHPID}.exitcode"' EXIT
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + trap 'echo "$?" >"${COMMANDS_TMP_DIR}/${BASHPID}.exitcode"' EXIT
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + trap 'echo "$?" >"${COMMANDS_TMP_DIR}/${BASHPID}.exitcode"' EXIT
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + trap 'echo "$?" >"${COMMANDS_TMP_DIR}/${BASHPID}.exitcode"' EXIT
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + trap 'echo "$?" >"${COMMANDS_TMP_DIR}/${BASHPID}.exitcode"' EXIT
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + trap 'echo "$?" >"${COMMANDS_TMP_DIR}/${BASHPID}.exitcode"' EXIT
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: ++ logger -s -t 'dataproc-activate-component-mapreduce[5103]'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: ++ logger -s -t 'dataproc-activate-component-miniconda3[5105]'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + trap 'echo "$?" >"${COMMANDS_TMP_DIR}/${BASHPID}.exitcode"' EXIT
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + trap 'echo "$?" >"${COMMANDS_TMP_DIR}/${BASHPID}.exitcode"' EXIT
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + run_with_logger --tag activate-component-hive-server2 activate_component hive-server2
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + run_with_logger --tag activate-component-knox activate_component knox
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + run_with_logger --tag activate-component-pig activate_component pig
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + run_with_logger --tag activate-component-npd activate_component npd
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + run_with_logger --tag activate-component-tez activate_component tez
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + run_with_logger --tag activate-component-mysql activate_component mysql
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + run_with_logger --tag activate-component-otel-ucp activate_component otel-ucp
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + trap 'echo "$?" >"${COMMANDS_TMP_DIR}/${BASHPID}.exitcode"' EXIT
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local tag=dataproc-script
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + run_with_logger --tag activate-component-hive-metastore activate_component hive-metastore
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local tag=dataproc-script
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local tag=dataproc-script
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + run_with_logger --tag activate-component-spark activate_component spark
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local tag=dataproc-script
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local tag=dataproc-script
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local tag=dataproc-script
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local tag=dataproc-script
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + run_with_logger --tag activate-component-proxy-agent activate_component proxy-agent
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local tag=dataproc-script
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local pid=5109
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local pid=5106
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ --tag == \-\-\t\a\g ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + trap 'echo "$?" >"${COMMANDS_TMP_DIR}/${BASHPID}.exitcode"' EXIT
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local pid=5100
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local pid=5107
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local tag=dataproc-script
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local tag=dataproc-script
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local pid=5108
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local pid=5101
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local pid=5113
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ --tag == \-\-\t\a\g ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ --tag == \-\-\t\a\g ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ --tag == \-\-\t\a\g ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ --tag == \-\-\t\a\g ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local pid=5099
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + tag=dataproc-activate-component-pig
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + tag=dataproc-activate-component-mysql
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + shift 2
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ --tag == \-\-\t\a\g ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + tag=dataproc-activate-component-knox
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + tag=dataproc-activate-component-npd
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + shift 2
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ --tag == \-\-\t\a\g ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + tag=dataproc-activate-component-hive-server2
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + shift 2
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local pid=5110
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + tag=dataproc-activate-component-otel-ucp
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + shift 2
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + shift 2
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + shift 2
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ 2 -eq 0 ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local pid=5111
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ --tag == \-\-\t\a\g ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + tag=dataproc-activate-component-hive-metastore
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + shift 2
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + tag=dataproc-activate-component-tez
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ 2 -eq 0 ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ 2 -eq 0 ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ 2 -eq 0 ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ 2 -eq 0 ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ --tag == \-\-\t\a\g ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + tag=dataproc-activate-component-spark
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + shift 2
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ 2 -eq 0 ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ 2 -eq 0 ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ --tag == \-\-\t\a\g ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + activate_component knox
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + activate_component pig
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + activate_component npd
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ 2 -eq 0 ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + activate_component hive-metastore
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + tag=dataproc-activate-component-proxy-agent
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + activate_component mysql
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + activate_component tez
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + activate_component hive-server2
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + activate_component otel-ucp
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + shift 2
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + shift 2
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ 2 -eq 0 ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: ++ logger -s -t 'dataproc-activate-component-yarn[5114]'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ 2 -eq 0 ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + activate_component spark
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + activate_component proxy-agent
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: ++ /usr/share/google/get_metadata_value attributes/hermetic-vm
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + run_with_logger --tag activate-component-hdfs activate_component hdfs
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: ++ logger -s -t 'dataproc-setup-hadoop-yarn-timelineserver[5093]'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local tag=dataproc-script
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local pid=5098
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ --tag == \-\-\t\a\g ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + tag=dataproc-activate-component-hdfs
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + shift 2
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + trap 'echo "$?" >"${COMMANDS_TMP_DIR}/${BASHPID}.exitcode"' EXIT
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ 2 -eq 0 ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + activate_component hdfs
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + run_with_logger --tag activate-component-fluentbit-ucp activate_component fluentbit-ucp
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local tag=dataproc-script
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local pid=5097
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ --tag == \-\-\t\a\g ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + tag=dataproc-activate-component-fluentbit-ucp
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + shift 2
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ 2 -eq 0 ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + activate_component fluentbit-ucp
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-earlyoom[5096]: + local -r component=earlyoom
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-earlyoom[5096]: + local -r activate_script=/usr/local/share/google/dataproc/bdutil/components/activate/earlyoom.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-earlyoom[5096]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/activate/earlyoom.sh ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-earlyoom[5096]: + echo 'Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/earlyoom.sh'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-earlyoom[5096]: Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/earlyoom.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-earlyoom[5096]: + mkdir -p /tmp/dataproc/components/activate
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-earlyoom[5096]: + touch /tmp/dataproc/components/activate/earlyoom.running
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: ++ logger -s -t 'dataproc-setup-hadoop-mapreduce-historyserver[5092]'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-miniconda3[5105]: + local -r component=miniconda3
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-miniconda3[5105]: + local -r activate_script=/usr/local/share/google/dataproc/bdutil/components/activate/miniconda3.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-miniconda3[5105]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/activate/miniconda3.sh ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-miniconda3[5105]: + echo 'Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/miniconda3.sh'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-miniconda3[5105]: Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/miniconda3.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-miniconda3[5105]: + mkdir -p /tmp/dataproc/components/activate
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-miniconda3[5105]: + touch /tmp/dataproc/components/activate/miniconda3.running
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-miniconda3[5105]: + local exit_code=0
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-mapreduce[5103]: + local -r component=mapreduce
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-mapreduce[5103]: + local -r activate_script=/usr/local/share/google/dataproc/bdutil/components/activate/mapreduce.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-mapreduce[5103]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/activate/mapreduce.sh ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-mapreduce[5103]: + echo 'Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/mapreduce.sh'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-mapreduce[5103]: Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/mapreduce.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-mapreduce[5103]: + mkdir -p /tmp/dataproc/components/activate
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-mapreduce[5103]: + touch /tmp/dataproc/components/activate/mapreduce.running
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: ++ logger -s -t 'dataproc-activate-component-hive-metastore[5099]'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-miniconda3[5105]: ++ date +%s.%N
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: ++ logger -s -t 'dataproc-activate-component-pig[5109]'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-earlyoom[5096]: + local exit_code=0
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: ++ logger -s -t 'dataproc-activate-component-mysql[5106]'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: ++ logger -s -t 'dataproc-activate-component-knox[5101]'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: ++ logger -s -t 'dataproc-activate-component-hive-server2[5100]'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-mapreduce[5103]: + local exit_code=0
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: ++ logger -s -t 'dataproc-activate-component-proxy-agent[5110]'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: ++ logger -s -t 'dataproc-activate-component-npd[5107]'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: ++ logger -s -t 'dataproc-activate-component-otel-ucp[5108]'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: ++ logger -s -t 'dataproc-activate-component-hdfs[5098]'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: ++ logger -s -t 'dataproc-activate-component-spark[5111]'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: ++ logger -s -t 'dataproc-activate-component-tez[5113]'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-yarn[5114]: + local -r component=yarn
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-yarn[5114]: + local -r activate_script=/usr/local/share/google/dataproc/bdutil/components/activate/yarn.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-yarn[5114]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/activate/yarn.sh ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-yarn[5114]: + echo 'Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/yarn.sh'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-yarn[5114]: Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/yarn.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-yarn[5114]: + mkdir -p /tmp/dataproc/components/activate
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-yarn[5114]: + touch /tmp/dataproc/components/activate/yarn.running
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-yarn[5114]: + local exit_code=0
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: ++ logger -s -t 'dataproc-setup-hadoop-yarn-resourcemanager[5090]'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-mapreduce[5103]: ++ date +%s.%N
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-setup-hadoop-yarn-timelineserver[5093]: + local -r service=hadoop-yarn-timelineserver
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-setup-hadoop-yarn-timelineserver[5093]: + enable_service hadoop-yarn-timelineserver
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-setup-hadoop-yarn-timelineserver[5093]: + local -r service=hadoop-yarn-timelineserver
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-setup-hadoop-yarn-timelineserver[5093]: + local -r unit=hadoop-yarn-timelineserver.service
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-setup-hadoop-yarn-timelineserver[5093]: + retry_constant_short systemctl enable hadoop-yarn-timelineserver.service
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-setup-hadoop-yarn-timelineserver[5093]: + retry_constant_custom 30 1 systemctl enable hadoop-yarn-timelineserver.service
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-setup-hadoop-yarn-timelineserver[5093]: + local -r max_retry_time=30
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-setup-hadoop-yarn-timelineserver[5093]: + local -r retry_delay=1
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-setup-hadoop-yarn-timelineserver[5093]: + cmd=('systemctl' 'enable' 'hadoop-yarn-timelineserver.service')
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-setup-hadoop-yarn-timelineserver[5093]: + local -r cmd
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-setup-hadoop-yarn-timelineserver[5093]: + local -r max_retries=30
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-setup-hadoop-yarn-timelineserver[5093]: + local reenable_x=false
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-setup-hadoop-yarn-timelineserver[5093]: + [[ -o xtrace ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-setup-hadoop-yarn-timelineserver[5093]: + set +x
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-setup-hadoop-yarn-timelineserver[5093]: About to run 'systemctl enable hadoop-yarn-timelineserver.service' with retries...
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-setup-hadoop-yarn-timelineserver[5093]: hadoop-yarn-timelineserver.service is not a native service, redirecting to systemd-sysv-install.
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-setup-hadoop-yarn-timelineserver[5093]: Executing: /lib/systemd/systemd-sysv-install enable hadoop-yarn-timelineserver
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-miniconda3[5105]: + local -r start=1745777263.549120491
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-miniconda3[5105]: + bash -e /usr/local/share/google/dataproc/bdutil/components/activate/miniconda3.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: ++ logger -s -t 'dataproc-activate-component-fluentbit-ucp[5097]'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-setup-hadoop-mapreduce-historyserver[5092]: + local -r service=hadoop-mapreduce-historyserver
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-setup-hadoop-mapreduce-historyserver[5092]: + enable_service hadoop-mapreduce-historyserver
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-setup-hadoop-mapreduce-historyserver[5092]: + local -r service=hadoop-mapreduce-historyserver
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-setup-hadoop-mapreduce-historyserver[5092]: + local -r unit=hadoop-mapreduce-historyserver.service
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-setup-hadoop-mapreduce-historyserver[5092]: + retry_constant_short systemctl enable hadoop-mapreduce-historyserver.service
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-setup-hadoop-mapreduce-historyserver[5092]: + retry_constant_custom 30 1 systemctl enable hadoop-mapreduce-historyserver.service
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-setup-hadoop-mapreduce-historyserver[5092]: + local -r max_retry_time=30
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-setup-hadoop-mapreduce-historyserver[5092]: + local -r retry_delay=1
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-setup-hadoop-mapreduce-historyserver[5092]: + cmd=('systemctl' 'enable' 'hadoop-mapreduce-historyserver.service')
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-setup-hadoop-mapreduce-historyserver[5092]: + local -r cmd
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-setup-hadoop-mapreduce-historyserver[5092]: + local -r max_retries=30
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-setup-hadoop-mapreduce-historyserver[5092]: + local reenable_x=false
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-setup-hadoop-mapreduce-historyserver[5092]: + [[ -o xtrace ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-setup-hadoop-mapreduce-historyserver[5092]: + set +x
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-setup-hadoop-mapreduce-historyserver[5092]: About to run 'systemctl enable hadoop-mapreduce-historyserver.service' with retries...
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-hive-metastore[5099]: + local -r component=hive-metastore
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-hive-metastore[5099]: + local -r activate_script=/usr/local/share/google/dataproc/bdutil/components/activate/hive-metastore.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-hive-metastore[5099]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/activate/hive-metastore.sh ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-hive-metastore[5099]: + echo 'Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/hive-metastore.sh'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-hive-metastore[5099]: Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/hive-metastore.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-hive-metastore[5099]: + mkdir -p /tmp/dataproc/components/activate
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-yarn[5114]: ++ date +%s.%N
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-hive-metastore[5099]: + touch /tmp/dataproc/components/activate/hive-metastore.running
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-hive-metastore[5099]: + local exit_code=0
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-proxy-agent[5110]: + local -r component=proxy-agent
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-proxy-agent[5110]: + local -r activate_script=/usr/local/share/google/dataproc/bdutil/components/activate/proxy-agent.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-proxy-agent[5110]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/activate/proxy-agent.sh ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-proxy-agent[5110]: + echo 'Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/proxy-agent.sh'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-proxy-agent[5110]: Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/proxy-agent.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-proxy-agent[5110]: + mkdir -p /tmp/dataproc/components/activate
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-proxy-agent[5110]: + touch /tmp/dataproc/components/activate/proxy-agent.running
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-mapreduce[5103]: + local -r start=1745777263.557395772
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-mapreduce[5103]: + bash -e /usr/local/share/google/dataproc/bdutil/components/activate/mapreduce.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-earlyoom[5096]: ++ date +%s.%N
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-hdfs[5098]: + local -r component=hdfs
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-hdfs[5098]: + local -r activate_script=/usr/local/share/google/dataproc/bdutil/components/activate/hdfs.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-hdfs[5098]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/activate/hdfs.sh ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-hdfs[5098]: + echo 'Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/hdfs.sh'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-hdfs[5098]: Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/hdfs.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-hdfs[5098]: + mkdir -p /tmp/dataproc/components/activate
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-hdfs[5098]: + touch /tmp/dataproc/components/activate/hdfs.running
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-hdfs[5098]: + local exit_code=0
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-pig[5109]: + local -r component=pig
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-pig[5109]: + local -r activate_script=/usr/local/share/google/dataproc/bdutil/components/activate/pig.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-pig[5109]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/activate/pig.sh ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-pig[5109]: + echo 'Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/pig.sh'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-pig[5109]: Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/pig.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-pig[5109]: + mkdir -p /tmp/dataproc/components/activate
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-pig[5109]: + touch /tmp/dataproc/components/activate/pig.running
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-hive-metastore[5099]: ++ date +%s.%N
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-yarn[5114]: + local -r start=1745777263.565154140
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-yarn[5114]: + bash -e /usr/local/share/google/dataproc/bdutil/components/activate/yarn.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-setup-hadoop-mapreduce-historyserver[5092]: hadoop-mapreduce-historyserver.service is not a native service, redirecting to systemd-sysv-install.
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-setup-hadoop-mapreduce-historyserver[5092]: Executing: /lib/systemd/systemd-sysv-install enable hadoop-mapreduce-historyserver
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-earlyoom[5096]: + local -r start=1745777263.567590398
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-earlyoom[5096]: + bash -e /usr/local/share/google/dataproc/bdutil/components/activate/earlyoom.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-hdfs[5098]: ++ date +%s.%N
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-hive-metastore[5099]: + local -r start=1745777263.572120450
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-hive-metastore[5099]: + bash -e /usr/local/share/google/dataproc/bdutil/components/activate/hive-metastore.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-spark[5111]: + local -r component=spark
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-spark[5111]: + local -r activate_script=/usr/local/share/google/dataproc/bdutil/components/activate/spark.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-spark[5111]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/activate/spark.sh ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-spark[5111]: + echo 'Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/spark.sh'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-spark[5111]: Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/spark.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-spark[5111]: + mkdir -p /tmp/dataproc/components/activate
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-spark[5111]: + touch /tmp/dataproc/components/activate/spark.running
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-spark[5111]: + local exit_code=0
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-hdfs[5098]: + local -r start=1745777263.576781605
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-hdfs[5098]: + bash -e /usr/local/share/google/dataproc/bdutil/components/activate/hdfs.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-knox[5101]: + local -r component=knox
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-knox[5101]: + local -r activate_script=/usr/local/share/google/dataproc/bdutil/components/activate/knox.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-knox[5101]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/activate/knox.sh ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-knox[5101]: + echo 'Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/knox.sh'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-knox[5101]: Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/knox.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-knox[5101]: + mkdir -p /tmp/dataproc/components/activate
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-knox[5101]: + touch /tmp/dataproc/components/activate/knox.running
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-npd[5107]: + local -r component=npd
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-npd[5107]: + local -r activate_script=/usr/local/share/google/dataproc/bdutil/components/activate/npd.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-npd[5107]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/activate/npd.sh ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-npd[5107]: + echo 'Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/npd.sh'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-npd[5107]: Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/npd.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-npd[5107]: + mkdir -p /tmp/dataproc/components/activate
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-npd[5107]: + touch /tmp/dataproc/components/activate/npd.running
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-mysql[5106]: + local -r component=mysql
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-mysql[5106]: + local -r activate_script=/usr/local/share/google/dataproc/bdutil/components/activate/mysql.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-mysql[5106]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/activate/mysql.sh ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-mysql[5106]: + echo 'Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/mysql.sh'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-mysql[5106]: Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/mysql.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-mysql[5106]: + mkdir -p /tmp/dataproc/components/activate
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-mysql[5106]: + touch /tmp/dataproc/components/activate/mysql.running
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-mysql[5106]: + local exit_code=0
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-fluentbit-ucp[5097]: + local -r component=fluentbit-ucp
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-fluentbit-ucp[5097]: + local -r activate_script=/usr/local/share/google/dataproc/bdutil/components/activate/fluentbit-ucp.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-fluentbit-ucp[5097]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/activate/fluentbit-ucp.sh ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-fluentbit-ucp[5097]: + echo 'Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/fluentbit-ucp.sh'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-fluentbit-ucp[5097]: Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/fluentbit-ucp.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-fluentbit-ucp[5097]: + mkdir -p /tmp/dataproc/components/activate
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-fluentbit-ucp[5097]: + touch /tmp/dataproc/components/activate/fluentbit-ucp.running
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-otel-ucp[5108]: + local -r component=otel-ucp
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-otel-ucp[5108]: + local -r activate_script=/usr/local/share/google/dataproc/bdutil/components/activate/otel-ucp.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-otel-ucp[5108]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/activate/otel-ucp.sh ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-otel-ucp[5108]: + echo 'Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/otel-ucp.sh'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-otel-ucp[5108]: Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/otel-ucp.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-hive-server2[5100]: + local -r component=hive-server2
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-hive-server2[5100]: + local -r activate_script=/usr/local/share/google/dataproc/bdutil/components/activate/hive-server2.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-hive-server2[5100]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/activate/hive-server2.sh ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-hive-server2[5100]: + echo 'Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/hive-server2.sh'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-hive-server2[5100]: Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/hive-server2.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-hive-server2[5100]: + mkdir -p /tmp/dataproc/components/activate
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-hive-server2[5100]: + touch /tmp/dataproc/components/activate/hive-server2.running
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-otel-ucp[5108]: + mkdir -p /tmp/dataproc/components/activate
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-otel-ucp[5108]: + touch /tmp/dataproc/components/activate/otel-ucp.running
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-otel-ucp[5108]: + local exit_code=0
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-pig[5109]: + local exit_code=0
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-npd[5107]: + local exit_code=0
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-knox[5101]: + local exit_code=0
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-tez[5113]: + local -r component=tez
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-tez[5113]: + local -r activate_script=/usr/local/share/google/dataproc/bdutil/components/activate/tez.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-tez[5113]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/activate/tez.sh ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-tez[5113]: + echo 'Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/tez.sh'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-tez[5113]: Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/tez.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-tez[5113]: + mkdir -p /tmp/dataproc/components/activate
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-tez[5113]: + touch /tmp/dataproc/components/activate/tez.running
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-tez[5113]: + local exit_code=0
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-hive-server2[5100]: + local exit_code=0
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-proxy-agent[5110]: + local exit_code=0
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-setup-hadoop-yarn-resourcemanager[5090]: + local -r service=hadoop-yarn-resourcemanager
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-setup-hadoop-yarn-resourcemanager[5090]: + enable_service hadoop-yarn-resourcemanager
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-setup-hadoop-yarn-resourcemanager[5090]: + local -r service=hadoop-yarn-resourcemanager
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-setup-hadoop-yarn-resourcemanager[5090]: + local -r unit=hadoop-yarn-resourcemanager.service
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-setup-hadoop-yarn-resourcemanager[5090]: + retry_constant_short systemctl enable hadoop-yarn-resourcemanager.service
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-setup-hadoop-yarn-resourcemanager[5090]: + retry_constant_custom 30 1 systemctl enable hadoop-yarn-resourcemanager.service
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-setup-hadoop-yarn-resourcemanager[5090]: + local -r max_retry_time=30
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-setup-hadoop-yarn-resourcemanager[5090]: + local -r retry_delay=1
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-setup-hadoop-yarn-resourcemanager[5090]: + cmd=('systemctl' 'enable' 'hadoop-yarn-resourcemanager.service')
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-setup-hadoop-yarn-resourcemanager[5090]: + local -r cmd
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-setup-hadoop-yarn-resourcemanager[5090]: + local -r max_retries=30
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-setup-hadoop-yarn-resourcemanager[5090]: + local reenable_x=false
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-setup-hadoop-yarn-resourcemanager[5090]: + [[ -o xtrace ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-setup-hadoop-yarn-resourcemanager[5090]: + set +x
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-setup-hadoop-yarn-resourcemanager[5090]: About to run 'systemctl enable hadoop-yarn-resourcemanager.service' with retries...
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-setup-hadoop-yarn-resourcemanager[5090]: hadoop-yarn-resourcemanager.service is not a native service, redirecting to systemd-sysv-install.
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-setup-hadoop-yarn-resourcemanager[5090]: Executing: /lib/systemd/systemd-sysv-install enable hadoop-yarn-resourcemanager
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-fluentbit-ucp[5097]: + local exit_code=0
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-pig[5109]: ++ date +%s.%N
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-spark[5111]: ++ date +%s.%N
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-otel-ucp[5108]: ++ date +%s.%N
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-mysql[5106]: ++ date +%s.%N
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-tez[5113]: ++ date +%s.%N
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-npd[5107]: ++ date +%s.%N
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-knox[5101]: ++ date +%s.%N
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-proxy-agent[5110]: ++ date +%s.%N
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-hive-server2[5100]: ++ date +%s.%N
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-fluentbit-ucp[5097]: ++ date +%s.%N
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-pig[5109]: + local -r start=1745777263.597338809
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-pig[5109]: + bash -e /usr/local/share/google/dataproc/bdutil/components/activate/pig.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-mysql[5106]: + local -r start=1745777263.600852490
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-mysql[5106]: + bash -e /usr/local/share/google/dataproc/bdutil/components/activate/mysql.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-knox[5101]: + local -r start=1745777263.602198886
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-knox[5101]: + bash -e /usr/local/share/google/dataproc/bdutil/components/activate/knox.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-hive-server2[5100]: + local -r start=1745777263.603482769
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-hive-server2[5100]: + bash -e /usr/local/share/google/dataproc/bdutil/components/activate/hive-server2.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-spark[5111]: + local -r start=1745777263.598655590
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-spark[5111]: + bash -e /usr/local/share/google/dataproc/bdutil/components/activate/spark.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-otel-ucp[5108]: + local -r start=1745777263.606044481
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-otel-ucp[5108]: + bash -e /usr/local/share/google/dataproc/bdutil/components/activate/otel-ucp.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-proxy-agent[5110]: + local -r start=1745777263.607667674
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-proxy-agent[5110]: + bash -e /usr/local/share/google/dataproc/bdutil/components/activate/proxy-agent.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-tez[5113]: + local -r start=1745777263.600018126
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-tez[5113]: + bash -e /usr/local/share/google/dataproc/bdutil/components/activate/tez.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-npd[5107]: + local -r start=1745777263.609779676
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-npd[5107]: + bash -e /usr/local/share/google/dataproc/bdutil/components/activate/npd.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-fluentbit-ucp[5097]: + local -r start=1745777263.613332263
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-fluentbit-ucp[5097]: + bash -e /usr/local/share/google/dataproc/bdutil/components/activate/fluentbit-ucp.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-pig[5109]: ++ date +%s.%N
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-mysql[5106]: ++ date +%s.%N
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-tez[5113]: ++ date +%s.%N
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-pig[5109]: + local -r end=1745777263.649720185
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-pig[5109]: + local -r runtime_s=0
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-pig[5109]: + echo 'Component pig took 0s to activate'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-pig[5109]: Component pig took 0s to activate
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-pig[5109]: + local -r time_file=/tmp/dataproc/components/activate/pig.time
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-pig[5109]: + touch /tmp/dataproc/components/activate/pig.time
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-mysql[5106]: + local -r end=1745777263.656474319
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-mysql[5106]: + local -r runtime_s=0
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-mysql[5106]: + echo 'Component mysql took 0s to activate'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-mysql[5106]: Component mysql took 0s to activate
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-mysql[5106]: + local -r time_file=/tmp/dataproc/components/activate/mysql.time
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-mysql[5106]: + touch /tmp/dataproc/components/activate/mysql.time
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-earlyoom[5096]: + main
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-earlyoom[5096]: + local -r earlyoom_config_file=/etc/default/earlyoom
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-earlyoom[5096]: + is_earlyoom_enabled
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-earlyoom[5096]: + [[ Master == \M\a\s\t\e\r ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-earlyoom[5096]: + get_dataproc_property_or_default internal.node.main.memory-protection.enabled false
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-earlyoom[5096]: + set +x
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-tez[5113]: + local -r end=1745777263.664526353
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-tez[5113]: + local -r runtime_s=0
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-tez[5113]: + echo 'Component tez took 0s to activate'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-tez[5113]: Component tez took 0s to activate
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-tez[5113]: + local -r time_file=/tmp/dataproc/components/activate/tez.time
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-tez[5113]: + touch /tmp/dataproc/components/activate/tez.time
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-pig[5109]: + cat
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-mapreduce[5103]: + [[ Master == \M\a\s\t\e\r ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-mapreduce[5103]: + is_component_selected hdfs
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-mapreduce[5103]: + local -r component=hdfs
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-mapreduce[5103]: + local activated_components
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-mysql[5106]: + cat
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-tez[5113]: + cat
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-mysql[5106]: + [[ 0 -ne 0 ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-mysql[5106]: + echo 'Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/mysql.sh'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-mysql[5106]: Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/mysql.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-mysql[5106]: + touch /tmp/dataproc/components/activate/mysql.done
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-pig[5109]: + [[ 0 -ne 0 ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-pig[5109]: + echo 'Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/pig.sh'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-pig[5109]: Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/pig.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-pig[5109]: + touch /tmp/dataproc/components/activate/pig.done
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: ++ echo 0
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-mapreduce[5103]: ++ get_components_to_activate
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-miniconda3[5105]: + install_effective_python_profile
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-miniconda3[5105]: + [[ ! -f /etc/profile.d/effective-python.sh ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-miniconda3[5105]: + generate_effective_python_profile
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-miniconda3[5105]: + [[ /opt/conda/miniconda3 != \/\o\p\t\/\c\o\n\d\a\/\d\e\f\a\u\l\t ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-miniconda3[5105]: + ln -f -s /opt/conda/miniconda3 /opt/conda/default
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-tez[5113]: + [[ 0 -ne 0 ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-tez[5113]: + echo 'Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/tez.sh'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-tez[5113]: Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/tez.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-tez[5113]: + touch /tmp/dataproc/components/activate/tez.done
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: ++ echo 0
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: ++ echo 0
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-mapreduce[5103]: ++ get_dataproc_property dataproc.components.activate
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-mapreduce[5103]: ++ tr '[:upper:]' '[:lower:]'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-mapreduce[5103]: ++ set +x
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-miniconda3[5105]: + emit_conda_profile /opt/conda/default
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-miniconda3[5105]: + local -r conda_dir=/opt/conda/default
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-miniconda3[5105]: + local -r python_bin=/opt/conda/default/bin
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-miniconda3[5105]: + local temp
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-miniconda3[5105]: ++ mktemp
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-miniconda3[5105]: + temp=/tmp/tmp.t38mPBCaAx
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-miniconda3[5105]: + ln -f -s /opt/conda/default/etc/profile.d/conda.sh /etc/profile.d/conda.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-npd[5107]: + is_default_system_metrics_enabled
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-earlyoom[5096]: true
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-earlyoom[5096]: + configure_earlyoom
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-earlyoom[5096]: + local -r earlyoom_log_file=/var/log/earlyoom.log
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-earlyoom[5096]: + local -r 'prefer_regex=^.*java[[:space:]].*application[_0-9]+[^[:space:]]+container[_0-9]+.*'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-earlyoom[5096]: + local earlyoom_threshold
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-earlyoom[5096]: + [[ Master == \M\a\s\t\e\r ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-earlyoom[5096]: ++ get_dataproc_property_or_default internal.node.main.memory-protection.threshold.kib 65536
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-earlyoom[5096]: ++ set +x
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-npd[5107]: ++ get_dataproc_property dataproc.monitoring.default.metrics.system.enable
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-npd[5107]: ++ set +x
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-knox[5101]: + readonly CLUSTER_PROPS=/tmp/cluster/properties
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-knox[5101]: + CLUSTER_PROPS=/tmp/cluster/properties
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-knox[5101]: + readonly KNOX_INSTALL_DIRECTORY=/usr/lib/knox
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-knox[5101]: + KNOX_INSTALL_DIRECTORY=/usr/lib/knox
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-mapreduce[5103]: + activated_components='knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-mapreduce[5103]: + [[ knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3 == *hdfs* ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-mapreduce[5103]: + echo 'Delaying starting MapReduce history server until HDFS is ready'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-mapreduce[5103]: Delaying starting MapReduce history server until HDFS is ready
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-miniconda3[5105]: + chmod +x /opt/conda/default/etc/profile.d/conda.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-knox[5101]: ++ hostname -f
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-mapreduce[5103]: ++ date +%s.%N
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-knox[5101]: + FQDN=cluster-dip-01-m.c.euphoric-coral-451717-v8.internal
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-knox[5101]: + readonly FQDN
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-knox[5101]: + readonly GATEWAY_CONF=/usr/lib/knox/conf/gateway-site.xml
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-knox[5101]: + GATEWAY_CONF=/usr/lib/knox/conf/gateway-site.xml
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-knox[5101]: + readonly KNOX_SERVICE_INIT_SCRIPT=/usr/lib/systemd/system/knox.service
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-knox[5101]: + KNOX_SERVICE_INIT_SCRIPT=/usr/lib/systemd/system/knox.service
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-miniconda3[5105]: + cat
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-knox[5101]: ++ get_dataproc_property_or_default dataproc:componentgateway.ha.enabled false
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-knox[5101]: ++ set +x
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-mapreduce[5103]: + local -r end=1745777263.833237886
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-mapreduce[5103]: + local -r runtime_s=0
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-mapreduce[5103]: + echo 'Component mapreduce took 0s to activate'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-mapreduce[5103]: Component mapreduce took 0s to activate
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-mapreduce[5103]: + local -r time_file=/tmp/dataproc/components/activate/mapreduce.time
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-mapreduce[5103]: + touch /tmp/dataproc/components/activate/mapreduce.time
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-miniconda3[5105]: + mv -n -v /tmp/tmp.t38mPBCaAx /etc/profile.d/effective-python.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-hive-server2[5100]: ++++ get_metadata_master
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-hive-server2[5100]: ++++ get_dataproc_metadata DATAPROC_METADATA_MASTER attributes/dataproc-master
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-hive-server2[5100]: ++++ set +x
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-npd[5107]: + local -r default_metrics_system_enabled=true
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-npd[5107]: + [[ true == \t\r\u\e ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-mapreduce[5103]: + cat
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-npd[5107]: ++ get_dataproc_property dataproc.monitoring.job.yarn.metrics.enable
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-npd[5107]: ++ set +x
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-mapreduce[5103]: + [[ 0 -ne 0 ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-mapreduce[5103]: + echo 'Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/mapreduce.sh'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-mapreduce[5103]: Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/mapreduce.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-mapreduce[5103]: + touch /tmp/dataproc/components/activate/mapreduce.done
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-earlyoom[5096]: + earlyoom_threshold=65536
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-earlyoom[5096]: + cat
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-earlyoom[5096]: + touch /var/log/earlyoom.log
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-miniconda3[5105]: renamed '/tmp/tmp.t38mPBCaAx' -> '/etc/profile.d/effective-python.sh'
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-miniconda3[5105]: + chmod a+r /etc/profile.d/effective-python.sh
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: ++ echo 0
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: ++ echo false
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + hermetic_vm=false
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + [[ false == \t\r\u\e ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + return 1
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + is_service_installed google-osconfig-agent
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local -r service=google-osconfig-agent
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local output
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + local status
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + (( i = 0 ))
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: + (( i < 10 ))
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-earlyoom[5096]: + chmod a+rw /var/log/earlyoom.log
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-miniconda3[5105]: + rm -Rf /tmp/tmp.t38mPBCaAx
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: ++ systemctl cat google-osconfig-agent.service
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-miniconda3[5105]: + customize_conda_env /opt/conda/miniconda3 /opt/conda/miniconda3/bin
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-miniconda3[5105]: + local -r conda_install_path=/opt/conda/miniconda3
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-miniconda3[5105]: + local -r conda_bin_dir=/opt/conda/miniconda3/bin
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-earlyoom[5096]: + enable_and_start_service earlyoom
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-earlyoom[5096]: + local -r service=earlyoom
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-earlyoom[5096]: + enable_service earlyoom
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-earlyoom[5096]: + local -r service=earlyoom
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-earlyoom[5096]: + local -r unit=earlyoom.service
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-earlyoom[5096]: + retry_constant_short systemctl enable earlyoom.service
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-earlyoom[5096]: + retry_constant_custom 30 1 systemctl enable earlyoom.service
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-earlyoom[5096]: + local -r max_retry_time=30
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-earlyoom[5096]: + local -r retry_delay=1
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-earlyoom[5096]: + cmd=('systemctl' 'enable' 'earlyoom.service')
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-earlyoom[5096]: + local -r cmd
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-earlyoom[5096]: + local -r max_retries=30
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-earlyoom[5096]: + local reenable_x=false
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-earlyoom[5096]: + [[ -o xtrace ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-earlyoom[5096]: + set +x
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-earlyoom[5096]: About to run 'systemctl enable earlyoom.service' with retries...
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-knox[5101]: + readonly IS_COMPONENT_GATEWAY_HA_ENABLED=false
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-knox[5101]: + IS_COMPONENT_GATEWAY_HA_ENABLED=false
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-knox[5101]: + TOPOLOGY_TEMPLATE=knox/topology-template.xml
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-knox[5101]: + (( 1 > 1 ))
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-knox[5101]: + generate_config
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-knox[5101]: + mv knox/gateway-site-template.xml /usr/lib/knox/conf/gateway-site.xml
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-miniconda3[5105]: ++ get_dataproc_property conda.env.config.uri
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-miniconda3[5105]: ++ set +x
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-npd[5107]: ++ get_dataproc_property dataproc.metrics.node.yarn.nodemanager.health.enable
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-npd[5107]: ++ set +x
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-knox[5101]: + merge_xml_properties /tmp/cluster/properties/knox.xml /usr/lib/knox/conf/gateway-site.xml
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-knox[5101]: + local src=/tmp/cluster/properties/knox.xml
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-knox[5101]: + local dest=/usr/lib/knox/conf/gateway-site.xml
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-knox[5101]: + [[ ! -f /tmp/cluster/properties/knox.xml ]]
<13>Apr 27 18:07:43 dataproc-startup-script[1265]: <13>Apr 27 18:07:43 dataproc-activate-component-knox[5101]: + bdconfig merge_configurations --configuration_file /usr/lib/knox/conf/gateway-site.xml --source_configuration_file /tmp/cluster/properties/knox.xml --resolve_environment_variables --create_if_absent --clobber
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-miniconda3[5105]: + local -r conda_env_config_uri=
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-npd[5107]: + configure_npd true true
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-npd[5107]: + local -r yarn_job_metrics_enabled=true
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-npd[5107]: + local -r yarn_nm_metrics_enabled=true
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-npd[5107]: + local -r init_script=/etc/systemd/system/npd.service
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-npd[5107]: + local -r config_dir=/usr/local/share/google/dataproc/npd-config
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-npd[5107]: + npd_configs=()
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-npd[5107]: + local npd_configs
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-npd[5107]: + npd_configs+=("--stackdriver-config=${config_dir}/exporter/stackdriver-exporter.json")
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-npd[5107]: + npd_configs+=("--config.system-stats-monitor=${config_dir}/system-stats-monitor.json,${config_dir}/net-cgroup-system-stats-monitor.json")
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-npd[5107]: + [[ true == \t\r\u\e ]]
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-npd[5107]: + local yarn_job_monitor_config_file=yarn-rm-monitor
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-npd[5107]: + is_component_selected kerberos
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-npd[5107]: + local -r component=kerberos
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-npd[5107]: + local activated_components
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-otel-ucp[5108]: ++ get_dataproc_property dataproc.observability.containerised.legacy.agents.enable
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-otel-ucp[5108]: ++ set +x
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-miniconda3[5105]: ++ get_dataproc_property conda.packages
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-miniconda3[5105]: ++ set +x
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-npd[5107]: ++ get_components_to_activate
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-npd[5107]: ++ get_dataproc_property dataproc.components.activate
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-npd[5107]: ++ set +x
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-npd[5107]: ++ tr '[:upper:]' '[:lower:]'
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-miniconda3[5105]: + local -r conda_packages=
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-miniconda3[5105]: ++ get_dataproc_property pip.packages
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-miniconda3[5105]: ++ set +x
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-server2[5100]: +++ DATAPROC_MASTER=cluster-dip-01-m
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-server2[5100]: ++++ get_metadata_master_additional
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-server2[5100]: ++++ get_dataproc_metadata DATAPROC_METADATA_MASTER_ADDITIONAL attributes/dataproc-master-additional
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-server2[5100]: ++++ set +x
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-otel-ucp[5108]: + CONTAINERISED_AGENTS_ENABLED=
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-otel-ucp[5108]: + [[ '' == \t\r\u\e ]]
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-otel-ucp[5108]: + enable_and_start_service otel-metrics-agent
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-otel-ucp[5108]: + local -r service=otel-metrics-agent
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-otel-ucp[5108]: + enable_service otel-metrics-agent
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-otel-ucp[5108]: + local -r service=otel-metrics-agent
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-otel-ucp[5108]: + local -r unit=otel-metrics-agent.service
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-otel-ucp[5108]: + retry_constant_short systemctl enable otel-metrics-agent.service
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-otel-ucp[5108]: + retry_constant_custom 30 1 systemctl enable otel-metrics-agent.service
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-otel-ucp[5108]: + local -r max_retry_time=30
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-otel-ucp[5108]: + local -r retry_delay=1
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-otel-ucp[5108]: + cmd=('systemctl' 'enable' 'otel-metrics-agent.service')
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-otel-ucp[5108]: + local -r cmd
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-otel-ucp[5108]: + local -r max_retries=30
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-otel-ucp[5108]: + local reenable_x=false
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-otel-ucp[5108]: + [[ -o xtrace ]]
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-otel-ucp[5108]: + set +x
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-otel-ucp[5108]: About to run 'systemctl enable otel-metrics-agent.service' with retries...
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: ++++ get_metadata_master
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: ++++ get_dataproc_metadata DATAPROC_METADATA_MASTER attributes/dataproc-master
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: ++++ set +x
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-npd[5107]: + activated_components='knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3'
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-npd[5107]: + [[ knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3 == *kerberos* ]]
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-npd[5107]: + npd_configs+=("--config.yarn-monitor=${config_dir}/${yarn_job_monitor_config_file}.json")
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-npd[5107]: + [[ true == \t\r\u\e ]]
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-npd[5107]: + local yarn_nm_monitor_config_file=yarn-nm-monitor
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-npd[5107]: + local nm_base_url=http://localhost:8042
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-npd[5107]: + is_component_selected kerberos
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-npd[5107]: + local -r component=kerberos
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-npd[5107]: + local activated_components
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-npd[5107]: ++ get_components_to_activate
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-npd[5107]: ++ get_dataproc_property dataproc.components.activate
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-npd[5107]: ++ set +x
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-npd[5107]: ++ tr '[:upper:]' '[:lower:]'
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-miniconda3[5105]: + local -r pip_packages=
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-miniconda3[5105]: + [[ -n '' ]]
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-miniconda3[5105]: + retry_constant_custom 4 1 install_conda_packages /opt/conda/miniconda3/bin ''
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-miniconda3[5105]: + local -r max_retry_time=4
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-miniconda3[5105]: + local -r retry_delay=1
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-miniconda3[5105]: + cmd=('install_conda_packages' '/opt/conda/miniconda3/bin' '')
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-miniconda3[5105]: + local -r cmd
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-miniconda3[5105]: + local -r max_retries=4
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-miniconda3[5105]: + local reenable_x=false
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-miniconda3[5105]: + [[ -o xtrace ]]
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-miniconda3[5105]: + set +x
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-miniconda3[5105]: About to run 'install_conda_packages /opt/conda/miniconda3/bin ' with retries...
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-miniconda3[5105]: 'install_conda_packages /opt/conda/miniconda3/bin ' succeeded after 1 execution(s).
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-miniconda3[5105]: + return 0
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-miniconda3[5105]: + retry_constant_custom 4 1 install_pip_packages ''
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-miniconda3[5105]: + local -r max_retry_time=4
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-miniconda3[5105]: + local -r retry_delay=1
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-miniconda3[5105]: + cmd=('install_pip_packages' '')
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-miniconda3[5105]: + local -r cmd
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-miniconda3[5105]: + local -r max_retries=4
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-miniconda3[5105]: + local reenable_x=false
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-miniconda3[5105]: + [[ -o xtrace ]]
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-miniconda3[5105]: + set +x
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-miniconda3[5105]: About to run 'install_pip_packages ' with retries...
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-miniconda3[5105]: 'install_pip_packages ' succeeded after 1 execution(s).
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-miniconda3[5105]: + return 0
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-miniconda3[5105]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-yarn[5114]: ++ get_dataproc_property yarn.atsv2.bigtable.instance
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-yarn[5114]: ++ set +x
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-miniconda3[5105]: ++ date +%s.%N
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-miniconda3[5105]: + local -r end=1745777264.175926766
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-miniconda3[5105]: + local -r runtime_s=1
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-miniconda3[5105]: + echo 'Component miniconda3 took 1s to activate'
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-miniconda3[5105]: Component miniconda3 took 1s to activate
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-miniconda3[5105]: + local -r time_file=/tmp/dataproc/components/activate/miniconda3.time
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-miniconda3[5105]: + touch /tmp/dataproc/components/activate/miniconda3.time
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-miniconda3[5105]: + cat
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-spark[5111]: + readonly WAIT_TIMEOUT_SECONDS=200
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-spark[5111]: + WAIT_TIMEOUT_SECONDS=200
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-spark[5111]: + readonly INITIAL_WORKER_COUNT=1
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-spark[5111]: + INITIAL_WORKER_COUNT=1
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-miniconda3[5105]: + [[ 0 -ne 0 ]]
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-miniconda3[5105]: + echo 'Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/miniconda3.sh'
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-miniconda3[5105]: Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/miniconda3.sh
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-miniconda3[5105]: + touch /tmp/dataproc/components/activate/miniconda3.done
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-yarn[5114]: + readonly ATSV2_BIGTABLE_RESOURCE=
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-yarn[5114]: + ATSV2_BIGTABLE_RESOURCE=
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-yarn[5114]: + [[ -n '' ]]
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-spark[5111]: ++ get_dataproc_property_or_default dataproc:componentgateway.ha.enabled false
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-spark[5111]: ++ set +x
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-npd[5107]: + activated_components='knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3'
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-npd[5107]: + [[ knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3 == *kerberos* ]]
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-npd[5107]: + sed -i -e s#NM_URL_PLACEHOLDER#http://localhost:8042# /usr/local/share/google/dataproc/npd-config/yarn-nm-monitor.json
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: ++ echo 0
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-npd[5107]: + npd_configs+=("--config.yarn-monitor=${config_dir}/${yarn_nm_monitor_config_file}.json")
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-npd[5107]: + is_earlyoom_enabled
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-npd[5107]: + [[ Master == \M\a\s\t\e\r ]]
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-npd[5107]: + get_dataproc_property_or_default internal.node.main.memory-protection.enabled false
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-npd[5107]: + set +x
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-yarn[5114]: ++ date +%s.%N
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-server2[5100]: +++ DATAPROC_MASTER_ADDITIONAL=
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-server2[5100]: +++ MASTER_HOSTNAMES=($DATAPROC_MASTER ${DATAPROC_MASTER_ADDITIONAL//,/ })
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-server2[5100]: +++ NUM_MASTERS=1
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-yarn[5114]: + local -r end=1745777264.240389276
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-yarn[5114]: + local -r runtime_s=1
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-yarn[5114]: + echo 'Component yarn took 1s to activate'
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-yarn[5114]: Component yarn took 1s to activate
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-yarn[5114]: + local -r time_file=/tmp/dataproc/components/activate/yarn.time
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-yarn[5114]: + touch /tmp/dataproc/components/activate/yarn.time
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-yarn[5114]: + cat
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-server2[5100]: ++++ get_metadata_role
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-server2[5100]: ++++ get_dataproc_metadata DATAPROC_METADATA_ROLE attributes/dataproc-role
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-server2[5100]: ++++ set +x
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-yarn[5114]: + [[ 0 -ne 0 ]]
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-yarn[5114]: + echo 'Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/yarn.sh'
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-yarn[5114]: Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/yarn.sh
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-yarn[5114]: + touch /tmp/dataproc/components/activate/yarn.done
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: ++ echo 0
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: +++ DATAPROC_MASTER=cluster-dip-01-m
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: ++++ get_metadata_master_additional
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: ++++ get_dataproc_metadata DATAPROC_METADATA_MASTER_ADDITIONAL attributes/dataproc-master-additional
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: ++++ set +x
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-npd[5107]: true
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-npd[5107]: + local -r earlyoom_log_file=/var/log/earlyoom.log
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-npd[5107]: + touch /var/log/earlyoom.log
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-npd[5107]: + chmod a+rw /var/log/earlyoom.log
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-spark[5111]: + COMPONENT_GATEWAY_HA_ENABLED=false
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-spark[5111]: + readonly COMPONENT_GATEWAY_HA_ENABLED
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-spark[5111]: + activate_spark
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-spark[5111]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-spark[5111]: + should_start_history_server
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-spark[5111]: + [[ standard == \s\t\a\n\d\a\r\d ]]
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-spark[5111]: + [[ Master == \M\a\s\t\e\r ]]
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-spark[5111]: + [[ false == \t\r\u\e ]]
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-spark[5111]: + [[ 0 == \0 ]]
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-spark[5111]: + is_component_selected hdfs
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-spark[5111]: + local -r component=hdfs
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-spark[5111]: + local activated_components
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-spark[5111]: ++ get_components_to_activate
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-npd[5107]: + npd_configs+=("--config.system-log-monitor=${config_dir}/earlyoom-monitor.json")
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-npd[5107]: + create_systemd_service /etc/systemd/system/npd.service '--stackdriver-config=/usr/local/share/google/dataproc/npd-config/exporter/stackdriver-exporter.json --config.system-stats-monitor=/usr/local/share/google/dataproc/npd-config/system-stats-monitor.json,/usr/local/share/google/dataproc/npd-config/net-cgroup-system-stats-monitor.json --config.yarn-monitor=/usr/local/share/google/dataproc/npd-config/yarn-rm-monitor.json --config.yarn-monitor=/usr/local/share/google/dataproc/npd-config/yarn-nm-monitor.json --config.system-log-monitor=/usr/local/share/google/dataproc/npd-config/earlyoom-monitor.json'
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-npd[5107]: + local -r service_path=/etc/systemd/system/npd.service
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-npd[5107]: + local -r 'npd_config_args=--stackdriver-config=/usr/local/share/google/dataproc/npd-config/exporter/stackdriver-exporter.json --config.system-stats-monitor=/usr/local/share/google/dataproc/npd-config/system-stats-monitor.json,/usr/local/share/google/dataproc/npd-config/net-cgroup-system-stats-monitor.json --config.yarn-monitor=/usr/local/share/google/dataproc/npd-config/yarn-rm-monitor.json --config.yarn-monitor=/usr/local/share/google/dataproc/npd-config/yarn-nm-monitor.json --config.system-log-monitor=/usr/local/share/google/dataproc/npd-config/earlyoom-monitor.json'
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-npd[5107]: + cat
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-spark[5111]: ++ get_dataproc_property dataproc.components.activate
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-spark[5111]: ++ set +x
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-spark[5111]: ++ tr '[:upper:]' '[:lower:]'
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-npd[5107]: + systemctl daemon-reload
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-earlyoom[5096]: Created symlink /etc/systemd/system/multi-user.target.wants/earlyoom.service → /etc/systemd/system/earlyoom.service.
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-spark[5111]: + activated_components='knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3'
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-spark[5111]: + [[ knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3 == *hdfs* ]]
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-spark[5111]: + should_start_spark_connect
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-spark[5111]: + local role
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-spark[5111]: ++ get_metadata_role
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-spark[5111]: ++ get_dataproc_metadata DATAPROC_METADATA_ROLE attributes/dataproc-role
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-spark[5111]: ++ set +x
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-server2[5100]: +++ ROLE=Master
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-server2[5100]: +++ [[ 1 -gt 1 ]]
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-server2[5100]: +++ CLUSTER_MASTER_METASTORE_URIS=thrift://cluster-dip-01-m:9083
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-server2[5100]: ++ HIVE_CONF_DIR=/etc/hive/conf
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-server2[5100]: + set -x
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-server2[5100]: + activate_hive_server2
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-server2[5100]: + [[ Master == \M\a\s\t\e\r ]]
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-server2[5100]: + configure_hive_hbase
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-server2[5100]: + is_component_selected hbase
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-server2[5100]: + local -r component=hbase
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-server2[5100]: + local activated_components
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-server2[5100]: ++ get_components_to_activate
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-server2[5100]: ++ get_dataproc_property dataproc.components.activate
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-server2[5100]: ++ set +x
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: +++ DATAPROC_MASTER_ADDITIONAL=
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: +++ MASTER_HOSTNAMES=($DATAPROC_MASTER ${DATAPROC_MASTER_ADDITIONAL//,/ })
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: +++ NUM_MASTERS=1
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-fluentbit-ucp[5097]: ++ get_dataproc_property dataproc.logging.stackdriver.enable
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-fluentbit-ucp[5097]: ++ set +x
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: ++++ get_metadata_role
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: ++++ get_dataproc_metadata DATAPROC_METADATA_ROLE attributes/dataproc-role
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: ++++ set +x
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-server2[5100]: ++ tr '[:upper:]' '[:lower:]'
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-server2[5100]: + activated_components='knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3'
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-server2[5100]: + [[ knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3 == *hbase* ]]
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-server2[5100]: + is_component_selected hdfs
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-server2[5100]: + local -r component=hdfs
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-server2[5100]: + local activated_components
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-fluentbit-ucp[5097]: + STACKDRIVER_LOGGING_ENABLED=
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-fluentbit-ucp[5097]: + [[ '' == \f\a\l\s\e ]]
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-fluentbit-ucp[5097]: + is_legacy_containerized_agents_enabled
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-server2[5100]: ++ get_components_to_activate
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-server2[5100]: ++ get_dataproc_property dataproc.components.activate
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-server2[5100]: ++ set +x
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-server2[5100]: ++ tr '[:upper:]' '[:lower:]'
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-fluentbit-ucp[5097]: ++ get_dataproc_property_or_default dataproc.observability.containerised.legacy.agents.enable false
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-fluentbit-ucp[5097]: ++ set +x
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: +++ ROLE=Master
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: +++ [[ 1 -gt 1 ]]
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: +++ CLUSTER_MASTER_METASTORE_URIS=thrift://cluster-dip-01-m:9083
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: ++ HIVE_CONF_DIR=/etc/hive/conf
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/hive-metastore.sh
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../shared/hive-metastore.sh
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: ++ set -euo pipefail
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-server2[5100]: + activated_components='knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3'
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-server2[5100]: + [[ knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3 == *hdfs* ]]
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-server2[5100]: + echo 'Delaying starting Hive server until HDFS is ready'
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-server2[5100]: Delaying starting Hive server until HDFS is ready
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: +++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/hive-metastore.sh
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-spark[5111]: + role=Master
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-spark[5111]: + [[ Master == \M\a\s\t\e\r ]]
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: ++ source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_metadata.sh
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-spark[5111]: + is_spark_connect_enabled
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-spark[5111]: + local spark_connect_enabled
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: ++ set -x
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-server2[5100]: ++ date +%s.%N
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-fluentbit-ucp[5097]: + local -r legacy_containerised_agents_enabled=false
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-fluentbit-ucp[5097]: + [[ false == \t\r\u\e ]]
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-fluentbit-ucp[5097]: + enable_and_start_service fluentbit-ucp
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-fluentbit-ucp[5097]: + local -r service=fluentbit-ucp
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-fluentbit-ucp[5097]: + enable_service fluentbit-ucp
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-fluentbit-ucp[5097]: + local -r service=fluentbit-ucp
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-fluentbit-ucp[5097]: + local -r unit=fluentbit-ucp.service
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-fluentbit-ucp[5097]: + retry_constant_short systemctl enable fluentbit-ucp.service
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-fluentbit-ucp[5097]: + retry_constant_custom 30 1 systemctl enable fluentbit-ucp.service
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-fluentbit-ucp[5097]: + local -r max_retry_time=30
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-fluentbit-ucp[5097]: + local -r retry_delay=1
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-fluentbit-ucp[5097]: + cmd=('systemctl' 'enable' 'fluentbit-ucp.service')
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-fluentbit-ucp[5097]: + local -r cmd
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-fluentbit-ucp[5097]: + local -r max_retries=30
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-fluentbit-ucp[5097]: + local reenable_x=false
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-fluentbit-ucp[5097]: + [[ -o xtrace ]]
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-fluentbit-ucp[5097]: + set +x
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-fluentbit-ucp[5097]: About to run 'systemctl enable fluentbit-ucp.service' with retries...
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: +++ get_metadata_master
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-spark[5111]: ++ get_java_property /tmp/cluster/properties/dataproc.properties internal.s8s.spark-connect.enabled
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-server2[5100]: + local -r end=1745777264.469426840
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: ++ get_dataproc_property dataproc.proxy.agent.endpoint
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-server2[5100]: + local -r runtime_s=1
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-server2[5100]: + echo 'Component hive-server2 took 1s to activate'
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-server2[5100]: Component hive-server2 took 1s to activate
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-server2[5100]: + local -r time_file=/tmp/dataproc/components/activate/hive-server2.time
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-server2[5100]: + touch /tmp/dataproc/components/activate/hive-server2.time
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: ++ sed 's/\\:/:/'
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: ++ set +x
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: +++ get_dataproc_metadata DATAPROC_METADATA_MASTER attributes/dataproc-master
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: +++ set +x
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-spark[5111]: ++ set +x
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-server2[5100]: + cat
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-server2[5100]: + [[ 0 -ne 0 ]]
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-server2[5100]: + echo 'Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/hive-server2.sh'
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-server2[5100]: Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/hive-server2.sh
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-server2[5100]: + touch /tmp/dataproc/components/activate/hive-server2.done
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-spark[5111]: + spark_connect_enabled=
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-spark[5111]: + [[ '' == \t\r\u\e ]]
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-spark[5111]: ++ date +%s.%N
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: + ENDPOINT_HOSTNAME=https://us-east1.dataproc.cloud.google.com/
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: + readonly ENDPOINT_HOSTNAME
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: + readonly PROXY=https://us-east1.dataproc.cloud.google.com/tun/m/4592f092208ecc84946b8f8f8016274df1b36a14/
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: + PROXY=https://us-east1.dataproc.cloud.google.com/tun/m/4592f092208ecc84946b8f8f8016274df1b36a14/
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: ++ echo 0
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: + readonly HOST=localhost:8443
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: + HOST=localhost:8443
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: + readonly SHIM_PATH=websocket-shim
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: + SHIM_PATH=websocket-shim
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-spark[5111]: + local -r end=1745777264.507833110
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-spark[5111]: + local -r runtime_s=1
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-spark[5111]: + echo 'Component spark took 1s to activate'
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-spark[5111]: Component spark took 1s to activate
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-spark[5111]: + local -r time_file=/tmp/dataproc/components/activate/spark.time
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-spark[5111]: + touch /tmp/dataproc/components/activate/spark.time
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-spark[5111]: + cat
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: ++ get_dataproc_property dataproc.proxy.backend.id
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: ++ set +x
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-spark[5111]: + [[ 0 -ne 0 ]]
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-spark[5111]: + echo 'Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/spark.sh'
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-spark[5111]: Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/spark.sh
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-spark[5111]: + touch /tmp/dataproc/components/activate/spark.done
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: ++ echo 0
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: + BACKEND=hai26gzvrngzjp6hwhycsnr4ly
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: + readonly BACKEND
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: ++ get_dataproc_property dataproc.proxy.agent.enablewebsockets
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: ++ set +x
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-earlyoom[5096]: 'systemctl enable earlyoom.service' succeeded after 1 execution(s).
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-earlyoom[5096]: + return 0
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-earlyoom[5096]: + local -r drop_in_dir=/etc/systemd/system/earlyoom.service.d
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-earlyoom[5096]: + mkdir -p /etc/systemd/system/earlyoom.service.d
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: ++ DATAPROC_MASTER=cluster-dip-01-m
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-earlyoom[5096]: + local props
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: +++ get_metadata_master_additional
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: +++ get_dataproc_metadata DATAPROC_METADATA_MASTER_ADDITIONAL attributes/dataproc-master-additional
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: +++ set +x
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-earlyoom[5096]: ++ retry_constant_short systemctl show earlyoom.service -p Restart,RemainAfterExit
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-earlyoom[5096]: ++ retry_constant_custom 30 1 systemctl show earlyoom.service -p Restart,RemainAfterExit
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-earlyoom[5096]: ++ local -r max_retry_time=30
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-earlyoom[5096]: ++ local -r retry_delay=1
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-earlyoom[5096]: ++ cmd=('systemctl' 'show' 'earlyoom.service' '-p' 'Restart,RemainAfterExit')
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-earlyoom[5096]: ++ local -r cmd
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-earlyoom[5096]: ++ local -r max_retries=30
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-earlyoom[5096]: ++ local reenable_x=false
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-earlyoom[5096]: ++ [[ -o xtrace ]]
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-earlyoom[5096]: ++ set +x
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-earlyoom[5096]: About to run 'systemctl show earlyoom.service -p Restart,RemainAfterExit' with retries...
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: + ENABLE_WEBSOCKETS=true
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: + readonly ENABLE_WEBSOCKETS
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: ++ get_dataproc_property dataproc.proxy.agent.sessioncookie
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: ++ set +x
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: + loginfo 'Merged /tmp/cluster/properties/knox.xml.'
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: + echo 'Merged /tmp/cluster/properties/knox.xml.'
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: Merged /tmp/cluster/properties/knox.xml.
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: + generate_topology
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: + mv knox/topology-template.xml /usr/lib/knox/conf/topologies/default.xml
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: + local master_hostname
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: ++ get_metadata_master
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: + SESSION_COOKIE_NAME=_xsrf
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: + readonly SESSION_COOKIE_NAME
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: ++ get_dataproc_metadata DATAPROC_METADATA_MASTER attributes/dataproc-master
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: ++ set +x
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: ++ get_dataproc_property dataproc.proxy.agent.enablebanner
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: ++ set +x
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: ++ DATAPROC_MASTER_ADDITIONAL=
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: ++ MASTER_HOSTNAMES=($DATAPROC_MASTER ${DATAPROC_MASTER_ADDITIONAL//,/ })
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: ++ NUM_MASTERS=1
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: +++ get_metadata_role
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: +++ get_dataproc_metadata DATAPROC_METADATA_ROLE attributes/dataproc-role
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: +++ set +x
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: + master_hostname=cluster-dip-01-m
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: + local scheme=http
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: + local host=localhost
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: + local apphistory_port=8188
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: + local yarn_ats_v2_port=8192
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: + local sparkhistory_port=18080
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: + local hdfs_port=9870
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: + local nodeui_port=8042
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: + local jobhistory_port=19888
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: + local yarn_port=8088
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: + local presto_port=8060
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: + local trino_port=8060
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: + local hiveserver2ui_port=10002
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: + local flinkhistory_port=8500
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: + is_component_selected kerberos
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: + local -r component=kerberos
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: + local activated_components
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: ++ get_components_to_activate
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: ++ tr '[:upper:]' '[:lower:]'
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: ++ get_dataproc_property dataproc.components.activate
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: ++ set +x
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: + ENABLE_BANNER=true
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: + readonly ENABLE_BANNER
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: + SHIM_WEBSOCKETS_FLAG=-shim-websockets
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: + [[ true == \t\r\u\e ]]
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: + SHIM_WEBSOCKETS_FLAG=
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: + SESSION_COOKIE_NAME_FLAG=
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: + [[ -n _xsrf ]]
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: + SESSION_COOKIE_NAME_FLAG=-session-cookie-name=_xsrf
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: ++ get_metadata_project_id
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: ++ get_dataproc_metadata DATAPROC_METADATA_PROJECT_ID ../project/project-id
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: ++ set +x
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: + activated_components='knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3'
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: + [[ knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3 == *kerberos* ]]
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: + local -r topology_conf=/usr/lib/knox/conf/topologies/default.xml
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: + sed -i -e 's/{_MASTER_HOSTNAME_0}/cluster-dip-01-m/g' /usr/lib/knox/conf/topologies/default.xml
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: + (( 1 > 1 ))
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: + sed -i -e 's/{_SCHEME_}/http/g' /usr/lib/knox/conf/topologies/default.xml
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: + sed -i -e 's/{_HOST_}/localhost/g' /usr/lib/knox/conf/topologies/default.xml
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: ++ ROLE=Master
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: ++ [[ 1 -gt 1 ]]
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: ++ CLUSTER_MASTER_METASTORE_URIS=thrift://cluster-dip-01-m:9083
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: + local jupyter_port
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: + is_component_selected jupyter-kernel-gateway
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: + local -r component=jupyter-kernel-gateway
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: + local activated_components
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/hive-metastore.sh
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: ++ get_components_to_activate
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../shared/mysql.sh
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_versions.sh
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: ++ set +x
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: ++ is_version_at_least 2.2 2.2
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: ++ local reenable_x=false
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: ++ [[ -o xtrace ]]
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: ++ set +x
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: ++ tr '[:upper:]' '[:lower:]'
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: ++ case ${compare_versions_result} in
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: ++ return 0
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: ++ MYSQL_VERSION=8.0
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: ++ MYSQL_EL_VERSION=8
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: + set -x
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: + HIVE_CONF_DIR=/etc/hive/conf
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: ++ get_dataproc_property dataproc.components.activate
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: ++ set +x
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: ++ get_metadata_role
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: ++ get_dataproc_metadata DATAPROC_METADATA_ROLE attributes/dataproc-role
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: + PROJECT_ID=euphoric-coral-451717-v8
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: + readonly PROJECT_ID
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: ++ set +x
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: ++ get_metadata_cluster_name
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: ++ get_dataproc_metadata DATAPROC_METADATA_CLUSTER_NAME attributes/dataproc-cluster-name
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: ++ set +x
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hdfs[5098]: + MASTER_HOSTNAMES=(${DATAPROC_MASTER} ${DATAPROC_MASTER_ADDITIONAL//,/ })
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hdfs[5098]: ++ get_metadata_cluster_name
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hdfs[5098]: ++ get_dataproc_metadata DATAPROC_METADATA_CLUSTER_NAME attributes/dataproc-cluster-name
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hdfs[5098]: ++ set +x
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: + activated_components='knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3'
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: + [[ knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3 == *jupyter-kernel-gateway* ]]
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: ++ get_dataproc_property_or_default jupyter.port 8123
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: ++ set +x
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: + output='# /lib/systemd/system/google-osconfig-agent.service
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: [Unit]
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: Description=Google OSConfig Agent
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: After=local-fs.target network-online.target
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: Wants=local-fs.target network-online.target
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: 
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: [Service]
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: ExecStart=/usr/bin/google_osconfig_agent
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: Restart=always
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: RestartSec=1
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: StartLimitInterval=120
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: StartLimitBurst=3
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: KillMode=mixed
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: KillSignal=SIGTERM
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: 
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: [Install]
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: WantedBy=multi-user.target
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: 
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: # /etc/systemd/system/google-osconfig-agent.service.d/dataproc.conf
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: [Unit]
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: ConditionPathExists=!/etc/google-dataproc/hermetic_vm'
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: + status=0
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: + [[ 0 -eq 0 ]]
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: + return 0
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: + loginfo 'Starting service google-osconfig-agent'
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: + echo 'Starting service google-osconfig-agent'
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: Starting service google-osconfig-agent
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: + run_in_background --tag start-osconfig-service enable_and_start_service google-osconfig-agent
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: + local -r pid=6135
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: + [[ --tag == \-\-\t\a\g ]]
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: + shift 2
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/6135.running ]]
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: + echo 'enable_and_start_service google-osconfig-agent'
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: + echo 'Started background process [enable_and_start_service google-osconfig-agent] as pid 6135'
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: Started background process [enable_and_start_service google-osconfig-agent] as pid 6135
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: + is_syslog_logging_enabled
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: + local syslog_enabled
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: + [[ standard == \s\t\a\n\d\a\r\d ]]
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: ++ get_dataproc_property_or_default dataproc.logging.syslog.enabled false
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: + trap 'echo "$?" >"${COMMANDS_TMP_DIR}/${BASHPID}.exitcode"' EXIT
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: + run_with_logger --tag start-osconfig-service enable_and_start_service google-osconfig-agent
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: + local tag=dataproc-script
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: + local pid=6135
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: + [[ --tag == \-\-\t\a\g ]]
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: + tag=dataproc-start-osconfig-service
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: + shift 2
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: + [[ 2 -eq 0 ]]
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: + enable_and_start_service google-osconfig-agent
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: ++ logger -s -t 'dataproc-start-osconfig-service[6135]'
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: ++ set +x
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-start-osconfig-service[6135]: + local -r service=google-osconfig-agent
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-start-osconfig-service[6135]: + enable_service google-osconfig-agent
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: + ROLE=Master
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hdfs[5098]: + CLUSTER_NAME=cluster-dip-01
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hdfs[5098]: + activate_hdfs
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hdfs[5098]: + mkdir -p /var/run/hadoop-hdfs
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hdfs[5098]: + sudo -u hdfs hdfs namenode -genclusterid
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-start-osconfig-service[6135]: + local -r service=google-osconfig-agent
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-start-osconfig-service[6135]: + local -r unit=google-osconfig-agent.service
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-start-osconfig-service[6135]: + retry_constant_short systemctl enable google-osconfig-agent.service
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-start-osconfig-service[6135]: + retry_constant_custom 30 1 systemctl enable google-osconfig-agent.service
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-start-osconfig-service[6135]: + local -r max_retry_time=30
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-start-osconfig-service[6135]: + local -r retry_delay=1
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-start-osconfig-service[6135]: + cmd=('systemctl' 'enable' 'google-osconfig-agent.service')
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-start-osconfig-service[6135]: + local -r cmd
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-start-osconfig-service[6135]: + local -r max_retries=30
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-start-osconfig-service[6135]: + local reenable_x=false
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: ++ get_metadata_master
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: ++ get_dataproc_metadata DATAPROC_METADATA_MASTER attributes/dataproc-master
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: ++ set +x
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-start-osconfig-service[6135]: + [[ -o xtrace ]]
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-start-osconfig-service[6135]: + set +x
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-start-osconfig-service[6135]: About to run 'systemctl enable google-osconfig-agent.service' with retries...
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hdfs[5098]: + chown root:hdfs /var/run/hadoop-hdfs
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: + CLUSTER_NAME=cluster-dip-01
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: + readonly CLUSTER_NAME
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hdfs[5098]: + chmod 775 /var/run/hadoop-hdfs
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: + jupyter_port=8123
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: ++ get_metadata_dataproc_region
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: ++ get_dataproc_metadata DATAPROC_METADATA_REGION attributes/dataproc-region
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: ++ set +x
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: + sed -i s/JUPYTER-PORT/8123/g /usr/lib/knox/conf/topologies/default.xml
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hdfs[5098]: + [[ Master == \M\a\s\t\e\r ]]
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hdfs[5098]: + start_master_services
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hdfs[5098]: + login_as_hdfs_if_kerberos_enabled
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hdfs[5098]: + is_component_selected kerberos
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hdfs[5098]: + local -r component=kerberos
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hdfs[5098]: + local activated_components
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: + local zeppelin_port
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: ++ get_dataproc_property_or_default zeppelin.port 8080
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: ++ set +x
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hdfs[5098]: ++ get_components_to_activate
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: + syslog_enabled=false
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: + [[ false == \t\r\u\e ]]
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: + [[ standard == \s\t\a\n\d\a\r\d ]]
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: + [[ us-east1 != \g\l\o\b\a\l ]]
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: + add_regional_bigtop_repo us-east1
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: + local -r region=us-east1
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: + local -r dataproc_repo_file=/etc/apt/sources.list.d/dataproc.list
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: + is_test_bigtop_repo /etc/apt/sources.list.d/dataproc.list
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: + local -r dataproc_repo_file=/etc/apt/sources.list.d/dataproc.list
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: + return '!' grep -q dataproc-bigtop-repo /etc/apt/sources.list.d/dataproc.list
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: /usr/local/share/google/dataproc/bdutil/os/shared.sh: line 8: return: !: numeric argument required
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: + local regional_bigtop_repo_uri
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hdfs[5098]: ++ get_dataproc_property dataproc.components.activate
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hdfs[5098]: ++ set +x
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hdfs[5098]: ++ tr '[:upper:]' '[:lower:]'
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: ++ cat /etc/apt/sources.list.d/dataproc.list
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: ++ grep 'deb .*goog-dataproc-bigtop-repo-us-east1.* dataproc contrib'
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: ++ cut -d ' ' -f 2
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: ++ head -1
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: ++ sed s#dataproc-bigtop-repo#goog-dataproc-bigtop-repo-us-east1#
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: + DATAPROC_MASTER=cluster-dip-01-m
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: ++ get_metadata_bucket
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: + zeppelin_port=8080
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: ++ get_dataproc_metadata DATAPROC_METADATA_BUCKET attributes/dataproc-bucket
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: + sed -i s/ZEPPELIN-PORT/8080/g /usr/lib/knox/conf/topologies/default.xml
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: ++ set +x
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: + sed -i -e 's/{_APPHISTORY_PORT_}/8188/g' /usr/lib/knox/conf/topologies/default.xml
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: + sed -i -e 's/{_APPTIMELINE_V2_PORT_}/8192/g' /usr/lib/knox/conf/topologies/default.xml
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: + regional_bigtop_repo_uri=https://storage.googleapis.com/goog-dataproc-bigtop-repo-us-east1/2_2_deb12_20250415_125600-RC01
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: + [[ https://storage.googleapis.com/goog-dataproc-bigtop-repo-us-east1/2_2_deb12_20250415_125600-RC01 == */ ]]
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: + local -r bigtop_key_uri=https://storage.googleapis.com/goog-dataproc-bigtop-repo-us-east1/2_2_deb12_20250415_125600-RC01/archive.key
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: + apt-key add -
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: + curl -fsS --retry-connrefused --retry 3 --retry-delay 5 https://storage.googleapis.com/goog-dataproc-bigtop-repo-us-east1/2_2_deb12_20250415_125600-RC01/archive.key
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hdfs[5098]: + activated_components='knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3'
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hdfs[5098]: + [[ knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3 == *kerberos* ]]
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hdfs[5098]: + [[ 1 == \1 ]]
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hdfs[5098]: + start_hdfs_namenode
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hdfs[5098]: + case "${MASTER_INDEX?}" in
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hdfs[5098]: + loginfo 'Formatting NameNode'
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hdfs[5098]: + echo 'Formatting NameNode'
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hdfs[5098]: Formatting NameNode
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hdfs[5098]: + retry_constant_short su -s /bin/bash hdfs -c 'source /etc/default/hadoop-hdfs-namenode &&           hdfs namenode -format -nonInteractive'
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hdfs[5098]: + retry_constant_custom 30 1 su -s /bin/bash hdfs -c 'source /etc/default/hadoop-hdfs-namenode &&           hdfs namenode -format -nonInteractive'
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hdfs[5098]: + local -r max_retry_time=30
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hdfs[5098]: + local -r retry_delay=1
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hdfs[5098]: + cmd=('su' '-s' '/bin/bash' 'hdfs' '-c' 'source /etc/default/hadoop-hdfs-namenode &&           hdfs namenode -format -nonInteractive')
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hdfs[5098]: + local -r cmd
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hdfs[5098]: + local -r max_retries=30
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hdfs[5098]: + local reenable_x=false
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hdfs[5098]: + [[ -o xtrace ]]
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hdfs[5098]: + set +x
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hdfs[5098]: About to run 'su -s /bin/bash hdfs -c source /etc/default/hadoop-hdfs-namenode &&           hdfs namenode -format -nonInteractive' with retries...
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: + sed -i -e 's/{_SPARKHISTORY_PORT_}/18080/g' /usr/lib/knox/conf/topologies/default.xml
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: + REGION_NAME=us-east1
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: + readonly REGION_NAME
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: + readonly SIGNOUT_URL=https://us-east1.dataproc.cloud.google.com/_signout
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: + SIGNOUT_URL=https://us-east1.dataproc.cloud.google.com/_signout
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: + readonly BANNER_HTML=/opt/dataproc/proxy-agent/banner.html
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: + BANNER_HTML=/opt/dataproc/proxy-agent/banner.html
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: + INJECT_BANNER_FLAG=
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: + [[ true == \t\r\u\e ]]
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: ++ dirname /opt/dataproc/proxy-agent/banner.html
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: + sed -i -e 's/{_FLINKHISTORY_PORT_}/8500/g' /usr/lib/knox/conf/topologies/default.xml
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: + mkdir -p /opt/dataproc/proxy-agent
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: + cat
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: + sed -i -e 's/{_HDFS_PORT_}/9870/g' /usr/lib/knox/conf/topologies/default.xml
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: + INJECT_BANNER_FLAG='-banner-height=40px -inject-banner="$(cat /opt/dataproc/proxy-agent/banner.html)"'
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: + AFTER_UNITS=()
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: + REQUIRES_UNITS=()
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: + is_rm_image
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: + sed -i -e 's/{_NODEUI_PORT_}/8042/g' /usr/lib/knox/conf/topologies/default.xml
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\r\a\y ]]
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: + AFTER_UNITS+=(google-guest-agent.service google-startup-scripts.service local-fs.target network-online.target)
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: + is_ubuntu
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: ++ os_id
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: + sed -i -e 's/{_JOBHISTORY_PORT_}/19888/g' /usr/lib/knox/conf/topologies/default.xml
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: ++ xargs
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: ++ grep '^ID=' /etc/os-release
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: ++ cut -d= -f2
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: + sed -i -e 's/{_YARN_PORT_}/8088/g' /usr/lib/knox/conf/topologies/default.xml
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: + CONFIGBUCKET=dataproc-staging-us-east1-679657336577-kutp8kah
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: + HIVE_STAGING_LOCATION=gs://dataproc-staging-us-east1-679657336577-kutp8kah/google-cloud-dataproc-metainfo/0b63cfa9-1f8a-481e-87c8-a0cf661f3625/hive
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-hive-metastore[5099]: + set +x
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: + sed -i -e 's/{_PRESTO_PORT_}/8060/g' /usr/lib/knox/conf/topologies/default.xml
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: + [[ debian == \u\b\u\n\t\u ]]
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: + sed -i -e 's/{_TRINO_PORT_}/8060/g' /usr/lib/knox/conf/topologies/default.xml
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: ++ get_dataproc_property_or_default dataproc:componentgateway.ha.enabled false
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-proxy-agent[5110]: ++ set +x
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: + sed -i -e 's/{_HIVESERVER2UI_PORT_}/10002/g' /usr/lib/knox/conf/topologies/default.xml
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: + is_component_selected jupyter-kernel-gateway
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: + local -r component=jupyter-kernel-gateway
<13>Apr 27 18:07:44 dataproc-startup-script[1265]: <13>Apr 27 18:07:44 dataproc-activate-component-knox[5101]: + local activated_components
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: ++ get_components_to_activate
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: ++ get_dataproc_property dataproc.components.activate
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: ++ set +x
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: ++ tr '[:upper:]' '[:lower:]'
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-proxy-agent[5110]: + COMPONENT_GATEWAY_HA_ENABLED=false
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-proxy-agent[5110]: + readonly COMPONENT_GATEWAY_HA_ENABLED
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-proxy-agent[5110]: + [[ false == \t\r\u\e ]]
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-proxy-agent[5110]: + FORCE_HTTP2_FLAG=
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-proxy-agent[5110]: + should_start_spark_connect
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-proxy-agent[5110]: + local role
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-proxy-agent[5110]: ++ get_metadata_role
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-proxy-agent[5110]: ++ get_dataproc_metadata DATAPROC_METADATA_ROLE attributes/dataproc-role
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-proxy-agent[5110]: ++ set +x
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: + activated_components='knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3'
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: + [[ knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3 == *jupyter-kernel-gateway* ]]
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: + is_component_selected kerberos
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: + local -r component=kerberos
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: + local activated_components
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: ++ get_components_to_activate
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: ++ tr '[:upper:]' '[:lower:]'
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: ++ get_dataproc_property dataproc.components.activate
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: ++ set +x
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: + activated_components='knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3'
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: + [[ knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3 == *kerberos* ]]
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: + set_websocket_size
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: + local max_ws_size=1024000
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: + is_component_selected zeppelin
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: + local -r component=zeppelin
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: + local activated_components
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: ++ get_components_to_activate
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: ++ tr '[:upper:]' '[:lower:]'
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: ++ get_dataproc_property dataproc.components.activate
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-proxy-agent[5110]: + role=Master
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-proxy-agent[5110]: + [[ Master == \M\a\s\t\e\r ]]
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-proxy-agent[5110]: + is_spark_connect_enabled
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-proxy-agent[5110]: + local spark_connect_enabled
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: ++ set +x
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-proxy-agent[5110]: ++ get_java_property /tmp/cluster/properties/dataproc.properties internal.s8s.spark-connect.enabled
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-proxy-agent[5110]: ++ set +x
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: + activated_components='knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3'
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: + [[ knox proxy-agent earlyoom hdfs hive-metastore hive-server2 mapreduce mysql npd spark tez yarn pig fluentbit-ucp otel-ucp miniconda3 == *zeppelin* ]]
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: + set_property_in_xml /usr/lib/knox/conf/gateway-site.xml gateway.websocket.max.binary.buffer.size 1024000
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: + local -r xml_file=/usr/lib/knox/conf/gateway-site.xml
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: + local -r name=gateway.websocket.max.binary.buffer.size
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: + local -r value=1024000
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: + local -r mode=overwrite
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: + local -r delimiter=,
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: + local skip=false
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: + local old_value
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: + local new_value
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: + [[ overwrite == \o\v\e\r\w\r\i\t\e ]]
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: + new_value=1024000
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: + [[ false == \f\a\l\s\e ]]
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: + bdconfig set_property --configuration_file /usr/lib/knox/conf/gateway-site.xml --name gateway.websocket.max.binary.buffer.size --value 1024000 --create_if_absent --clobber
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-proxy-agent[5110]: + spark_connect_enabled=
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-proxy-agent[5110]: + [[ '' == \t\r\u\e ]]
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-proxy-agent[5110]: + cat
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-proxy-agent[5110]: + is_rm_image
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-proxy-agent[5110]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-proxy-agent[5110]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\r\a\y ]]
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-proxy-agent[5110]: + enable_service google-dataproc-component-gateway
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-proxy-agent[5110]: + local -r service=google-dataproc-component-gateway
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-proxy-agent[5110]: + local -r unit=google-dataproc-component-gateway.service
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-proxy-agent[5110]: + retry_constant_short systemctl enable google-dataproc-component-gateway.service
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-proxy-agent[5110]: + retry_constant_custom 30 1 systemctl enable google-dataproc-component-gateway.service
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-proxy-agent[5110]: + local -r max_retry_time=30
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-proxy-agent[5110]: + local -r retry_delay=1
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-proxy-agent[5110]: + cmd=('systemctl' 'enable' 'google-dataproc-component-gateway.service')
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-proxy-agent[5110]: + local -r cmd
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-proxy-agent[5110]: + local -r max_retries=30
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-proxy-agent[5110]: + local reenable_x=false
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-proxy-agent[5110]: + [[ -o xtrace ]]
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-proxy-agent[5110]: + set +x
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-proxy-agent[5110]: About to run 'systemctl enable google-dataproc-component-gateway.service' with retries...
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: Warning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead (see apt-key(8)).
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-otel-ucp[5108]: 'systemctl enable otel-metrics-agent.service' succeeded after 1 execution(s).
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-otel-ucp[5108]: + return 0
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-otel-ucp[5108]: + local -r drop_in_dir=/etc/systemd/system/otel-metrics-agent.service.d
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-otel-ucp[5108]: + mkdir -p /etc/systemd/system/otel-metrics-agent.service.d
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-otel-ucp[5108]: + local props
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-otel-ucp[5108]: ++ retry_constant_short systemctl show otel-metrics-agent.service -p Restart,RemainAfterExit
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-otel-ucp[5108]: ++ retry_constant_custom 30 1 systemctl show otel-metrics-agent.service -p Restart,RemainAfterExit
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-otel-ucp[5108]: ++ local -r max_retry_time=30
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-otel-ucp[5108]: ++ local -r retry_delay=1
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-otel-ucp[5108]: ++ cmd=('systemctl' 'show' 'otel-metrics-agent.service' '-p' 'Restart,RemainAfterExit')
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-otel-ucp[5108]: ++ local -r cmd
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-otel-ucp[5108]: ++ local -r max_retries=30
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-otel-ucp[5108]: ++ local reenable_x=false
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-otel-ucp[5108]: ++ [[ -o xtrace ]]
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-otel-ucp[5108]: ++ set +x
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-otel-ucp[5108]: About to run 'systemctl show otel-metrics-agent.service -p Restart,RemainAfterExit' with retries...
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-hive-metastore[5099]: + activate_hive_metastore
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-hive-metastore[5099]: + [[ Master == \M\a\s\t\e\r ]]
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-hive-metastore[5099]: + wait_for_mysql
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-hive-metastore[5099]: + wait_for_port mysql cluster-dip-01-m 3306
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-hive-metastore[5099]: + local -r name=mysql
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-hive-metastore[5099]: + local -r host=cluster-dip-01-m
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-hive-metastore[5099]: + local -r port=3306
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-hive-metastore[5099]: + local -r timeout=300
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-hive-metastore[5099]: + local -r capped_timeout=300
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-hive-metastore[5099]: + loginfo 'Waiting 300 seconds for service to come up on host=cluster-dip-01-m port=3306 name=mysql.'
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-hive-metastore[5099]: + echo 'Waiting 300 seconds for service to come up on host=cluster-dip-01-m port=3306 name=mysql.'
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-hive-metastore[5099]: Waiting 300 seconds for service to come up on host=cluster-dip-01-m port=3306 name=mysql.
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-hive-metastore[5099]: + retry_constant_custom 300 1 nc -v -z -w 1 cluster-dip-01-m 3306
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-hive-metastore[5099]: + local -r max_retry_time=300
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-hive-metastore[5099]: + local -r retry_delay=1
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-hive-metastore[5099]: + cmd=('nc' '-v' '-z' '-w' '1' 'cluster-dip-01-m' '3306')
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-hive-metastore[5099]: + local -r cmd
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-hive-metastore[5099]: + local -r max_retries=300
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-hive-metastore[5099]: + local reenable_x=false
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-hive-metastore[5099]: + [[ -o xtrace ]]
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-hive-metastore[5099]: + set +x
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-hive-metastore[5099]: About to run 'nc -v -z -w 1 cluster-dip-01-m 3306' with retries...
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-hive-metastore[5099]: Connection to cluster-dip-01-m (10.142.0.8) 3306 port [tcp/mysql] succeeded!
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-hive-metastore[5099]: 'nc -v -z -w 1 cluster-dip-01-m 3306' succeeded after 1 execution(s).
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-hive-metastore[5099]: + return 0
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-hive-metastore[5099]: + loginfo 'Service up on host=cluster-dip-01-m port=3306 name=mysql.'
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-hive-metastore[5099]: + echo 'Service up on host=cluster-dip-01-m port=3306 name=mysql.'
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-hive-metastore[5099]: Service up on host=cluster-dip-01-m port=3306 name=mysql.
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-hive-metastore[5099]: + update_hive_password
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-hive-metastore[5099]: + set +x
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: + set_property_in_xml /usr/lib/knox/conf/gateway-site.xml gateway.websocket.max.text.buffer.size 1024000
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: + local -r xml_file=/usr/lib/knox/conf/gateway-site.xml
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: + local -r name=gateway.websocket.max.text.buffer.size
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: + local -r value=1024000
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: + local -r mode=overwrite
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: + local -r delimiter=,
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: + local skip=false
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: + local old_value
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: + local new_value
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: + [[ overwrite == \o\v\e\r\w\r\i\t\e ]]
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: + new_value=1024000
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: + [[ false == \f\a\l\s\e ]]
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: + bdconfig set_property --configuration_file /usr/lib/knox/conf/gateway-site.xml --name gateway.websocket.max.text.buffer.size --value 1024000 --create_if_absent --clobber
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-npd[5107]: + enable_and_start_service npd
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-npd[5107]: + local -r service=npd
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-npd[5107]: + enable_service npd
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-npd[5107]: + local -r service=npd
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-npd[5107]: + local -r unit=npd.service
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-npd[5107]: + retry_constant_short systemctl enable npd.service
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-npd[5107]: + retry_constant_custom 30 1 systemctl enable npd.service
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-npd[5107]: + local -r max_retry_time=30
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-npd[5107]: + local -r retry_delay=1
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-npd[5107]: + cmd=('systemctl' 'enable' 'npd.service')
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-npd[5107]: + local -r cmd
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-npd[5107]: + local -r max_retries=30
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-npd[5107]: + local reenable_x=false
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-npd[5107]: + [[ -o xtrace ]]
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-npd[5107]: + set +x
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-npd[5107]: About to run 'systemctl enable npd.service' with retries...
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: + [[ false == \t\r\u\e ]]
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: + generate_runfiles_and_start
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: + [[ -L /usr/lib/knox/pids ]]
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: + rm /usr/lib/knox/pids
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: + mkdir /usr/lib/knox/pids
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: + retry_constant_short /usr/lib/knox/bin/knoxcli.sh create-master --generate
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: + retry_constant_custom 30 1 /usr/lib/knox/bin/knoxcli.sh create-master --generate
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: + local -r max_retry_time=30
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: + local -r retry_delay=1
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: + cmd=('/usr/lib/knox/bin/knoxcli.sh' 'create-master' '--generate')
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: + local -r cmd
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: + local -r max_retries=30
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: + local reenable_x=false
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: + [[ -o xtrace ]]
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: + set +x
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-knox[5101]: About to run '/usr/lib/knox/bin/knoxcli.sh create-master --generate' with retries...
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-earlyoom[5096]: 'systemctl show earlyoom.service -p Restart,RemainAfterExit' succeeded after 1 execution(s).
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-earlyoom[5096]: ++ return 0
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-earlyoom[5096]: + props='Restart=always
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-earlyoom[5096]: RemainAfterExit=no'
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-earlyoom[5096]: + [[ earlyoom != \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-earlyoom[5096]: + [[ earlyoom != \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-earlyoom[5096]: + [[ Restart=always
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-earlyoom[5096]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-earlyoom[5096]: + local -r gate_start_on_agent_success_drop_in=/etc/systemd/system/common/agent-gate.conf
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-earlyoom[5096]: + [[ ! -f /etc/systemd/system/common/agent-gate.conf ]]
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-earlyoom[5096]: + [[ earlyoom == \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-earlyoom[5096]: + [[ earlyoom == \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-earlyoom[5096]: + start_service earlyoom
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-earlyoom[5096]: + start_services earlyoom
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-earlyoom[5096]: + units=('earlyoom.service')
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-earlyoom[5096]: + local -r units
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-earlyoom[5096]: + retry_constant_short systemctl start earlyoom.service
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-earlyoom[5096]: + retry_constant_custom 30 1 systemctl start earlyoom.service
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-earlyoom[5096]: + local -r max_retry_time=30
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-earlyoom[5096]: + local -r retry_delay=1
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-earlyoom[5096]: + cmd=('systemctl' 'start' 'earlyoom.service')
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-earlyoom[5096]: + local -r cmd
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-earlyoom[5096]: + local -r max_retries=30
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-earlyoom[5096]: + local reenable_x=false
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-earlyoom[5096]: + [[ -o xtrace ]]
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-earlyoom[5096]: + set +x
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-earlyoom[5096]: About to run 'systemctl start earlyoom.service' with retries...
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-fluentbit-ucp[5097]: 'systemctl enable fluentbit-ucp.service' succeeded after 1 execution(s).
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-fluentbit-ucp[5097]: + return 0
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-fluentbit-ucp[5097]: + local -r drop_in_dir=/etc/systemd/system/fluentbit-ucp.service.d
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-fluentbit-ucp[5097]: + mkdir -p /etc/systemd/system/fluentbit-ucp.service.d
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-fluentbit-ucp[5097]: + local props
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-fluentbit-ucp[5097]: ++ retry_constant_short systemctl show fluentbit-ucp.service -p Restart,RemainAfterExit
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-fluentbit-ucp[5097]: ++ retry_constant_custom 30 1 systemctl show fluentbit-ucp.service -p Restart,RemainAfterExit
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-fluentbit-ucp[5097]: ++ local -r max_retry_time=30
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-fluentbit-ucp[5097]: ++ local -r retry_delay=1
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-fluentbit-ucp[5097]: ++ cmd=('systemctl' 'show' 'fluentbit-ucp.service' '-p' 'Restart,RemainAfterExit')
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-fluentbit-ucp[5097]: ++ local -r cmd
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-fluentbit-ucp[5097]: ++ local -r max_retries=30
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-fluentbit-ucp[5097]: ++ local reenable_x=false
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-fluentbit-ucp[5097]: ++ [[ -o xtrace ]]
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-fluentbit-ucp[5097]: ++ set +x
<13>Apr 27 18:07:45 dataproc-startup-script[1265]: <13>Apr 27 18:07:45 dataproc-activate-component-fluentbit-ucp[5097]: About to run 'systemctl show fluentbit-ucp.service -p Restart,RemainAfterExit' with retries...
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-setup-hadoop-yarn-resourcemanager[5090]: 'systemctl enable hadoop-yarn-resourcemanager.service' succeeded after 1 execution(s).
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-setup-hadoop-yarn-resourcemanager[5090]: + return 0
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-setup-hadoop-yarn-resourcemanager[5090]: + local -r drop_in_dir=/etc/systemd/system/hadoop-yarn-resourcemanager.service.d
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-setup-hadoop-yarn-resourcemanager[5090]: + mkdir -p /etc/systemd/system/hadoop-yarn-resourcemanager.service.d
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-setup-hadoop-yarn-resourcemanager[5090]: + local props
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-start-osconfig-service[6135]: Created symlink /etc/systemd/system/multi-user.target.wants/google-osconfig-agent.service → /lib/systemd/system/google-osconfig-agent.service.
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-setup-hadoop-yarn-resourcemanager[5090]: ++ retry_constant_short systemctl show hadoop-yarn-resourcemanager.service -p Restart,RemainAfterExit
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-setup-hadoop-yarn-resourcemanager[5090]: ++ retry_constant_custom 30 1 systemctl show hadoop-yarn-resourcemanager.service -p Restart,RemainAfterExit
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-setup-hadoop-yarn-resourcemanager[5090]: ++ local -r max_retry_time=30
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-setup-hadoop-yarn-resourcemanager[5090]: ++ local -r retry_delay=1
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-setup-hadoop-yarn-resourcemanager[5090]: ++ cmd=('systemctl' 'show' 'hadoop-yarn-resourcemanager.service' '-p' 'Restart,RemainAfterExit')
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-setup-hadoop-yarn-resourcemanager[5090]: ++ local -r cmd
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-setup-hadoop-yarn-resourcemanager[5090]: ++ local -r max_retries=30
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-setup-hadoop-yarn-resourcemanager[5090]: ++ local reenable_x=false
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-setup-hadoop-yarn-resourcemanager[5090]: ++ [[ -o xtrace ]]
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-setup-hadoop-yarn-resourcemanager[5090]: ++ set +x
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-setup-hadoop-yarn-resourcemanager[5090]: About to run 'systemctl show hadoop-yarn-resourcemanager.service -p Restart,RemainAfterExit' with retries...
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-setup-hadoop-mapreduce-historyserver[5092]: 'systemctl enable hadoop-mapreduce-historyserver.service' succeeded after 1 execution(s).
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-setup-hadoop-mapreduce-historyserver[5092]: + return 0
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-setup-hadoop-mapreduce-historyserver[5092]: + local -r drop_in_dir=/etc/systemd/system/hadoop-mapreduce-historyserver.service.d
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-setup-hadoop-mapreduce-historyserver[5092]: + mkdir -p /etc/systemd/system/hadoop-mapreduce-historyserver.service.d
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-setup-hadoop-mapreduce-historyserver[5092]: + local props
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-setup-hadoop-mapreduce-historyserver[5092]: ++ retry_constant_short systemctl show hadoop-mapreduce-historyserver.service -p Restart,RemainAfterExit
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-setup-hadoop-mapreduce-historyserver[5092]: ++ retry_constant_custom 30 1 systemctl show hadoop-mapreduce-historyserver.service -p Restart,RemainAfterExit
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-setup-hadoop-mapreduce-historyserver[5092]: ++ local -r max_retry_time=30
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-setup-hadoop-mapreduce-historyserver[5092]: ++ local -r retry_delay=1
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-setup-hadoop-mapreduce-historyserver[5092]: ++ cmd=('systemctl' 'show' 'hadoop-mapreduce-historyserver.service' '-p' 'Restart,RemainAfterExit')
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-setup-hadoop-mapreduce-historyserver[5092]: ++ local -r cmd
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-setup-hadoop-mapreduce-historyserver[5092]: ++ local -r max_retries=30
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-setup-hadoop-mapreduce-historyserver[5092]: ++ local reenable_x=false
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-setup-hadoop-mapreduce-historyserver[5092]: ++ [[ -o xtrace ]]
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-setup-hadoop-mapreduce-historyserver[5092]: ++ set +x
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-setup-hadoop-mapreduce-historyserver[5092]: About to run 'systemctl show hadoop-mapreduce-historyserver.service -p Restart,RemainAfterExit' with retries...
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-activate-component-proxy-agent[5110]: Created symlink /etc/systemd/system/multi-user.target.wants/google-dataproc-component-gateway.service → /lib/systemd/system/google-dataproc-component-gateway.service.
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-start-osconfig-service[6135]: 'systemctl enable google-osconfig-agent.service' succeeded after 1 execution(s).
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-start-osconfig-service[6135]: + return 0
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-start-osconfig-service[6135]: + local -r drop_in_dir=/etc/systemd/system/google-osconfig-agent.service.d
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-start-osconfig-service[6135]: + mkdir -p /etc/systemd/system/google-osconfig-agent.service.d
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-activate-component-otel-ucp[5108]: 'systemctl show otel-metrics-agent.service -p Restart,RemainAfterExit' succeeded after 1 execution(s).
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-activate-component-otel-ucp[5108]: ++ return 0
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-activate-component-otel-ucp[5108]: + props='Restart=on-failure
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-activate-component-otel-ucp[5108]: RemainAfterExit=no'
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-activate-component-otel-ucp[5108]: + [[ otel-metrics-agent != \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-activate-component-otel-ucp[5108]: + [[ otel-metrics-agent != \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-activate-component-otel-ucp[5108]: + [[ Restart=on-failure
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-activate-component-otel-ucp[5108]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-activate-component-otel-ucp[5108]: + local -r gate_start_on_agent_success_drop_in=/etc/systemd/system/common/agent-gate.conf
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-activate-component-otel-ucp[5108]: + [[ ! -f /etc/systemd/system/common/agent-gate.conf ]]
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-activate-component-otel-ucp[5108]: + [[ otel-metrics-agent == \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-activate-component-otel-ucp[5108]: + [[ otel-metrics-agent == \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-activate-component-otel-ucp[5108]: + start_service otel-metrics-agent
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-activate-component-otel-ucp[5108]: + start_services otel-metrics-agent
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-activate-component-otel-ucp[5108]: + units=('otel-metrics-agent.service')
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-activate-component-otel-ucp[5108]: + local -r units
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-activate-component-otel-ucp[5108]: + retry_constant_short systemctl start otel-metrics-agent.service
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-activate-component-otel-ucp[5108]: + retry_constant_custom 30 1 systemctl start otel-metrics-agent.service
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-activate-component-otel-ucp[5108]: + local -r max_retry_time=30
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-activate-component-otel-ucp[5108]: + local -r retry_delay=1
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-activate-component-otel-ucp[5108]: + cmd=('systemctl' 'start' 'otel-metrics-agent.service')
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-activate-component-otel-ucp[5108]: + local -r cmd
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-activate-component-otel-ucp[5108]: + local -r max_retries=30
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-activate-component-otel-ucp[5108]: + local reenable_x=false
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-activate-component-otel-ucp[5108]: + [[ -o xtrace ]]
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-activate-component-otel-ucp[5108]: + set +x
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-activate-component-otel-ucp[5108]: About to run 'systemctl start otel-metrics-agent.service' with retries...
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-start-osconfig-service[6135]: + local props
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-start-osconfig-service[6135]: ++ retry_constant_short systemctl show google-osconfig-agent.service -p Restart,RemainAfterExit
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-start-osconfig-service[6135]: ++ retry_constant_custom 30 1 systemctl show google-osconfig-agent.service -p Restart,RemainAfterExit
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-start-osconfig-service[6135]: ++ local -r max_retry_time=30
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-start-osconfig-service[6135]: ++ local -r retry_delay=1
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-start-osconfig-service[6135]: ++ cmd=('systemctl' 'show' 'google-osconfig-agent.service' '-p' 'Restart,RemainAfterExit')
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-start-osconfig-service[6135]: ++ local -r cmd
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-start-osconfig-service[6135]: ++ local -r max_retries=30
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-start-osconfig-service[6135]: ++ local reenable_x=false
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-start-osconfig-service[6135]: ++ [[ -o xtrace ]]
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-start-osconfig-service[6135]: ++ set +x
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-start-osconfig-service[6135]: About to run 'systemctl show google-osconfig-agent.service -p Restart,RemainAfterExit' with retries...
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: OK
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: + echo 'Adding regional Bigtop repo for us-east1 in APT sources.'
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: Adding regional Bigtop repo for us-east1 in APT sources.
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: + cat
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: + cat /etc/apt/sources.list.d/dataproc.list
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: + mv -f /tmp/dataproc.list /etc/apt/sources.list.d/dataproc.list
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: + [[ standard == \s\t\a\n\d\a\r\d ]]
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: + run_in_background --tag backup-original-configs backup_original_configs
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: + local -r pid=6842
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: + [[ --tag == \-\-\t\a\g ]]
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: + shift 2
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/6842.running ]]
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: + echo backup_original_configs
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: + echo 'Started background process [backup_original_configs] as pid 6842'
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: Started background process [backup_original_configs] as pid 6842
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: + wait_on_async_processes
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: + loginfo 'Waiting on async processes'
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: + echo 'Waiting on async processes'
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: Waiting on async processes
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: + local running_file
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: + for running_file in "${COMMANDS_TMP_DIR}/"*'.running'
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: + local pid
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: ++ basename /tmp/dataproc/commands/4233
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: + trap 'echo "$?" >"${COMMANDS_TMP_DIR}/${BASHPID}.exitcode"' EXIT
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: + run_with_logger --tag backup-original-configs backup_original_configs
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: + local tag=dataproc-script
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: + local pid=6842
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: + [[ --tag == \-\-\t\a\g ]]
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: + tag=dataproc-backup-original-configs
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: + shift 2
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: + [[ 1 -eq 0 ]]
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: + backup_original_configs
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: ++ logger -s -t 'dataproc-backup-original-configs[6842]'
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: + pid=4233
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: + local cmd
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: + cmd=create_event_log_dir
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: + loginfo 'Waiting on pid=4233 cmd=[create_event_log_dir]'
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: + echo 'Waiting on pid=4233 cmd=[create_event_log_dir]'
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: Waiting on pid=4233 cmd=[create_event_log_dir]
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: + echo create_event_log_dir
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: + local exitcode_file=/tmp/dataproc/commands/4233.exitcode
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/4233.exitcode ]]
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: + local status
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: + status=0
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: + (( status != 0 ))
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: + tee /tmp/dataproc/commands/4233.done
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: + echo 'Command cmd=[create_event_log_dir] pid=4233 exited with 0'
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-backup-original-configs[6842]: + local -r HIVE_CONF_DIR=/etc/hive/conf
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-backup-original-configs[6842]: + local -r TEZ_CONF_DIR=/etc/tez/conf
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-backup-original-configs[6842]: + mkdir -p /usr/local/share/google/dataproc/conf/original
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-backup-original-configs[6842]: + cp /etc/hadoop/conf/yarn-site.xml /usr/local/share/google/dataproc/conf/original/original-yarn-site.xml
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-backup-original-configs[6842]: + cp /etc/hadoop/conf/hdfs-site.xml /usr/local/share/google/dataproc/conf/original/original-hdfs-site.xml
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: Command cmd=[create_event_log_dir] pid=4233 exited with 0
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: + rm /tmp/dataproc/commands/4233.exitcode /tmp/dataproc/commands/4233.running
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: + for running_file in "${COMMANDS_TMP_DIR}/"*'.running'
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: + local pid
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: ++ basename /tmp/dataproc/commands/5090
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: + pid=5090
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: + local cmd
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: + cmd='setup_service hadoop-yarn-resourcemanager'
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: + loginfo 'Waiting on pid=5090 cmd=[setup_service hadoop-yarn-resourcemanager]'
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: + echo 'Waiting on pid=5090 cmd=[setup_service hadoop-yarn-resourcemanager]'
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: Waiting on pid=5090 cmd=[setup_service hadoop-yarn-resourcemanager]
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: + echo 'setup_service hadoop-yarn-resourcemanager'
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: + local exitcode_file=/tmp/dataproc/commands/5090.exitcode
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5090.exitcode ]]
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-backup-original-configs[6842]: + cp /etc/hadoop/conf/core-site.xml /usr/local/share/google/dataproc/conf/original/original-core-site.xml
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-backup-original-configs[6842]: + cp /etc/hadoop/conf/mapred-site.xml /usr/local/share/google/dataproc/conf/original/original-mapred-site.xml
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-backup-original-configs[6842]: + cp /etc/hadoop/conf/capacity-scheduler.xml /usr/local/share/google/dataproc/conf/original/original-capacity-scheduler.xml
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-backup-original-configs[6842]: + cp /etc/hive/conf/hive-site.xml /usr/local/share/google/dataproc/conf/original/original-hive-site.xml
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-backup-original-configs[6842]: + cp /etc/tez/conf/tez-site.xml /usr/local/share/google/dataproc/conf/original/original-tez-site.xml
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-backup-original-configs[6842]: + cp /etc/spark/conf/spark-defaults.conf /usr/local/share/google/dataproc/conf/original/original-spark-defaults.conf
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: ++ echo 0
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-activate-component-proxy-agent[5110]: 'systemctl enable google-dataproc-component-gateway.service' succeeded after 1 execution(s).
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-activate-component-proxy-agent[5110]: + return 0
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-activate-component-proxy-agent[5110]: + local -r drop_in_dir=/etc/systemd/system/google-dataproc-component-gateway.service.d
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-activate-component-proxy-agent[5110]: + mkdir -p /etc/systemd/system/google-dataproc-component-gateway.service.d
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-activate-component-npd[5107]: Created symlink /etc/systemd/system/multi-user.target.wants/npd.service → /etc/systemd/system/npd.service.
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-activate-component-hive-metastore[5099]: Updating hive-pasword on all master node
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-activate-component-proxy-agent[5110]: + local props
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-activate-component-proxy-agent[5110]: ++ retry_constant_short systemctl show google-dataproc-component-gateway.service -p Restart,RemainAfterExit
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-activate-component-proxy-agent[5110]: ++ retry_constant_custom 30 1 systemctl show google-dataproc-component-gateway.service -p Restart,RemainAfterExit
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-activate-component-proxy-agent[5110]: ++ local -r max_retry_time=30
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-activate-component-proxy-agent[5110]: ++ local -r retry_delay=1
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-activate-component-proxy-agent[5110]: ++ cmd=('systemctl' 'show' 'google-dataproc-component-gateway.service' '-p' 'Restart,RemainAfterExit')
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-activate-component-proxy-agent[5110]: ++ local -r cmd
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-activate-component-proxy-agent[5110]: ++ local -r max_retries=30
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-activate-component-proxy-agent[5110]: ++ local reenable_x=false
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-activate-component-proxy-agent[5110]: ++ [[ -o xtrace ]]
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-activate-component-proxy-agent[5110]: ++ set +x
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-activate-component-proxy-agent[5110]: About to run 'systemctl show google-dataproc-component-gateway.service -p Restart,RemainAfterExit' with retries...
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-activate-component-hive-metastore[5099]: mysql: [Warning] Using a password on the command line interface can be insecure.
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-setup-hadoop-yarn-timelineserver[5093]: 'systemctl enable hadoop-yarn-timelineserver.service' succeeded after 1 execution(s).
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-setup-hadoop-yarn-timelineserver[5093]: + return 0
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-setup-hadoop-yarn-timelineserver[5093]: + local -r drop_in_dir=/etc/systemd/system/hadoop-yarn-timelineserver.service.d
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-setup-hadoop-yarn-timelineserver[5093]: + mkdir -p /etc/systemd/system/hadoop-yarn-timelineserver.service.d
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-setup-hadoop-yarn-timelineserver[5093]: + local props
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-setup-hadoop-yarn-timelineserver[5093]: ++ retry_constant_short systemctl show hadoop-yarn-timelineserver.service -p Restart,RemainAfterExit
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-setup-hadoop-yarn-timelineserver[5093]: ++ retry_constant_custom 30 1 systemctl show hadoop-yarn-timelineserver.service -p Restart,RemainAfterExit
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-setup-hadoop-yarn-timelineserver[5093]: ++ local -r max_retry_time=30
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-setup-hadoop-yarn-timelineserver[5093]: ++ local -r retry_delay=1
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-setup-hadoop-yarn-timelineserver[5093]: ++ cmd=('systemctl' 'show' 'hadoop-yarn-timelineserver.service' '-p' 'Restart,RemainAfterExit')
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-setup-hadoop-yarn-timelineserver[5093]: ++ local -r cmd
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-setup-hadoop-yarn-timelineserver[5093]: ++ local -r max_retries=30
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-setup-hadoop-yarn-timelineserver[5093]: ++ local reenable_x=false
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-setup-hadoop-yarn-timelineserver[5093]: ++ [[ -o xtrace ]]
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-setup-hadoop-yarn-timelineserver[5093]: ++ set +x
<13>Apr 27 18:07:46 dataproc-startup-script[1265]: <13>Apr 27 18:07:46 dataproc-setup-hadoop-yarn-timelineserver[5093]: About to run 'systemctl show hadoop-yarn-timelineserver.service -p Restart,RemainAfterExit' with retries...
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-hive-metastore[5099]: Files to remove if all 3 master nodes have been updated
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-npd[5107]: 'systemctl enable npd.service' succeeded after 1 execution(s).
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-npd[5107]: + return 0
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-npd[5107]: + local -r drop_in_dir=/etc/systemd/system/npd.service.d
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-npd[5107]: + mkdir -p /etc/systemd/system/npd.service.d
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-fluentbit-ucp[5097]: 'systemctl show fluentbit-ucp.service -p Restart,RemainAfterExit' succeeded after 1 execution(s).
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-fluentbit-ucp[5097]: ++ return 0
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-fluentbit-ucp[5097]: + props='Restart=on-failure
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-fluentbit-ucp[5097]: RemainAfterExit=no'
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-fluentbit-ucp[5097]: + [[ fluentbit-ucp != \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-fluentbit-ucp[5097]: + [[ fluentbit-ucp != \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-fluentbit-ucp[5097]: + [[ Restart=on-failure
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-fluentbit-ucp[5097]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-fluentbit-ucp[5097]: + local -r gate_start_on_agent_success_drop_in=/etc/systemd/system/common/agent-gate.conf
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-fluentbit-ucp[5097]: + [[ ! -f /etc/systemd/system/common/agent-gate.conf ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-fluentbit-ucp[5097]: + [[ fluentbit-ucp == \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-fluentbit-ucp[5097]: + [[ fluentbit-ucp == \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-fluentbit-ucp[5097]: + start_service fluentbit-ucp
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-fluentbit-ucp[5097]: + start_services fluentbit-ucp
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-fluentbit-ucp[5097]: + units=('fluentbit-ucp.service')
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-fluentbit-ucp[5097]: + local -r units
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-fluentbit-ucp[5097]: + retry_constant_short systemctl start fluentbit-ucp.service
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-fluentbit-ucp[5097]: + retry_constant_custom 30 1 systemctl start fluentbit-ucp.service
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-fluentbit-ucp[5097]: + local -r max_retry_time=30
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-fluentbit-ucp[5097]: + local -r retry_delay=1
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-fluentbit-ucp[5097]: + cmd=('systemctl' 'start' 'fluentbit-ucp.service')
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-fluentbit-ucp[5097]: + local -r cmd
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-fluentbit-ucp[5097]: + local -r max_retries=30
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-fluentbit-ucp[5097]: + local reenable_x=false
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-fluentbit-ucp[5097]: + [[ -o xtrace ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-fluentbit-ucp[5097]: + set +x
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-fluentbit-ucp[5097]: About to run 'systemctl start fluentbit-ucp.service' with retries...
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-resourcemanager[5090]: 'systemctl show hadoop-yarn-resourcemanager.service -p Restart,RemainAfterExit' succeeded after 1 execution(s).
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-resourcemanager[5090]: ++ return 0
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-resourcemanager[5090]: + props='Restart=no
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-resourcemanager[5090]: RemainAfterExit=no'
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-resourcemanager[5090]: + [[ hadoop-yarn-resourcemanager != \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-resourcemanager[5090]: + [[ hadoop-yarn-resourcemanager != \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-resourcemanager[5090]: + [[ Restart=no
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-resourcemanager[5090]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-resourcemanager[5090]: + [[ Restart=no
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-resourcemanager[5090]: RemainAfterExit=no == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-resourcemanager[5090]: + ln -s -f /etc/systemd/system/common/restart.conf /etc/systemd/system/hadoop-yarn-resourcemanager.service.d
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-mapreduce-historyserver[5092]: 'systemctl show hadoop-mapreduce-historyserver.service -p Restart,RemainAfterExit' succeeded after 1 execution(s).
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-mapreduce-historyserver[5092]: ++ return 0
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-mapreduce-historyserver[5092]: + props='Restart=no
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-mapreduce-historyserver[5092]: RemainAfterExit=no'
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-mapreduce-historyserver[5092]: + [[ hadoop-mapreduce-historyserver != \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-mapreduce-historyserver[5092]: + [[ hadoop-mapreduce-historyserver != \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-mapreduce-historyserver[5092]: + [[ Restart=no
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-mapreduce-historyserver[5092]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-mapreduce-historyserver[5092]: + [[ Restart=no
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-mapreduce-historyserver[5092]: RemainAfterExit=no == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-mapreduce-historyserver[5092]: + ln -s -f /etc/systemd/system/common/restart.conf /etc/systemd/system/hadoop-mapreduce-historyserver.service.d
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-proxy-agent[5110]: 'systemctl show google-dataproc-component-gateway.service -p Restart,RemainAfterExit' succeeded after 1 execution(s).
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-proxy-agent[5110]: ++ return 0
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-start-osconfig-service[6135]: 'systemctl show google-osconfig-agent.service -p Restart,RemainAfterExit' succeeded after 1 execution(s).
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-proxy-agent[5110]: + props='Restart=on-failure
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-npd[5107]: + local props
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-proxy-agent[5110]: RemainAfterExit=no'
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-proxy-agent[5110]: + [[ google-dataproc-component-gateway != \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-proxy-agent[5110]: + [[ google-dataproc-component-gateway != \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-proxy-agent[5110]: + [[ Restart=on-failure
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-proxy-agent[5110]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-proxy-agent[5110]: + local -r gate_start_on_agent_success_drop_in=/etc/systemd/system/common/agent-gate.conf
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-proxy-agent[5110]: + [[ ! -f /etc/systemd/system/common/agent-gate.conf ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-proxy-agent[5110]: + [[ google-dataproc-component-gateway == \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-start-osconfig-service[6135]: ++ return 0
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-start-osconfig-service[6135]: + props='Restart=always
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-start-osconfig-service[6135]: RemainAfterExit=no'
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-start-osconfig-service[6135]: + [[ google-osconfig-agent != \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-start-osconfig-service[6135]: + [[ google-osconfig-agent != \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-start-osconfig-service[6135]: + [[ Restart=always
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-start-osconfig-service[6135]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-start-osconfig-service[6135]: + local -r gate_start_on_agent_success_drop_in=/etc/systemd/system/common/agent-gate.conf
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-start-osconfig-service[6135]: + [[ ! -f /etc/systemd/system/common/agent-gate.conf ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-start-osconfig-service[6135]: + [[ google-osconfig-agent == \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-start-osconfig-service[6135]: + [[ google-osconfig-agent == \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-start-osconfig-service[6135]: + start_service google-osconfig-agent
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-start-osconfig-service[6135]: + start_services google-osconfig-agent
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-start-osconfig-service[6135]: + units=('google-osconfig-agent.service')
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-start-osconfig-service[6135]: + local -r units
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-start-osconfig-service[6135]: + retry_constant_short systemctl start google-osconfig-agent.service
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-start-osconfig-service[6135]: + retry_constant_custom 30 1 systemctl start google-osconfig-agent.service
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-start-osconfig-service[6135]: + local -r max_retry_time=30
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-proxy-agent[5110]: + [[ google-dataproc-component-gateway == \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-proxy-agent[5110]: + [[    == *\ \k\n\o\x\.\s\e\r\v\i\c\e\ * ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-proxy-agent[5110]: + start_service google-dataproc-component-gateway
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-proxy-agent[5110]: + start_services google-dataproc-component-gateway
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-proxy-agent[5110]: + units=('google-dataproc-component-gateway.service')
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-proxy-agent[5110]: + local -r units
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-proxy-agent[5110]: + retry_constant_short systemctl start google-dataproc-component-gateway.service
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-proxy-agent[5110]: + retry_constant_custom 30 1 systemctl start google-dataproc-component-gateway.service
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-proxy-agent[5110]: + local -r max_retry_time=30
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-proxy-agent[5110]: + local -r retry_delay=1
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-proxy-agent[5110]: + cmd=('systemctl' 'start' 'google-dataproc-component-gateway.service')
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-proxy-agent[5110]: + local -r cmd
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-proxy-agent[5110]: + local -r max_retries=30
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-proxy-agent[5110]: + local reenable_x=false
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-proxy-agent[5110]: + [[ -o xtrace ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-proxy-agent[5110]: + set +x
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-proxy-agent[5110]: About to run 'systemctl start google-dataproc-component-gateway.service' with retries...
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-mapreduce-historyserver[5092]: + local -r gate_start_on_agent_success_drop_in=/etc/systemd/system/common/agent-gate.conf
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-mapreduce-historyserver[5092]: + [[ ! -f /etc/systemd/system/common/agent-gate.conf ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-mapreduce-historyserver[5092]: + [[ hadoop-mapreduce-historyserver == \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-mapreduce-historyserver[5092]: + [[ hadoop-mapreduce-historyserver == \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-mapreduce-historyserver[5092]: + local am_on_primary_worker_enabled
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-mapreduce-historyserver[5092]: ++ get_dataproc_property am.primary_only
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-mapreduce-historyserver[5092]: ++ set +x
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-start-osconfig-service[6135]: + local -r retry_delay=1
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-start-osconfig-service[6135]: + cmd=('systemctl' 'start' 'google-osconfig-agent.service')
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-start-osconfig-service[6135]: + local -r cmd
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-start-osconfig-service[6135]: + local -r max_retries=30
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-start-osconfig-service[6135]: + local reenable_x=false
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-start-osconfig-service[6135]: + [[ -o xtrace ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-start-osconfig-service[6135]: + set +x
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-start-osconfig-service[6135]: About to run 'systemctl start google-osconfig-agent.service' with retries...
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-timelineserver[5093]: 'systemctl show hadoop-yarn-timelineserver.service -p Restart,RemainAfterExit' succeeded after 1 execution(s).
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-timelineserver[5093]: ++ return 0
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-timelineserver[5093]: + props='Restart=no
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-timelineserver[5093]: RemainAfterExit=no'
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-timelineserver[5093]: + [[ hadoop-yarn-timelineserver != \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-timelineserver[5093]: + [[ hadoop-yarn-timelineserver != \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-timelineserver[5093]: + [[ Restart=no
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-timelineserver[5093]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-timelineserver[5093]: + [[ Restart=no
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-timelineserver[5093]: RemainAfterExit=no == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-timelineserver[5093]: + ln -s -f /etc/systemd/system/common/restart.conf /etc/systemd/system/hadoop-yarn-timelineserver.service.d
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-timelineserver[5093]: + local -r gate_start_on_agent_success_drop_in=/etc/systemd/system/common/agent-gate.conf
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-timelineserver[5093]: + [[ ! -f /etc/systemd/system/common/agent-gate.conf ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-timelineserver[5093]: + [[ hadoop-yarn-timelineserver == \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-timelineserver[5093]: + [[ hadoop-yarn-timelineserver == \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-timelineserver[5093]: + local am_on_primary_worker_enabled
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-timelineserver[5093]: ++ get_dataproc_property am.primary_only
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-timelineserver[5093]: ++ set +x
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-timelineserver[5093]: + am_on_primary_worker_enabled=false
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-timelineserver[5093]: + [[ hadoop-yarn-timelineserver == \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-timelineserver[5093]: + local master_run_driver_location
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-resourcemanager[5090]: + local -r gate_start_on_agent_success_drop_in=/etc/systemd/system/common/agent-gate.conf
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-resourcemanager[5090]: + [[ ! -f /etc/systemd/system/common/agent-gate.conf ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-resourcemanager[5090]: + [[ hadoop-yarn-resourcemanager == \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-resourcemanager[5090]: + [[ hadoop-yarn-resourcemanager == \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-resourcemanager[5090]: + local am_on_primary_worker_enabled
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-resourcemanager[5090]: ++ get_dataproc_property am.primary_only
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-resourcemanager[5090]: ++ set +x
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-npd[5107]: ++ retry_constant_short systemctl show npd.service -p Restart,RemainAfterExit
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-npd[5107]: ++ retry_constant_custom 30 1 systemctl show npd.service -p Restart,RemainAfterExit
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-npd[5107]: ++ local -r max_retry_time=30
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-npd[5107]: ++ local -r retry_delay=1
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-npd[5107]: ++ cmd=('systemctl' 'show' 'npd.service' '-p' 'Restart,RemainAfterExit')
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-npd[5107]: ++ local -r cmd
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-npd[5107]: ++ local -r max_retries=30
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-npd[5107]: ++ local reenable_x=false
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-npd[5107]: ++ [[ -o xtrace ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-npd[5107]: ++ set +x
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-npd[5107]: About to run 'systemctl show npd.service -p Restart,RemainAfterExit' with retries...
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-otel-ucp[5108]: 'systemctl start otel-metrics-agent.service' succeeded after 1 execution(s).
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-otel-ucp[5108]: + return 0
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-earlyoom[5096]: 'systemctl start earlyoom.service' succeeded after 1 execution(s).
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-earlyoom[5096]: + return 0
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-timelineserver[5093]: ++ /usr/share/google/get_metadata_value attributes/master-run-driver-location
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: ++ echo 0
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-start-osconfig-service[6135]: 'systemctl start google-osconfig-agent.service' succeeded after 1 execution(s).
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-start-osconfig-service[6135]: + return 0
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-otel-ucp[5108]: ++ date +%s.%N
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-proxy-agent[5110]: 'systemctl start google-dataproc-component-gateway.service' succeeded after 1 execution(s).
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-earlyoom[5096]: ++ date +%s.%N
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-fluentbit-ucp[5097]: 'systemctl start fluentbit-ucp.service' succeeded after 1 execution(s).
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-fluentbit-ucp[5097]: + return 0
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-proxy-agent[5110]: + return 0
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-mapreduce-historyserver[5092]: + am_on_primary_worker_enabled=false
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-mapreduce-historyserver[5092]: + [[ hadoop-mapreduce-historyserver == \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-mapreduce-historyserver[5092]: + local master_run_driver_location
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-fluentbit-ucp[5097]: ++ date +%s.%N
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-otel-ucp[5108]: + local -r end=1745777267.254872530
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-otel-ucp[5108]: + local -r runtime_s=4
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-otel-ucp[5108]: + echo 'Component otel-ucp took 4s to activate'
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-otel-ucp[5108]: Component otel-ucp took 4s to activate
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-otel-ucp[5108]: + local -r time_file=/tmp/dataproc/components/activate/otel-ucp.time
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-otel-ucp[5108]: + touch /tmp/dataproc/components/activate/otel-ucp.time
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-earlyoom[5096]: + local -r end=1745777267.253823257
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-earlyoom[5096]: + local -r runtime_s=4
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-earlyoom[5096]: + echo 'Component earlyoom took 4s to activate'
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-earlyoom[5096]: Component earlyoom took 4s to activate
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-earlyoom[5096]: + local -r time_file=/tmp/dataproc/components/activate/earlyoom.time
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-earlyoom[5096]: + touch /tmp/dataproc/components/activate/earlyoom.time
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-mapreduce-historyserver[5092]: ++ /usr/share/google/get_metadata_value attributes/master-run-driver-location
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-proxy-agent[5110]: ++ date +%s.%N
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-fluentbit-ucp[5097]: + local -r end=1745777267.265966075
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-fluentbit-ucp[5097]: + local -r runtime_s=4
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-fluentbit-ucp[5097]: + echo 'Component fluentbit-ucp took 4s to activate'
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-fluentbit-ucp[5097]: Component fluentbit-ucp took 4s to activate
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-fluentbit-ucp[5097]: + local -r time_file=/tmp/dataproc/components/activate/fluentbit-ucp.time
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-fluentbit-ucp[5097]: + touch /tmp/dataproc/components/activate/fluentbit-ucp.time
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-earlyoom[5096]: + cat
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-otel-ucp[5108]: + cat
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-resourcemanager[5090]: + am_on_primary_worker_enabled=false
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-resourcemanager[5090]: + [[ hadoop-yarn-resourcemanager == \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-resourcemanager[5090]: + [[ false == \t\r\u\e ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-resourcemanager[5090]: + local master_run_driver_location
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-fluentbit-ucp[5097]: + cat
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-proxy-agent[5110]: + local -r end=1745777267.284748189
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-proxy-agent[5110]: + local -r runtime_s=4
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-proxy-agent[5110]: + echo 'Component proxy-agent took 4s to activate'
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-proxy-agent[5110]: Component proxy-agent took 4s to activate
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-proxy-agent[5110]: + local -r time_file=/tmp/dataproc/components/activate/proxy-agent.time
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-proxy-agent[5110]: + touch /tmp/dataproc/components/activate/proxy-agent.time
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-earlyoom[5096]: + [[ 0 -ne 0 ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-earlyoom[5096]: + echo 'Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/earlyoom.sh'
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-earlyoom[5096]: Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/earlyoom.sh
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-earlyoom[5096]: + touch /tmp/dataproc/components/activate/earlyoom.done
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-resourcemanager[5090]: ++ /usr/share/google/get_metadata_value attributes/master-run-driver-location
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-proxy-agent[5110]: + cat
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-otel-ucp[5108]: + [[ 0 -ne 0 ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-otel-ucp[5108]: + echo 'Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/otel-ucp.sh'
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-otel-ucp[5108]: Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/otel-ucp.sh
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-otel-ucp[5108]: + touch /tmp/dataproc/components/activate/otel-ucp.done
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-npd[5107]: 'systemctl show npd.service -p Restart,RemainAfterExit' succeeded after 1 execution(s).
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-npd[5107]: ++ return 0
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-npd[5107]: + props='Restart=on-failure
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-npd[5107]: RemainAfterExit=no'
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-npd[5107]: + [[ npd != \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-npd[5107]: + [[ npd != \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-npd[5107]: + [[ Restart=on-failure
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-npd[5107]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-npd[5107]: + local -r gate_start_on_agent_success_drop_in=/etc/systemd/system/common/agent-gate.conf
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-npd[5107]: + [[ ! -f /etc/systemd/system/common/agent-gate.conf ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-npd[5107]: + [[ npd == \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-npd[5107]: + [[ npd == \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-npd[5107]: + start_service npd
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-npd[5107]: + start_services npd
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-npd[5107]: + units=('npd.service')
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-npd[5107]: + local -r units
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-npd[5107]: + retry_constant_short systemctl start npd.service
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-npd[5107]: + retry_constant_custom 30 1 systemctl start npd.service
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-npd[5107]: + local -r max_retry_time=30
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-npd[5107]: + local -r retry_delay=1
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-npd[5107]: + cmd=('systemctl' 'start' 'npd.service')
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-npd[5107]: + local -r cmd
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-npd[5107]: + local -r max_retries=30
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-npd[5107]: + local reenable_x=false
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-npd[5107]: + [[ -o xtrace ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-npd[5107]: + set +x
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-npd[5107]: About to run 'systemctl start npd.service' with retries...
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-fluentbit-ucp[5097]: + [[ 0 -ne 0 ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-fluentbit-ucp[5097]: + echo 'Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/fluentbit-ucp.sh'
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-fluentbit-ucp[5097]: Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/fluentbit-ucp.sh
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-fluentbit-ucp[5097]: + touch /tmp/dataproc/components/activate/fluentbit-ucp.done
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: ++ echo 0
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: ++ echo 0
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-proxy-agent[5110]: + [[ 0 -ne 0 ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-proxy-agent[5110]: + echo 'Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/proxy-agent.sh'
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-proxy-agent[5110]: Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/proxy-agent.sh
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-proxy-agent[5110]: + touch /tmp/dataproc/components/activate/proxy-agent.done
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: ++ echo 0
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: ++ echo 0
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-timelineserver[5093]: + master_run_driver_location=LOCAL
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-timelineserver[5093]: + [[ hadoop-yarn-timelineserver == \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-timelineserver[5093]: + start_service hadoop-yarn-timelineserver
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-timelineserver[5093]: + start_services hadoop-yarn-timelineserver
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-timelineserver[5093]: + units=('hadoop-yarn-timelineserver.service')
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-timelineserver[5093]: + local -r units
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-timelineserver[5093]: + retry_constant_short systemctl start hadoop-yarn-timelineserver.service
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-timelineserver[5093]: + retry_constant_custom 30 1 systemctl start hadoop-yarn-timelineserver.service
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-timelineserver[5093]: + local -r max_retry_time=30
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-timelineserver[5093]: + local -r retry_delay=1
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-timelineserver[5093]: + cmd=('systemctl' 'start' 'hadoop-yarn-timelineserver.service')
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-timelineserver[5093]: + local -r cmd
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-timelineserver[5093]: + local -r max_retries=30
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-timelineserver[5093]: + local reenable_x=false
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-timelineserver[5093]: + [[ -o xtrace ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-timelineserver[5093]: + set +x
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-timelineserver[5093]: About to run 'systemctl start hadoop-yarn-timelineserver.service' with retries...
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-npd[5107]: 'systemctl start npd.service' succeeded after 1 execution(s).
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-npd[5107]: + return 0
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-npd[5107]: ++ date +%s.%N
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-npd[5107]: + local -r end=1745777267.456565727
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-npd[5107]: + local -r runtime_s=4
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-npd[5107]: + echo 'Component npd took 4s to activate'
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-npd[5107]: Component npd took 4s to activate
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-npd[5107]: + local -r time_file=/tmp/dataproc/components/activate/npd.time
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-npd[5107]: + touch /tmp/dataproc/components/activate/npd.time
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-timelineserver[5093]: Warning: The unit file, source configuration file or drop-ins of hadoop-yarn-timelineserver.service changed on disk. Run 'systemctl daemon-reload' to reload units.
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-npd[5107]: + cat
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-hdfs[5098]: WARNING: HADOOP_NAMENODE_OPTS has been replaced by HDFS_NAMENODE_OPTS. Using value of HADOOP_NAMENODE_OPTS.
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-hdfs[5098]: OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-npd[5107]: + [[ 0 -ne 0 ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-npd[5107]: + echo 'Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/npd.sh'
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-npd[5107]: Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/npd.sh
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-npd[5107]: + touch /tmp/dataproc/components/activate/npd.done
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:07:47.509+0000][info][gc] Using Concurrent Mark Sweep
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:07:47.509+0000][info][gc,heap,coops] Heap address: 0x000000009ca00000, size: 1590 MB, Compressed Oops mode: 32-bit
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: ++ echo 0
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-mapreduce-historyserver[5092]: + master_run_driver_location=LOCAL
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-mapreduce-historyserver[5092]: + [[ hadoop-mapreduce-historyserver == \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-mapreduce-historyserver[5092]: + start_service hadoop-mapreduce-historyserver
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-mapreduce-historyserver[5092]: + start_services hadoop-mapreduce-historyserver
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-mapreduce-historyserver[5092]: + units=('hadoop-mapreduce-historyserver.service')
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-mapreduce-historyserver[5092]: + local -r units
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-mapreduce-historyserver[5092]: + retry_constant_short systemctl start hadoop-mapreduce-historyserver.service
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-mapreduce-historyserver[5092]: + retry_constant_custom 30 1 systemctl start hadoop-mapreduce-historyserver.service
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-mapreduce-historyserver[5092]: + local -r max_retry_time=30
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-mapreduce-historyserver[5092]: + local -r retry_delay=1
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-mapreduce-historyserver[5092]: + cmd=('systemctl' 'start' 'hadoop-mapreduce-historyserver.service')
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-mapreduce-historyserver[5092]: + local -r cmd
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-mapreduce-historyserver[5092]: + local -r max_retries=30
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-mapreduce-historyserver[5092]: + local reenable_x=false
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-mapreduce-historyserver[5092]: + [[ -o xtrace ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-mapreduce-historyserver[5092]: + set +x
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-mapreduce-historyserver[5092]: About to run 'systemctl start hadoop-mapreduce-historyserver.service' with retries...
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5090.exitcode ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-resourcemanager[5090]: + master_run_driver_location=LOCAL
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-resourcemanager[5090]: + [[ hadoop-yarn-resourcemanager == \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-resourcemanager[5090]: + [[ LOCAL == \Y\A\R\N ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-resourcemanager[5090]: + start_service hadoop-yarn-resourcemanager
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-resourcemanager[5090]: + start_services hadoop-yarn-resourcemanager
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-resourcemanager[5090]: + units=('hadoop-yarn-resourcemanager.service')
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-resourcemanager[5090]: + local -r units
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-resourcemanager[5090]: + retry_constant_short systemctl start hadoop-yarn-resourcemanager.service
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-resourcemanager[5090]: + retry_constant_custom 30 1 systemctl start hadoop-yarn-resourcemanager.service
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-resourcemanager[5090]: + local -r max_retry_time=30
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-resourcemanager[5090]: + local -r retry_delay=1
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-resourcemanager[5090]: + cmd=('systemctl' 'start' 'hadoop-yarn-resourcemanager.service')
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-resourcemanager[5090]: + local -r cmd
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-resourcemanager[5090]: + local -r max_retries=30
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-resourcemanager[5090]: + local reenable_x=false
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-resourcemanager[5090]: + [[ -o xtrace ]]
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-resourcemanager[5090]: + set +x
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-resourcemanager[5090]: About to run 'systemctl start hadoop-yarn-resourcemanager.service' with retries...
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-mapreduce-historyserver[5092]: Warning: The unit file, source configuration file or drop-ins of hadoop-mapreduce-historyserver.service changed on disk. Run 'systemctl daemon-reload' to reload units.
<13>Apr 27 18:07:47 dataproc-startup-script[1265]: <13>Apr 27 18:07:47 dataproc-setup-hadoop-yarn-resourcemanager[5090]: Warning: The unit file, source configuration file or drop-ins of hadoop-yarn-resourcemanager.service changed on disk. Run 'systemctl daemon-reload' to reload units.
<13>Apr 27 18:07:48 dataproc-startup-script[1265]: <13>Apr 27 18:07:48 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:07:48.372+0000][info][gc,start     ] GC(0) Pause Young (Allocation Failure)
<13>Apr 27 18:07:48 dataproc-startup-script[1265]: <13>Apr 27 18:07:48 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:07:48.378+0000][info][gc,task      ] GC(0) Using 2 workers of 2 for evacuation
<13>Apr 27 18:07:48 dataproc-startup-script[1265]: <13>Apr 27 18:07:48 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:07:48.432+0000][info][gc,heap      ] GC(0) ParNew: 31997K->4287K(38720K)
<13>Apr 27 18:07:48 dataproc-startup-script[1265]: <13>Apr 27 18:07:48 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:07:48.432+0000][info][gc,heap      ] GC(0) CMS: 0K->24676K(86016K)
<13>Apr 27 18:07:48 dataproc-startup-script[1265]: <13>Apr 27 18:07:48 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:07:48.432+0000][info][gc,metaspace ] GC(0) Metaspace: 4838K(5120K)->4838K(5120K) NonClass: 4446K(4608K)->4446K(4608K) Class: 392K(512K)->392K(512K)
<13>Apr 27 18:07:48 dataproc-startup-script[1265]: <13>Apr 27 18:07:48 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:07:48.432+0000][info][gc           ] GC(0) Pause Young (Allocation Failure) 31M->28M(121M) 60.069ms
<13>Apr 27 18:07:48 dataproc-startup-script[1265]: <13>Apr 27 18:07:48 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:07:48.432+0000][info][gc,cpu       ] GC(0) User=0.03s Sys=0.01s Real=0.06s
<13>Apr 27 18:07:48 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5090.exitcode ]]
<13>Apr 27 18:07:48 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:07:49 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5090.exitcode ]]
<13>Apr 27 18:07:49 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:07:50 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5090.exitcode ]]
<13>Apr 27 18:07:50 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:07:51 dataproc-startup-script[1265]: <13>Apr 27 18:07:51 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:07:51.077+0000][info][gc,start     ] GC(1) Pause Young (Allocation Failure)
<13>Apr 27 18:07:51 dataproc-startup-script[1265]: <13>Apr 27 18:07:51 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:07:51.077+0000][info][gc,task      ] GC(1) Using 2 workers of 2 for evacuation
<13>Apr 27 18:07:51 dataproc-startup-script[1265]: <13>Apr 27 18:07:51 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:07:51.150+0000][info][gc,heap      ] GC(1) ParNew: 38719K->4288K(38720K)
<13>Apr 27 18:07:51 dataproc-startup-script[1265]: <13>Apr 27 18:07:51 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:07:51.150+0000][info][gc,heap      ] GC(1) CMS: 24676K->41083K(86016K)
<13>Apr 27 18:07:51 dataproc-startup-script[1265]: <13>Apr 27 18:07:51 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:07:51.150+0000][info][gc,metaspace ] GC(1) Metaspace: 9793K(10112K)->9793K(10112K) NonClass: 8945K(9216K)->8945K(9216K) Class: 847K(896K)->847K(896K)
<13>Apr 27 18:07:51 dataproc-startup-script[1265]: <13>Apr 27 18:07:51 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:07:51.150+0000][info][gc           ] GC(1) Pause Young (Allocation Failure) 61M->44M(121M) 72.730ms
<13>Apr 27 18:07:51 dataproc-startup-script[1265]: <13>Apr 27 18:07:51 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:07:51.150+0000][info][gc,cpu       ] GC(1) User=0.01s Sys=0.01s Real=0.07s
<13>Apr 27 18:07:51 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5090.exitcode ]]
<13>Apr 27 18:07:51 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:07:52 dataproc-startup-script[1265]: <13>Apr 27 18:07:52 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:07:52,215 INFO namenode.NameNode: STARTUP_MSG: 
<13>Apr 27 18:07:52 dataproc-startup-script[1265]: <13>Apr 27 18:07:52 dataproc-activate-component-hdfs[5098]: /************************************************************
<13>Apr 27 18:07:52 dataproc-startup-script[1265]: <13>Apr 27 18:07:52 dataproc-activate-component-hdfs[5098]: STARTUP_MSG: Starting NameNode
<13>Apr 27 18:07:52 dataproc-startup-script[1265]: <13>Apr 27 18:07:52 dataproc-activate-component-hdfs[5098]: STARTUP_MSG:   host = cluster-dip-01-m/10.142.0.8
<13>Apr 27 18:07:52 dataproc-startup-script[1265]: <13>Apr 27 18:07:52 dataproc-activate-component-hdfs[5098]: STARTUP_MSG:   args = [-format, -nonInteractive]
<13>Apr 27 18:07:52 dataproc-startup-script[1265]: <13>Apr 27 18:07:52 dataproc-activate-component-hdfs[5098]: STARTUP_MSG:   version = 3.3.6
<13>Apr 27 18:07:52 dataproc-startup-script[1265]: <13>Apr 27 18:07:52 dataproc-activate-component-hdfs[5098]: STARTUP_MSG:   classpath = /etc/hadoop/conf:/usr/lib/hadoop/lib/token-provider-2.0.3.jar:/usr/lib/hadoop/lib/kafka-clients-2.8.2.jar:/usr/lib/hadoop/lib/ini4j-0.5.4.jar:/usr/lib/hadoop/lib/httpclient-4.5.13.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/jersey-core-1.19.4.jar:/usr/lib/hadoop/lib/commons-codec-1.15.jar:/usr/lib/hadoop/lib/kerb-util-2.0.3.jar:/usr/lib/hadoop/lib/netty-handler-ssl-ocsp-4.1.100.Final.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/junit-4.13.2.jar:/usr/lib/hadoop/lib/netty-transport-sctp-4.1.100.Final.jar:/usr/lib/hadoop/lib/netty-codec-redis-4.1.100.Final.jar:/usr/lib/hadoop/lib/opentracing-api-0.33.0.jar:/usr/lib/hadoop/lib/jsr305-3.0.2.jar:/usr/lib/hadoop/lib/jersey-server-1.19.4.jar:/usr/lib/hadoop/lib/kerb-common-2.0.3.jar:/usr/lib/hadoop/lib/commons-compress-1.21.jar:/usr/lib/hadoop/lib/netty-transport-udt-4.1.100.Final.jar:/usr/lib/hadoop/lib/kerby-xdr-2.0.3.jar:/usr/lib/hado
<13>Apr 27 18:07:52 dataproc-startup-script[1265]: op/lib/aws-java-sdk-bundle-1.12.706.jar:/usr/lib/hadoop/lib/
<13>Apr 27 18:07:52 dataproc-startup-script[1265]: <13>Apr 27 18:07:52 dataproc-activate-component-hdfs[5098]: commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/netty-buffer-4.1.100.Final.jar:/usr/lib/hadoop/lib/gson-2.9.0.jar:/usr/lib/hadoop/lib/netty-transport-4.1.100.Final.jar:/usr/lib/hadoop/lib/netty-codec-haproxy-4.1.100.Final.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.56.v20240826.jar:/usr/lib/hadoop/lib/netty-transport-classes-kqueue-4.1.100.Final.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.36.jar:/usr/lib/hadoop/lib/aliyun-java-sdk-ram-3.1.0.jar:/usr/lib/hadoop/lib/jetty-security-9.4.56.v20240826.jar:/usr/lib/hadoop/lib/failureaccess-1.0.1.jar:/usr/lib/hadoop/lib/jsch-0.1.55.jar:/usr/lib/hadoop/lib/lz4-java-1.7.1.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.36.jar:/usr/lib/hadoop/lib/kerby-pkix-2.0.3.jar:/usr/lib/hadoop/lib/netty-all-4.1.100.Final.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/netty-handler-proxy-4.1.100.Final.jar:/usr/lib/hadoop/lib/commons-lang3-3.12.0.jar:/usr/lib/hadoop/lib/azure-keyvault
<13>Apr 27 18:07:52 dataproc-startup-script[1265]: -core-1.0.0.jar:/usr/lib/hadoop/lib/netty-transport-native-k
<13>Apr 27 18:07:52 dataproc-startup-script[1265]: <13>Apr 27 18:07:52 dataproc-activate-component-hdfs[5098]: queue-4.1.100.Final-osx-aarch_64.jar:/usr/lib/hadoop/lib/ojalgo-43.0.jar:/usr/lib/hadoop/lib/aliyun-java-sdk-kms-2.11.0.jar:/usr/lib/hadoop/lib/netty-codec-4.1.100.Final.jar:/usr/lib/hadoop/lib/commons-configuration2-2.8.0.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-9.37.2.jar:/usr/lib/hadoop/lib/commons-text-1.10.0.jar:/usr/lib/hadoop/lib/jetty-util-ajax-9.4.56.v20240826.jar:/usr/lib/hadoop/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop/lib/zookeeper-jute-3.8.4.jar:/usr/lib/hadoop/lib/netty-resolver-dns-native-macos-4.1.100.Final-osx-x86_64.jar:/usr/lib/hadoop/lib/j2objc-annotations-1.3.jar:/usr/lib/hadoop/lib/netty-transport-native-unix-common-4.1.100.Final.jar:/usr/lib/hadoop/lib/stax2-api-4.2.1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/curator-recipes-5.2.0.jar:/usr/lib/hadoop/lib/netty-codec-xml-4.1.100.Final.jar:/usr/lib/hadoop/lib/avro-1.11.4.jar:/usr/lib/hadoop/lib/netty-transport-native-epoll-4.1.100.Final-linux-x86_6
<13>Apr 27 18:07:52 dataproc-startup-script[1265]: 4.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.4.jar:/usr/lib
<13>Apr 27 18:07:52 dataproc-startup-script[1265]: <13>Apr 27 18:07:52 dataproc-activate-component-hdfs[5098]: /hadoop/lib/jersey-json-1.20.jar:/usr/lib/hadoop/lib/audience-annotations-0.12.0.jar:/usr/lib/hadoop/lib/kerb-client-2.0.3.jar:/usr/lib/hadoop/lib/org.jacoco.agent-0.8.5-runtime.jar:/usr/lib/hadoop/lib/woodstox-core-5.4.0.jar:/usr/lib/hadoop/lib/kerby-asn1-2.0.3.jar:/usr/lib/hadoop/lib/netty-resolver-dns-4.1.100.Final.jar:/usr/lib/hadoop/lib/kerb-identity-2.0.3.jar:/usr/lib/hadoop/lib/netty-codec-http-4.1.100.Final.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.56.v20240826.jar:/usr/lib/hadoop/lib/netty-codec-stomp-4.1.100.Final.jar:/usr/lib/hadoop/lib/hadoop-shaded-guava-1.1.1.jar:/usr/lib/hadoop/lib/aliyun-java-sdk-core-4.5.10.jar:/usr/lib/hadoop/lib/netty-transport-rxtx-4.1.100.Final.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jackson-core-2.12.7.jar:/usr/lib/hadoop/lib/netty-resolver-4.1.100.Final.jar:/usr/lib/hadoop/lib/commons-net-3.9.0.jar:/usr/lib/hadoop/lib/opentracing-util-0.33.0.jar:/usr/lib/hadoop/lib/jackson-annotations-2.
<13>Apr 27 18:07:52 dataproc-startup-script[1265]: 12.7.jar:/usr/lib/hadoop/lib/netty-codec-http2-4.1.100.Final
<13>Apr 27 18:07:52 dataproc-startup-script[1265]: <13>Apr 27 18:07:52 dataproc-activate-component-hdfs[5098]: .jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/checker-qual-3.12.0.jar:/usr/lib/hadoop/lib/kerb-crypto-2.0.3.jar:/usr/lib/hadoop/lib/kerby-config-2.0.3.jar:/usr/lib/hadoop/lib/netty-codec-mqtt-4.1.100.Final.jar:/usr/lib/hadoop/lib/kerb-server-2.0.3.jar:/usr/lib/hadoop/lib/jdom2-2.0.6.jar:/usr/lib/hadoop/lib/slf4j-reload4j-1.7.36.jar:/usr/lib/hadoop/lib/netty-transport-classes-epoll-4.1.100.Final.jar:/usr/lib/hadoop/lib/kerb-admin-2.0.3.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/hadoop-lzo-0.4.20.jar:/usr/lib/hadoop/lib/commons-io-2.8.0.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.56.v20240826.jar:/usr/lib/hadoop/lib/jettison-1.5.4.jar:/usr/lib/hadoop/lib/kerby-util-2.0.3.jar:/usr/lib/hadoop/lib/jetty-util-9.4.56.v20240826.jar:/usr/lib/hadoop/lib/netty-codec-dns-4.1.100.Final.jar:/usr/lib/hadoop/lib/snappy-java-1.1.10.4.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jetty-io-9.4.56.v20240826.j
<13>Apr 27 18:07:52 dataproc-startup-script[1265]: ar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/
<13>Apr 27 18:07:52 dataproc-startup-script[1265]: <13>Apr 27 18:07:52 dataproc-activate-component-hdfs[5098]: lib/guava-31.1-jre.jar:/usr/lib/hadoop/lib/netty-resolver-dns-classes-macos-4.1.100.Final.jar:/usr/lib/hadoop/lib/jetty-http-9.4.56.v20240826.jar:/usr/lib/hadoop/lib/curator-client-5.2.0.jar:/usr/lib/hadoop/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/lib/hadoop/lib/opentracing-noop-0.33.0.jar:/usr/lib/hadoop/lib/jline-3.22.0.jar:/usr/lib/hadoop/lib/jackson-databind-2.12.7.1.jar:/usr/lib/hadoop/lib/netty-codec-memcache-4.1.100.Final.jar:/usr/lib/hadoop/lib/netty-handler-4.1.100.Final.jar:/usr/lib/hadoop/lib/netty-common-4.1.100.Final.jar:/usr/lib/hadoop/lib/netty-transport-native-kqueue-4.1.100.Final-osx-x86_64.jar:/usr/lib/hadoop/lib/zstd-jni-1.4.9-1.jar:/usr/lib/hadoop/lib/netty-codec-socks-4.1.100.Final.jar:/usr/lib/hadoop/lib/netty-resolver-dns-native-macos-4.1.100.Final-osx-aarch_64.jar:/usr/lib/hadoop/lib/azure-data-lake-store-sdk-2.3.9.jar:/usr/lib/hadoop/lib/kerb-core-2.0.3.jar:/usr/lib/hadoop/lib/kerb-simplekdc-2.0
<13>Apr 27 18:07:52 dataproc-startup-script[1265]: .3.jar:/usr/lib/hadoop/lib/wildfly-openssl-1.1.3.Final.jar:/
<13>Apr 27 18:07:52 dataproc-startup-script[1265]: <13>Apr 27 18:07:52 dataproc-activate-component-hdfs[5098]: usr/lib/hadoop/lib/curator-framework-5.2.0.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop/lib/zookeeper-3.8.4.jar:/usr/lib/hadoop/lib/reload4j-1.2.22.jar:/usr/lib/hadoop/lib/dnsjava-3.4.0.jar:/usr/lib/hadoop/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/usr/lib/hadoop/lib/netty-transport-native-epoll-4.1.100.Final-linux-aarch_64.jar:/usr/lib/hadoop/lib/netty-codec-smtp-4.1.100.Final.jar:/usr/lib/hadoop/lib/aliyun-sdk-oss-3.13.0.jar:/usr/lib/hadoop/lib/jetty-server-9.4.56.v20240826.jar:/usr/lib/hadoop/lib/httpcore-4.4.13.jar:/usr/lib/hadoop/lib/azure-storage-7.0.1.jar:/usr/lib/hadoop/.//hadoop-dynamometer-blockgen-3.3.6.jar:/usr/lib/hadoop/.//hadoop-minicluster-3.3.6.jar:/usr/lib/hadoop/.//hadoop-kms-3.3.6.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hado
<13>Apr 27 18:07:52 dataproc-startup-script[1265]: op/.//hadoop-archives-3.3.6.jar:/usr/lib/hadoop/.//hadoop-go
<13>Apr 27 18:07:52 dataproc-startup-script[1265]: <13>Apr 27 18:07:52 dataproc-activate-component-hdfs[5098]: ogle-secret-manager-credential-provider.jar:/usr/lib/hadoop/.//hadoop-dynamometer-blockgen.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-rumen-3.3.6.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.3.6.jar:/usr/lib/hadoop/.//hadoop-extras-3.3.6.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-dynamometer-workload.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-nfs-3.3.6.jar:/usr/lib/hadoop/.//hadoop-distcp-3.3.6.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-aws-3.3.6.jar:/usr/lib/hadoop/.//hadoop-azure-3.3.6.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.3.6.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.3.6.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-google-secret-manager-cre
<13>Apr 27 18:07:52 dataproc-startup-script[1265]: dential-provider-3.3.6.jar:/usr/lib/hadoop/.//hadoop-annotat
<13>Apr 27 18:07:52 dataproc-startup-script[1265]: <13>Apr 27 18:07:52 dataproc-activate-component-hdfs[5098]: ions-3.3.6.jar:/usr/lib/hadoop/.//hadoop-common-3.3.6-tests.jar:/usr/lib/hadoop/.//hadoop-registry-3.3.6.jar:/usr/lib/hadoop/.//hadoop-dynamometer-infra-3.3.6.jar:/usr/lib/hadoop/.//hadoop-shaded-protobuf.jar:/usr/lib/hadoop/.//hadoop-client.jar:/usr/lib/hadoop/.//hadoop-dynamometer-infra.jar:/usr/lib/hadoop/.//hadoop-common-3.3.6.jar:/usr/lib/hadoop/.//hadoop-dynamometer-workload-3.3.6.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-shaded-guava.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.3.6.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.3.6.jar:/usr/lib/hadoop/.//hadoop-client-3.3.6.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.3.6.jar:/usr/lib/hadoop/.//hadoop-auth-3.3.6.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.3.6.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-sls-3.3.6.jar:/usr/lib/hadoop/.//hadoop-az
<13>Apr 27 18:07:52 dataproc-startup-script[1265]: ure-datalake.jar:/usr/lib/hadoop/.//hadoop-kafka-3.3.6.jar:/
<13>Apr 27 18:07:52 dataproc-startup-script[1265]: <13>Apr 27 18:07:52 dataproc-activate-component-hdfs[5098]: usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-streaming-3.3.6.jar:/usr/lib/hadoop/.//hadoop-registry.jar:/usr/lib/hadoop/.//hadoop-minicluster.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/token-provider-2.0.3.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.13.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.4.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.15.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-2.0.3.jar:/usr/lib/hadoop-hdfs/lib/netty-handler-ssl-ocsp-4.1.100.Final.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/lib/netty-transport-sctp-4.1.100.Final.jar:/usr/lib/hadoop-hdfs/lib/netty-codec-redis-4.1.100.Final.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.2.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.4.
<13>Apr 27 18:07:52 dataproc-startup-script[1265]: jar:/usr/lib/hadoop-hdfs/lib/HikariCP-java7-2.4.12.jar:/usr/
<13>Apr 27 18:07:52 dataproc-startup-script[1265]: <13>Apr 27 18:07:52 dataproc-activate-component-hdfs[5098]: lib/hadoop-hdfs/lib/kerb-common-2.0.3.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.21.jar:/usr/lib/hadoop-hdfs/lib/netty-transport-udt-4.1.100.Final.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-2.0.3.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/netty-buffer-4.1.100.Final.jar:/usr/lib/hadoop-hdfs/lib/gson-2.9.0.jar:/usr/lib/hadoop-hdfs/lib/netty-transport-4.1.100.Final.jar:/usr/lib/hadoop-hdfs/lib/netty-codec-haproxy-4.1.100.Final.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.56.v20240826.jar:/usr/lib/hadoop-hdfs/lib/netty-transport-classes-kqueue-4.1.100.Final.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.56.v20240826.jar:/usr/lib/hadoop-hdfs/lib/failureaccess-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.55.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-2.0.3.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.1.100.Final.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs
<13>Apr 27 18:07:52 dataproc-startup-script[1265]: /lib/netty-handler-proxy-4.1.100.Final.jar:/usr/lib/hadoop-h
<13>Apr 27 18:07:52 dataproc-startup-script[1265]: <13>Apr 27 18:07:52 dataproc-activate-component-hdfs[5098]: dfs/lib/commons-lang3-3.12.0.jar:/usr/lib/hadoop-hdfs/lib/kotlin-stdlib-common-1.4.10.jar:/usr/lib/hadoop-hdfs/lib/netty-transport-native-kqueue-4.1.100.Final-osx-aarch_64.jar:/usr/lib/hadoop-hdfs/lib/netty-codec-4.1.100.Final.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.8.0.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-9.37.2.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.10.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.56.v20240826.jar:/usr/lib/hadoop-hdfs/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-jute-3.8.4.jar:/usr/lib/hadoop-hdfs/lib/netty-resolver-dns-native-macos-4.1.100.Final-osx-x86_64.jar:/usr/lib/hadoop-hdfs/lib/j2objc-annotations-1.3.jar:/usr/lib/hadoop-hdfs/lib/netty-transport-native-unix-common-4.1.100.Final.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-4.2.1.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-5.2.0.jar
<13>Apr 27 18:07:52 dataproc-startup-script[1265]: :/usr/lib/hadoop-hdfs/lib/netty-codec-xml-4.1.100.Final.jar:
<13>Apr 27 18:07:52 dataproc-startup-script[1265]: <13>Apr 27 18:07:52 dataproc-activate-component-hdfs[5098]: /usr/lib/hadoop-hdfs/lib/avro-1.11.4.jar:/usr/lib/hadoop-hdfs/lib/netty-transport-native-epoll-4.1.100.Final-linux-x86_64.jar:/usr/lib/hadoop-hdfs/lib/okhttp-4.9.3.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.4.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.20.jar:/usr/lib/hadoop-hdfs/lib/audience-annotations-0.12.0.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-2.0.3.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.4.0.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-2.0.3.jar:/usr/lib/hadoop-hdfs/lib/netty-resolver-dns-4.1.100.Final.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-2.0.3.jar:/usr/lib/hadoop-hdfs/lib/netty-codec-http-4.1.100.Final.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.56.v20240826.jar:/usr/lib/hadoop-hdfs/lib/netty-codec-stomp-4.1.100.Final.jar:/usr/lib/hadoop-hdfs/lib/hadoop-shaded-guava-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/netty-transport-rxtx-4.1.100.Final.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.j
<13>Apr 27 18:07:52 dataproc-startup-script[1265]: ar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.12.7.jar:/usr/lib
<13>Apr 27 18:07:52 dataproc-startup-script[1265]: <13>Apr 27 18:07:52 dataproc-activate-component-hdfs[5098]: /hadoop-hdfs/lib/netty-resolver-4.1.100.Final.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.9.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.12.7.jar:/usr/lib/hadoop-hdfs/lib/netty-codec-http2-4.1.100.Final.jar:/usr/lib/hadoop-hdfs/lib/checker-qual-3.12.0.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-2.0.3.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-2.0.3.jar:/usr/lib/hadoop-hdfs/lib/netty-codec-mqtt-4.1.100.Final.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-2.0.3.jar:/usr/lib/hadoop-hdfs/lib/netty-transport-classes-epoll-4.1.100.Final.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-2.0.3.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.8.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.56.v20240826.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.5.4.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-2.0.3.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.56.v20240826.jar:/usr/lib/hadoop-hdfs/lib/netty-codec-dns-4.1.100.Final.jar:/usr/lib/hadoop-hdfs/li
<13>Apr 27 18:07:52 dataproc-startup-script[1265]: b/snappy-java-1.1.10.4.jar:/usr/lib/hadoop-hdfs/lib/commons-
<13>Apr 27 18:07:52 dataproc-startup-script[1265]: <13>Apr 27 18:07:52 dataproc-activate-component-hdfs[5098]: collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.56.v20240826.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/guava-31.1-jre.jar:/usr/lib/hadoop-hdfs/lib/netty-resolver-dns-classes-macos-4.1.100.Final.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.56.v20240826.jar:/usr/lib/hadoop-hdfs/lib/okio-2.8.0.jar:/usr/lib/hadoop-hdfs/lib/curator-client-5.2.0.jar:/usr/lib/hadoop-hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/lib/hadoop-hdfs/lib/jline-3.22.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.12.7.1.jar:/usr/lib/hadoop-hdfs/lib/netty-codec-memcache-4.1.100.Final.jar:/usr/lib/hadoop-hdfs/lib/netty-handler-4.1.100.Final.jar:/usr/lib/hadoop-hdfs/lib/netty-common-4.1.100.Final.jar:/usr/lib/hadoop-hdfs/lib/netty-transport-native-kqueue-4.1.100.Final-osx-x86_64.jar:/usr/lib/hadoop-hdfs/lib/kotlin-stdlib-1.4.10.jar:/usr/lib/hadoop-hdfs/lib/netty-codec-socks-4.1.100.Final.jar:/usr/lib/hadoop-h
<13>Apr 27 18:07:52 dataproc-startup-script[1265]: dfs/lib/netty-resolver-dns-native-macos-4.1.100.Final-osx-aa
<13>Apr 27 18:07:52 dataproc-startup-script[1265]: <13>Apr 27 18:07:52 dataproc-activate-component-hdfs[5098]: rch_64.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-2.0.3.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-2.0.3.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-5.2.0.jar:/usr/lib/hadoop-hdfs/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.8.4.jar:/usr/lib/hadoop-hdfs/lib/reload4j-1.2.22.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-3.4.0.jar:/usr/lib/hadoop-hdfs/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/netty-transport-native-epoll-4.1.100.Final-linux-aarch_64.jar:/usr/lib/hadoop-hdfs/lib/netty-codec-smtp-4.1.100.Final.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.56.v20240826.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.13.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//had
<13>Apr 27 18:07:52 dataproc-startup-script[1265]: oop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-clie
<13>Apr 27 18:07:52 dataproc-startup-script[1265]: <13>Apr 27 18:07:52 dataproc-activate-component-hdfs[5098]: nt-3.3.6-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.3.6.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.3.6.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.3.6.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.3.6.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.3.6-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.3.6-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.3.6-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.3.6.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.3.6.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.3.6-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.3.6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.3.6.jar:/usr
<13>Apr 27 18:07:52 dataproc-startup-script[1265]: /lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3
<13>Apr 27 18:07:52 dataproc-startup-script[1265]: <13>Apr 27 18:07:52 dataproc-activate-component-hdfs[5098]: .3.6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.3.6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.3.6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.3.6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.3.6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.3.6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.3.6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.3.6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//h
<13>Apr 27 18:07:52 dataproc-startup-script[1265]: adoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-yarn/li
<13>Apr 27 18:07:52 dataproc-startup-script[1265]: <13>Apr 27 18:07:52 dataproc-activate-component-hdfs[5098]: b/jetty-jndi-9.4.56.v20240826.jar:/usr/lib/hadoop-yarn/lib/jetty-client-9.4.56.v20240826.jar:/usr/lib/hadoop-yarn/lib/websocket-common-9.4.56.v20240826.jar:/usr/lib/hadoop-yarn/lib/jetty-annotations-9.4.56.v20240826.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.12.7.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.4.jar:/usr/lib/hadoop-yarn/lib/asm-commons-9.7.jar:/usr/lib/hadoop-yarn/lib/objenesis-2.6.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.4.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.10.0.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.12.7.jar:/usr/lib/hadoop-yarn/lib/javax-websocket-client-impl-9.4.56.v20240826.jar:/usr/lib/hadoop-yarn/lib/cache-api-1.1.0.jar:/usr/lib/hadoop-yarn/lib/jna-5.2.0.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/guice-
<13>Apr 27 18:07:52 dataproc-startup-script[1265]: servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/jetty-plus-9.4.56.v
<13>Apr 27 18:07:52 dataproc-startup-script[1265]: <13>Apr 27 18:07:52 dataproc-activate-component-hdfs[5098]: 20240826.jar:/usr/lib/hadoop-yarn/lib/asm-tree-9.7.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.12.7.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/javax.websocket-api-1.0.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/websocket-servlet-9.4.56.v20240826.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.68.jar:/usr/lib/hadoop-yarn/lib/websocket-client-9.4.56.v20240826.jar:/usr/lib/hadoop-yarn/lib/javax.websocket-client-api-1.0.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.68.jar:/usr/lib/hadoop-yarn/lib/websocket-api-9.4.56.v20240826.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/websocket-server-9.4.56.v20240826.jar:/usr/lib/hadoop-yarn/lib/javax-websocket-server-impl-9.4.56.v20240826.jar:/usr/lib/hadoop-yarn/.//hadoo
<13>Apr 27 18:07:52 dataproc-startup-script[1265]: p-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-te
<13>Apr 27 18:07:52 dataproc-startup-script[1265]: <13>Apr 27 18:07:52 dataproc-activate-component-hdfs[5098]: sts.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.3.6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.3.6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.3.6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.3.6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.3.6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-mawo-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.3.6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.3.6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.3.6.jar:/usr/lib/hadoop-yarn/.//hadoo
<13>Apr 27 18:07:52 dataproc-startup-script[1265]: p-yarn-server-tests-3.3.6.jar:/usr/lib/hadoop-yarn/.//hadoop
<13>Apr 27 18:07:52 dataproc-startup-script[1265]: <13>Apr 27 18:07:52 dataproc-activate-component-hdfs[5098]: -yarn-server-web-proxy-3.3.6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.3.6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.3.6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.3.6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.3.6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.3.6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-mawo-core-3.3.6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.3.6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-
<13>Apr 27 18:07:52 dataproc-startup-script[1265]: yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/us
<13>Apr 27 18:07:52 dataproc-startup-script[1265]: <13>Apr 27 18:07:52 dataproc-activate-component-hdfs[5098]: r/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.3.6.jar:/usr/local/share/google/dataproc/lib/conscrypt.jar:/usr/local/share/google/dataproc/lib/gcs-connector-3.0.5.jar:/usr/local/share/google/dataproc/lib/gcs-connector.jar:/usr/local/share/google/dataproc/lib/ranger_gcs_plugin_client.jar
<13>Apr 27 18:07:52 dataproc-startup-script[1265]: <13>Apr 27 18:07:52 dataproc-activate-component-hdfs[5098]: STARTUP_MSG:   build = https://bigdataoss-internal.googlesource.com/third_party/apache/hadoop -r 945b6ad3682511720b513a82df4eb717fd29fdba; compiled by 'bigtop' on 2025-04-15T21:02Z
<13>Apr 27 18:07:52 dataproc-startup-script[1265]: <13>Apr 27 18:07:52 dataproc-activate-component-hdfs[5098]: STARTUP_MSG:   java = 11.0.20.1
<13>Apr 27 18:07:52 dataproc-startup-script[1265]: <13>Apr 27 18:07:52 dataproc-activate-component-hdfs[5098]: ************************************************************/
<13>Apr 27 18:07:52 dataproc-startup-script[1265]: <13>Apr 27 18:07:52 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:07:52,303 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
<13>Apr 27 18:07:52 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5090.exitcode ]]
<13>Apr 27 18:07:52 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:07:53 dataproc-startup-script[1265]: <13>Apr 27 18:07:53 dataproc-activate-component-knox[5101]: Master secret has been persisted to disk.
<13>Apr 27 18:07:53 dataproc-startup-script[1265]: <13>Apr 27 18:07:53 dataproc-activate-component-knox[5101]: '/usr/lib/knox/bin/knoxcli.sh create-master --generate' succeeded after 1 execution(s).
<13>Apr 27 18:07:53 dataproc-startup-script[1265]: <13>Apr 27 18:07:53 dataproc-activate-component-knox[5101]: + return 0
<13>Apr 27 18:07:53 dataproc-startup-script[1265]: <13>Apr 27 18:07:53 dataproc-activate-component-knox[5101]: + chown knox:knox /usr/lib/knox/data/security/master
<13>Apr 27 18:07:53 dataproc-startup-script[1265]: <13>Apr 27 18:07:53 dataproc-activate-component-knox[5101]: + chmod 400 /usr/lib/knox/data/security/master
<13>Apr 27 18:07:53 dataproc-startup-script[1265]: <13>Apr 27 18:07:53 dataproc-activate-component-knox[5101]: + chown -R -L knox:knox /usr/lib/knox/pids /usr/lib/knox/conf /usr/lib/knox/data/security
<13>Apr 27 18:07:53 dataproc-startup-script[1265]: <13>Apr 27 18:07:53 dataproc-activate-component-knox[5101]: + systemctl daemon-reload
<13>Apr 27 18:07:53 dataproc-startup-script[1265]: <13>Apr 27 18:07:53 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:07:53,293 INFO namenode.NameNode: createNameNode [-format, -nonInteractive]
<13>Apr 27 18:07:53 dataproc-startup-script[1265]: <13>Apr 27 18:07:53 dataproc-activate-component-hive-metastore[5099]: + set -x
<13>Apr 27 18:07:53 dataproc-startup-script[1265]: <13>Apr 27 18:07:53 dataproc-activate-component-hive-metastore[5099]: + start_hive_metastore
<13>Apr 27 18:07:53 dataproc-startup-script[1265]: <13>Apr 27 18:07:53 dataproc-activate-component-hive-metastore[5099]: + enable_service hive-metastore
<13>Apr 27 18:07:53 dataproc-startup-script[1265]: <13>Apr 27 18:07:53 dataproc-activate-component-hive-metastore[5099]: + local -r service=hive-metastore
<13>Apr 27 18:07:53 dataproc-startup-script[1265]: <13>Apr 27 18:07:53 dataproc-activate-component-hive-metastore[5099]: + local -r unit=hive-metastore.service
<13>Apr 27 18:07:53 dataproc-startup-script[1265]: <13>Apr 27 18:07:53 dataproc-activate-component-hive-metastore[5099]: + retry_constant_short systemctl enable hive-metastore.service
<13>Apr 27 18:07:53 dataproc-startup-script[1265]: <13>Apr 27 18:07:53 dataproc-activate-component-hive-metastore[5099]: + retry_constant_custom 30 1 systemctl enable hive-metastore.service
<13>Apr 27 18:07:53 dataproc-startup-script[1265]: <13>Apr 27 18:07:53 dataproc-activate-component-hive-metastore[5099]: + local -r max_retry_time=30
<13>Apr 27 18:07:53 dataproc-startup-script[1265]: <13>Apr 27 18:07:53 dataproc-activate-component-hive-metastore[5099]: + local -r retry_delay=1
<13>Apr 27 18:07:53 dataproc-startup-script[1265]: <13>Apr 27 18:07:53 dataproc-activate-component-hive-metastore[5099]: + cmd=('systemctl' 'enable' 'hive-metastore.service')
<13>Apr 27 18:07:53 dataproc-startup-script[1265]: <13>Apr 27 18:07:53 dataproc-activate-component-hive-metastore[5099]: + local -r cmd
<13>Apr 27 18:07:53 dataproc-startup-script[1265]: <13>Apr 27 18:07:53 dataproc-activate-component-hive-metastore[5099]: + local -r max_retries=30
<13>Apr 27 18:07:53 dataproc-startup-script[1265]: <13>Apr 27 18:07:53 dataproc-activate-component-hive-metastore[5099]: + local reenable_x=false
<13>Apr 27 18:07:53 dataproc-startup-script[1265]: <13>Apr 27 18:07:53 dataproc-activate-component-hive-metastore[5099]: + [[ -o xtrace ]]
<13>Apr 27 18:07:53 dataproc-startup-script[1265]: <13>Apr 27 18:07:53 dataproc-activate-component-hive-metastore[5099]: + set +x
<13>Apr 27 18:07:53 dataproc-startup-script[1265]: <13>Apr 27 18:07:53 dataproc-activate-component-hive-metastore[5099]: About to run 'systemctl enable hive-metastore.service' with retries...
<13>Apr 27 18:07:53 dataproc-startup-script[1265]: <13>Apr 27 18:07:53 dataproc-activate-component-hive-metastore[5099]: hive-metastore.service is not a native service, redirecting to systemd-sysv-install.
<13>Apr 27 18:07:53 dataproc-startup-script[1265]: <13>Apr 27 18:07:53 dataproc-activate-component-hive-metastore[5099]: Executing: /lib/systemd/systemd-sysv-install enable hive-metastore
<13>Apr 27 18:07:53 dataproc-startup-script[1265]: <13>Apr 27 18:07:53 dataproc-activate-component-knox[5101]: + retry_constant_short systemctl enable knox
<13>Apr 27 18:07:53 dataproc-startup-script[1265]: <13>Apr 27 18:07:53 dataproc-activate-component-knox[5101]: + retry_constant_custom 30 1 systemctl enable knox
<13>Apr 27 18:07:53 dataproc-startup-script[1265]: <13>Apr 27 18:07:53 dataproc-activate-component-knox[5101]: + local -r max_retry_time=30
<13>Apr 27 18:07:53 dataproc-startup-script[1265]: <13>Apr 27 18:07:53 dataproc-activate-component-knox[5101]: + local -r retry_delay=1
<13>Apr 27 18:07:53 dataproc-startup-script[1265]: <13>Apr 27 18:07:53 dataproc-activate-component-knox[5101]: + cmd=('systemctl' 'enable' 'knox')
<13>Apr 27 18:07:53 dataproc-startup-script[1265]: <13>Apr 27 18:07:53 dataproc-activate-component-knox[5101]: + local -r cmd
<13>Apr 27 18:07:53 dataproc-startup-script[1265]: <13>Apr 27 18:07:53 dataproc-activate-component-knox[5101]: + local -r max_retries=30
<13>Apr 27 18:07:53 dataproc-startup-script[1265]: <13>Apr 27 18:07:53 dataproc-activate-component-knox[5101]: + local reenable_x=false
<13>Apr 27 18:07:53 dataproc-startup-script[1265]: <13>Apr 27 18:07:53 dataproc-activate-component-knox[5101]: + [[ -o xtrace ]]
<13>Apr 27 18:07:53 dataproc-startup-script[1265]: <13>Apr 27 18:07:53 dataproc-activate-component-knox[5101]: + set +x
<13>Apr 27 18:07:53 dataproc-startup-script[1265]: <13>Apr 27 18:07:53 dataproc-activate-component-knox[5101]: About to run 'systemctl enable knox' with retries...
<13>Apr 27 18:07:53 dataproc-startup-script[1265]: <13>Apr 27 18:07:53 dataproc-activate-component-knox[5101]: Created symlink /etc/systemd/system/multi-user.target.wants/knox.service → /lib/systemd/system/knox.service.
<13>Apr 27 18:07:53 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5090.exitcode ]]
<13>Apr 27 18:07:53 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:07:54 dataproc-startup-script[1265]: <13>Apr 27 18:07:54 dataproc-activate-component-knox[5101]: 'systemctl enable knox' succeeded after 1 execution(s).
<13>Apr 27 18:07:54 dataproc-startup-script[1265]: <13>Apr 27 18:07:54 dataproc-activate-component-knox[5101]: + return 0
<13>Apr 27 18:07:54 dataproc-startup-script[1265]: <13>Apr 27 18:07:54 dataproc-activate-component-knox[5101]: + retry_constant_short systemctl start knox
<13>Apr 27 18:07:54 dataproc-startup-script[1265]: <13>Apr 27 18:07:54 dataproc-activate-component-knox[5101]: + retry_constant_custom 30 1 systemctl start knox
<13>Apr 27 18:07:54 dataproc-startup-script[1265]: <13>Apr 27 18:07:54 dataproc-activate-component-knox[5101]: + local -r max_retry_time=30
<13>Apr 27 18:07:54 dataproc-startup-script[1265]: <13>Apr 27 18:07:54 dataproc-activate-component-knox[5101]: + local -r retry_delay=1
<13>Apr 27 18:07:54 dataproc-startup-script[1265]: <13>Apr 27 18:07:54 dataproc-activate-component-knox[5101]: + cmd=('systemctl' 'start' 'knox')
<13>Apr 27 18:07:54 dataproc-startup-script[1265]: <13>Apr 27 18:07:54 dataproc-activate-component-knox[5101]: + local -r cmd
<13>Apr 27 18:07:54 dataproc-startup-script[1265]: <13>Apr 27 18:07:54 dataproc-activate-component-knox[5101]: + local -r max_retries=30
<13>Apr 27 18:07:54 dataproc-startup-script[1265]: <13>Apr 27 18:07:54 dataproc-activate-component-knox[5101]: + local reenable_x=false
<13>Apr 27 18:07:54 dataproc-startup-script[1265]: <13>Apr 27 18:07:54 dataproc-activate-component-knox[5101]: + [[ -o xtrace ]]
<13>Apr 27 18:07:54 dataproc-startup-script[1265]: <13>Apr 27 18:07:54 dataproc-activate-component-knox[5101]: + set +x
<13>Apr 27 18:07:54 dataproc-startup-script[1265]: <13>Apr 27 18:07:54 dataproc-activate-component-knox[5101]: About to run 'systemctl start knox' with retries...
<13>Apr 27 18:07:54 dataproc-startup-script[1265]: <13>Apr 27 18:07:54 dataproc-activate-component-hive-metastore[5099]: 'systemctl enable hive-metastore.service' succeeded after 1 execution(s).
<13>Apr 27 18:07:54 dataproc-startup-script[1265]: <13>Apr 27 18:07:54 dataproc-activate-component-hive-metastore[5099]: + return 0
<13>Apr 27 18:07:54 dataproc-startup-script[1265]: <13>Apr 27 18:07:54 dataproc-activate-component-hive-metastore[5099]: + local -r drop_in_dir=/etc/systemd/system/hive-metastore.service.d
<13>Apr 27 18:07:54 dataproc-startup-script[1265]: <13>Apr 27 18:07:54 dataproc-activate-component-hive-metastore[5099]: + mkdir -p /etc/systemd/system/hive-metastore.service.d
<13>Apr 27 18:07:54 dataproc-startup-script[1265]: <13>Apr 27 18:07:54 dataproc-activate-component-hive-metastore[5099]: + local props
<13>Apr 27 18:07:54 dataproc-startup-script[1265]: <13>Apr 27 18:07:54 dataproc-activate-component-hive-metastore[5099]: ++ retry_constant_short systemctl show hive-metastore.service -p Restart,RemainAfterExit
<13>Apr 27 18:07:54 dataproc-startup-script[1265]: <13>Apr 27 18:07:54 dataproc-activate-component-hive-metastore[5099]: ++ retry_constant_custom 30 1 systemctl show hive-metastore.service -p Restart,RemainAfterExit
<13>Apr 27 18:07:54 dataproc-startup-script[1265]: <13>Apr 27 18:07:54 dataproc-activate-component-hive-metastore[5099]: ++ local -r max_retry_time=30
<13>Apr 27 18:07:54 dataproc-startup-script[1265]: <13>Apr 27 18:07:54 dataproc-activate-component-hive-metastore[5099]: ++ local -r retry_delay=1
<13>Apr 27 18:07:54 dataproc-startup-script[1265]: <13>Apr 27 18:07:54 dataproc-activate-component-hive-metastore[5099]: ++ cmd=('systemctl' 'show' 'hive-metastore.service' '-p' 'Restart,RemainAfterExit')
<13>Apr 27 18:07:54 dataproc-startup-script[1265]: <13>Apr 27 18:07:54 dataproc-activate-component-hive-metastore[5099]: ++ local -r cmd
<13>Apr 27 18:07:54 dataproc-startup-script[1265]: <13>Apr 27 18:07:54 dataproc-activate-component-hive-metastore[5099]: ++ local -r max_retries=30
<13>Apr 27 18:07:54 dataproc-startup-script[1265]: <13>Apr 27 18:07:54 dataproc-activate-component-hive-metastore[5099]: ++ local reenable_x=false
<13>Apr 27 18:07:54 dataproc-startup-script[1265]: <13>Apr 27 18:07:54 dataproc-activate-component-hive-metastore[5099]: ++ [[ -o xtrace ]]
<13>Apr 27 18:07:54 dataproc-startup-script[1265]: <13>Apr 27 18:07:54 dataproc-activate-component-hive-metastore[5099]: ++ set +x
<13>Apr 27 18:07:54 dataproc-startup-script[1265]: <13>Apr 27 18:07:54 dataproc-activate-component-hive-metastore[5099]: About to run 'systemctl show hive-metastore.service -p Restart,RemainAfterExit' with retries...
<13>Apr 27 18:07:54 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5090.exitcode ]]
<13>Apr 27 18:07:54 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:07:54 dataproc-startup-script[1265]: <13>Apr 27 18:07:54 dataproc-activate-component-hive-metastore[5099]: 'systemctl show hive-metastore.service -p Restart,RemainAfterExit' succeeded after 1 execution(s).
<13>Apr 27 18:07:54 dataproc-startup-script[1265]: <13>Apr 27 18:07:54 dataproc-activate-component-hive-metastore[5099]: ++ return 0
<13>Apr 27 18:07:54 dataproc-startup-script[1265]: <13>Apr 27 18:07:54 dataproc-activate-component-hive-metastore[5099]: + props='Restart=no
<13>Apr 27 18:07:54 dataproc-startup-script[1265]: <13>Apr 27 18:07:54 dataproc-activate-component-hive-metastore[5099]: RemainAfterExit=no'
<13>Apr 27 18:07:54 dataproc-startup-script[1265]: <13>Apr 27 18:07:54 dataproc-activate-component-hive-metastore[5099]: + [[ hive-metastore != \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Apr 27 18:07:54 dataproc-startup-script[1265]: <13>Apr 27 18:07:54 dataproc-activate-component-hive-metastore[5099]: + [[ hive-metastore != \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Apr 27 18:07:54 dataproc-startup-script[1265]: <13>Apr 27 18:07:54 dataproc-activate-component-hive-metastore[5099]: + [[ Restart=no
<13>Apr 27 18:07:54 dataproc-startup-script[1265]: <13>Apr 27 18:07:54 dataproc-activate-component-hive-metastore[5099]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Apr 27 18:07:54 dataproc-startup-script[1265]: <13>Apr 27 18:07:54 dataproc-activate-component-hive-metastore[5099]: + [[ Restart=no
<13>Apr 27 18:07:54 dataproc-startup-script[1265]: <13>Apr 27 18:07:54 dataproc-activate-component-hive-metastore[5099]: RemainAfterExit=no == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Apr 27 18:07:54 dataproc-startup-script[1265]: <13>Apr 27 18:07:54 dataproc-activate-component-hive-metastore[5099]: + ln -s -f /etc/systemd/system/common/restart.conf /etc/systemd/system/hive-metastore.service.d
<13>Apr 27 18:07:54 dataproc-startup-script[1265]: <13>Apr 27 18:07:54 dataproc-activate-component-hive-metastore[5099]: + local -r gate_start_on_agent_success_drop_in=/etc/systemd/system/common/agent-gate.conf
<13>Apr 27 18:07:54 dataproc-startup-script[1265]: <13>Apr 27 18:07:54 dataproc-activate-component-hive-metastore[5099]: + [[ ! -f /etc/systemd/system/common/agent-gate.conf ]]
<13>Apr 27 18:07:54 dataproc-startup-script[1265]: <13>Apr 27 18:07:54 dataproc-activate-component-hive-metastore[5099]: + [[ hive-metastore == \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Apr 27 18:07:54 dataproc-startup-script[1265]: <13>Apr 27 18:07:54 dataproc-activate-component-hive-metastore[5099]: + [[ hive-metastore == \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Apr 27 18:07:54 dataproc-startup-script[1265]: <13>Apr 27 18:07:54 dataproc-activate-component-hive-metastore[5099]: + retry_constant systemctl restart hive-metastore
<13>Apr 27 18:07:54 dataproc-startup-script[1265]: <13>Apr 27 18:07:54 dataproc-activate-component-hive-metastore[5099]: + retry_constant_custom 300 1 systemctl restart hive-metastore
<13>Apr 27 18:07:54 dataproc-startup-script[1265]: <13>Apr 27 18:07:54 dataproc-activate-component-hive-metastore[5099]: + local -r max_retry_time=300
<13>Apr 27 18:07:54 dataproc-startup-script[1265]: <13>Apr 27 18:07:54 dataproc-activate-component-hive-metastore[5099]: + local -r retry_delay=1
<13>Apr 27 18:07:54 dataproc-startup-script[1265]: <13>Apr 27 18:07:54 dataproc-activate-component-hive-metastore[5099]: + cmd=('systemctl' 'restart' 'hive-metastore')
<13>Apr 27 18:07:54 dataproc-startup-script[1265]: <13>Apr 27 18:07:54 dataproc-activate-component-hive-metastore[5099]: + local -r cmd
<13>Apr 27 18:07:54 dataproc-startup-script[1265]: <13>Apr 27 18:07:54 dataproc-activate-component-hive-metastore[5099]: + local -r max_retries=300
<13>Apr 27 18:07:54 dataproc-startup-script[1265]: <13>Apr 27 18:07:54 dataproc-activate-component-hive-metastore[5099]: + local reenable_x=false
<13>Apr 27 18:07:54 dataproc-startup-script[1265]: <13>Apr 27 18:07:54 dataproc-activate-component-hive-metastore[5099]: + [[ -o xtrace ]]
<13>Apr 27 18:07:54 dataproc-startup-script[1265]: <13>Apr 27 18:07:54 dataproc-activate-component-hive-metastore[5099]: + set +x
<13>Apr 27 18:07:54 dataproc-startup-script[1265]: <13>Apr 27 18:07:54 dataproc-activate-component-hive-metastore[5099]: About to run 'systemctl restart hive-metastore' with retries...
<13>Apr 27 18:07:54 dataproc-startup-script[1265]: <13>Apr 27 18:07:54 dataproc-activate-component-hive-metastore[5099]: Warning: The unit file, source configuration file or drop-ins of hive-metastore.service changed on disk. Run 'systemctl daemon-reload' to reload units.
<13>Apr 27 18:07:55 dataproc-startup-script[1265]: ++ echo 0
<13>Apr 27 18:07:55 dataproc-startup-script[1265]: <13>Apr 27 18:07:55 dataproc-setup-hadoop-yarn-timelineserver[5093]: 'systemctl start hadoop-yarn-timelineserver.service' succeeded after 1 execution(s).
<13>Apr 27 18:07:55 dataproc-startup-script[1265]: <13>Apr 27 18:07:55 dataproc-setup-hadoop-yarn-timelineserver[5093]: + return 0
<13>Apr 27 18:07:55 dataproc-startup-script[1265]: ++ echo 0
<13>Apr 27 18:07:55 dataproc-startup-script[1265]: <13>Apr 27 18:07:55 dataproc-setup-hadoop-yarn-resourcemanager[5090]: 'systemctl start hadoop-yarn-resourcemanager.service' succeeded after 1 execution(s).
<13>Apr 27 18:07:55 dataproc-startup-script[1265]: <13>Apr 27 18:07:55 dataproc-setup-hadoop-yarn-resourcemanager[5090]: + return 0
<13>Apr 27 18:07:55 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5090.exitcode ]]
<13>Apr 27 18:07:55 dataproc-startup-script[1265]: + local status
<13>Apr 27 18:07:55 dataproc-startup-script[1265]: + status=0
<13>Apr 27 18:07:55 dataproc-startup-script[1265]: + (( status != 0 ))
<13>Apr 27 18:07:55 dataproc-startup-script[1265]: + echo 'Command cmd=[setup_service hadoop-yarn-resourcemanager] pid=5090 exited with 0'
<13>Apr 27 18:07:55 dataproc-startup-script[1265]: + tee /tmp/dataproc/commands/5090.done
<13>Apr 27 18:07:55 dataproc-startup-script[1265]: Command cmd=[setup_service hadoop-yarn-resourcemanager] pid=5090 exited with 0
<13>Apr 27 18:07:55 dataproc-startup-script[1265]: + rm /tmp/dataproc/commands/5090.exitcode /tmp/dataproc/commands/5090.running
<13>Apr 27 18:07:55 dataproc-startup-script[1265]: + for running_file in "${COMMANDS_TMP_DIR}/"*'.running'
<13>Apr 27 18:07:55 dataproc-startup-script[1265]: + local pid
<13>Apr 27 18:07:55 dataproc-startup-script[1265]: ++ basename /tmp/dataproc/commands/5092
<13>Apr 27 18:07:55 dataproc-startup-script[1265]: + pid=5092
<13>Apr 27 18:07:55 dataproc-startup-script[1265]: + local cmd
<13>Apr 27 18:07:55 dataproc-startup-script[1265]: + cmd='setup_service hadoop-mapreduce-historyserver'
<13>Apr 27 18:07:55 dataproc-startup-script[1265]: + loginfo 'Waiting on pid=5092 cmd=[setup_service hadoop-mapreduce-historyserver]'
<13>Apr 27 18:07:55 dataproc-startup-script[1265]: + echo 'Waiting on pid=5092 cmd=[setup_service hadoop-mapreduce-historyserver]'
<13>Apr 27 18:07:55 dataproc-startup-script[1265]: Waiting on pid=5092 cmd=[setup_service hadoop-mapreduce-historyserver]
<13>Apr 27 18:07:55 dataproc-startup-script[1265]: + echo 'setup_service hadoop-mapreduce-historyserver'
<13>Apr 27 18:07:55 dataproc-startup-script[1265]: + local exitcode_file=/tmp/dataproc/commands/5092.exitcode
<13>Apr 27 18:07:55 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5092.exitcode ]]
<13>Apr 27 18:07:55 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:07:55 dataproc-startup-script[1265]: <13>Apr 27 18:07:55 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:07:55.771+0000][info][gc,start     ] GC(2) Pause Young (Allocation Failure)
<13>Apr 27 18:07:55 dataproc-startup-script[1265]: <13>Apr 27 18:07:55 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:07:55.771+0000][info][gc,task      ] GC(2) Using 2 workers of 2 for evacuation
<13>Apr 27 18:07:55 dataproc-startup-script[1265]: <13>Apr 27 18:07:55 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:07:55.812+0000][info][gc,heap      ] GC(2) ParNew: 38720K->1685K(38720K)
<13>Apr 27 18:07:55 dataproc-startup-script[1265]: <13>Apr 27 18:07:55 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:07:55.812+0000][info][gc,heap      ] GC(2) CMS: 41083K->45334K(86016K)
<13>Apr 27 18:07:55 dataproc-startup-script[1265]: <13>Apr 27 18:07:55 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:07:55.812+0000][info][gc,metaspace ] GC(2) Metaspace: 16172K(16512K)->16172K(16512K) NonClass: 14646K(14848K)->14646K(14848K) Class: 1526K(1664K)->1526K(1664K)
<13>Apr 27 18:07:55 dataproc-startup-script[1265]: <13>Apr 27 18:07:55 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:07:55.812+0000][info][gc           ] GC(2) Pause Young (Allocation Failure) 77M->45M(121M) 40.522ms
<13>Apr 27 18:07:55 dataproc-startup-script[1265]: <13>Apr 27 18:07:55 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:07:55.812+0000][info][gc,cpu       ] GC(2) User=0.01s Sys=0.00s Real=0.04s
<13>Apr 27 18:07:55 dataproc-startup-script[1265]: <13>Apr 27 18:07:55 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:07:55.812+0000][info][gc,start     ] GC(3) Pause Initial Mark
<13>Apr 27 18:07:55 dataproc-startup-script[1265]: <13>Apr 27 18:07:55 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:07:55.814+0000][info][gc           ] GC(3) Pause Initial Mark 46M->46M(121M) 1.773ms
<13>Apr 27 18:07:55 dataproc-startup-script[1265]: <13>Apr 27 18:07:55 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:07:55.814+0000][info][gc,cpu       ] GC(3) User=0.00s Sys=0.00s Real=0.00s
<13>Apr 27 18:07:55 dataproc-startup-script[1265]: <13>Apr 27 18:07:55 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:07:55.814+0000][info][gc           ] GC(3) Concurrent Mark
<13>Apr 27 18:07:55 dataproc-startup-script[1265]: ++ echo 0
<13>Apr 27 18:07:55 dataproc-startup-script[1265]: <13>Apr 27 18:07:55 dataproc-setup-hadoop-mapreduce-historyserver[5092]: 'systemctl start hadoop-mapreduce-historyserver.service' succeeded after 1 execution(s).
<13>Apr 27 18:07:55 dataproc-startup-script[1265]: <13>Apr 27 18:07:55 dataproc-setup-hadoop-mapreduce-historyserver[5092]: + return 0
<13>Apr 27 18:07:55 dataproc-startup-script[1265]: <13>Apr 27 18:07:55 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:07:55.865+0000][info][gc           ] GC(3) Concurrent Mark 50.441ms
<13>Apr 27 18:07:55 dataproc-startup-script[1265]: <13>Apr 27 18:07:55 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:07:55.865+0000][info][gc,cpu       ] GC(3) User=0.03s Sys=0.00s Real=0.06s
<13>Apr 27 18:07:55 dataproc-startup-script[1265]: <13>Apr 27 18:07:55 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:07:55.872+0000][info][gc           ] GC(3) Concurrent Preclean
<13>Apr 27 18:07:55 dataproc-startup-script[1265]: <13>Apr 27 18:07:55 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:07:55.873+0000][info][gc           ] GC(3) Concurrent Preclean 0.594ms
<13>Apr 27 18:07:55 dataproc-startup-script[1265]: <13>Apr 27 18:07:55 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:07:55.873+0000][info][gc,cpu       ] GC(3) User=0.00s Sys=0.00s Real=0.00s
<13>Apr 27 18:07:55 dataproc-startup-script[1265]: <13>Apr 27 18:07:55 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:07:55.878+0000][info][gc,start     ] GC(3) Pause Remark
<13>Apr 27 18:07:55 dataproc-startup-script[1265]: <13>Apr 27 18:07:55 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:07:55.883+0000][info][gc           ] GC(3) Pause Remark 47M->47M(121M) 4.703ms
<13>Apr 27 18:07:55 dataproc-startup-script[1265]: <13>Apr 27 18:07:55 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:07:55.883+0000][info][gc,cpu       ] GC(3) User=0.00s Sys=0.00s Real=0.00s
<13>Apr 27 18:07:55 dataproc-startup-script[1265]: <13>Apr 27 18:07:55 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:07:55.889+0000][info][gc           ] GC(3) Concurrent Sweep
<13>Apr 27 18:07:55 dataproc-startup-script[1265]: <13>Apr 27 18:07:55 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:07:55.890+0000][info][gc           ] GC(3) Concurrent Sweep 1.435ms
<13>Apr 27 18:07:55 dataproc-startup-script[1265]: <13>Apr 27 18:07:55 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:07:55.890+0000][info][gc,cpu       ] GC(3) User=0.00s Sys=0.00s Real=0.00s
<13>Apr 27 18:07:55 dataproc-startup-script[1265]: <13>Apr 27 18:07:55 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:07:55.890+0000][info][gc           ] GC(3) Concurrent Reset
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: <13>Apr 27 18:07:56 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:07:56.043+0000][info][gc           ] GC(3) Concurrent Reset 152.443ms
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: <13>Apr 27 18:07:56 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:07:56.043+0000][info][gc,cpu       ] GC(3) User=0.03s Sys=0.01s Real=0.15s
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: <13>Apr 27 18:07:56 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:07:56.043+0000][info][gc,heap      ] GC(3) Old: 45334K->45286K(86016K)
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5092.exitcode ]]
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + local status
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + status=0
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + (( status != 0 ))
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + echo 'Command cmd=[setup_service hadoop-mapreduce-historyserver] pid=5092 exited with 0'
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + tee /tmp/dataproc/commands/5092.done
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: Command cmd=[setup_service hadoop-mapreduce-historyserver] pid=5092 exited with 0
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + rm /tmp/dataproc/commands/5092.exitcode /tmp/dataproc/commands/5092.running
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + for running_file in "${COMMANDS_TMP_DIR}/"*'.running'
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + local pid
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: ++ basename /tmp/dataproc/commands/5093
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + pid=5093
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + local cmd
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + cmd='setup_service hadoop-yarn-timelineserver'
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + loginfo 'Waiting on pid=5093 cmd=[setup_service hadoop-yarn-timelineserver]'
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + echo 'Waiting on pid=5093 cmd=[setup_service hadoop-yarn-timelineserver]'
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: Waiting on pid=5093 cmd=[setup_service hadoop-yarn-timelineserver]
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + echo 'setup_service hadoop-yarn-timelineserver'
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + local exitcode_file=/tmp/dataproc/commands/5093.exitcode
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5093.exitcode ]]
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + local status
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + status=0
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + (( status != 0 ))
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + echo 'Command cmd=[setup_service hadoop-yarn-timelineserver] pid=5093 exited with 0'
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + tee /tmp/dataproc/commands/5093.done
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: Command cmd=[setup_service hadoop-yarn-timelineserver] pid=5093 exited with 0
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + rm /tmp/dataproc/commands/5093.exitcode /tmp/dataproc/commands/5093.running
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + for running_file in "${COMMANDS_TMP_DIR}/"*'.running'
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + local pid
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: ++ basename /tmp/dataproc/commands/5096
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + pid=5096
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + local cmd
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + cmd='activate_component earlyoom'
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + loginfo 'Waiting on pid=5096 cmd=[activate_component earlyoom]'
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + echo 'Waiting on pid=5096 cmd=[activate_component earlyoom]'
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: Waiting on pid=5096 cmd=[activate_component earlyoom]
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + echo 'activate_component earlyoom'
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + local exitcode_file=/tmp/dataproc/commands/5096.exitcode
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5096.exitcode ]]
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + local status
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + status=0
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + (( status != 0 ))
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + tee /tmp/dataproc/commands/5096.done
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + echo 'Command cmd=[activate_component earlyoom] pid=5096 exited with 0'
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: Command cmd=[activate_component earlyoom] pid=5096 exited with 0
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + rm /tmp/dataproc/commands/5096.exitcode /tmp/dataproc/commands/5096.running
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + for running_file in "${COMMANDS_TMP_DIR}/"*'.running'
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + local pid
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: ++ basename /tmp/dataproc/commands/5097
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + pid=5097
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + local cmd
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + cmd='activate_component fluentbit-ucp'
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + loginfo 'Waiting on pid=5097 cmd=[activate_component fluentbit-ucp]'
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + echo 'Waiting on pid=5097 cmd=[activate_component fluentbit-ucp]'
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: Waiting on pid=5097 cmd=[activate_component fluentbit-ucp]
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + echo 'activate_component fluentbit-ucp'
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + local exitcode_file=/tmp/dataproc/commands/5097.exitcode
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5097.exitcode ]]
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + local status
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + status=0
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + (( status != 0 ))
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + echo 'Command cmd=[activate_component fluentbit-ucp] pid=5097 exited with 0'
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + tee /tmp/dataproc/commands/5097.done
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: Command cmd=[activate_component fluentbit-ucp] pid=5097 exited with 0
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + rm /tmp/dataproc/commands/5097.exitcode /tmp/dataproc/commands/5097.running
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + for running_file in "${COMMANDS_TMP_DIR}/"*'.running'
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + local pid
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: ++ basename /tmp/dataproc/commands/5098
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + pid=5098
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + local cmd
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + cmd='activate_component hdfs'
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + loginfo 'Waiting on pid=5098 cmd=[activate_component hdfs]'
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + echo 'Waiting on pid=5098 cmd=[activate_component hdfs]'
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: Waiting on pid=5098 cmd=[activate_component hdfs]
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + echo 'activate_component hdfs'
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + local exitcode_file=/tmp/dataproc/commands/5098.exitcode
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5098.exitcode ]]
<13>Apr 27 18:07:56 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:07:57 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5098.exitcode ]]
<13>Apr 27 18:07:57 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:07:58 dataproc-startup-script[1265]: <13>Apr 27 18:07:58 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:07:58.072+0000][info][gc,start     ] GC(4) Pause Initial Mark
<13>Apr 27 18:07:58 dataproc-startup-script[1265]: <13>Apr 27 18:07:58 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:07:58.089+0000][info][gc           ] GC(4) Pause Initial Mark 70M->70M(121M) 16.403ms
<13>Apr 27 18:07:58 dataproc-startup-script[1265]: <13>Apr 27 18:07:58 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:07:58.089+0000][info][gc,cpu       ] GC(4) User=0.01s Sys=0.00s Real=0.02s
<13>Apr 27 18:07:58 dataproc-startup-script[1265]: <13>Apr 27 18:07:58 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:07:58.089+0000][info][gc           ] GC(4) Concurrent Mark
<13>Apr 27 18:07:58 dataproc-startup-script[1265]: <13>Apr 27 18:07:58 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:07:58.145+0000][info][gc           ] GC(4) Concurrent Mark 56.352ms
<13>Apr 27 18:07:58 dataproc-startup-script[1265]: <13>Apr 27 18:07:58 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:07:58.145+0000][info][gc,cpu       ] GC(4) User=0.02s Sys=0.00s Real=0.06s
<13>Apr 27 18:07:58 dataproc-startup-script[1265]: <13>Apr 27 18:07:58 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:07:58.145+0000][info][gc           ] GC(4) Concurrent Preclean
<13>Apr 27 18:07:58 dataproc-startup-script[1265]: <13>Apr 27 18:07:58 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:07:58.172+0000][info][gc           ] GC(4) Concurrent Preclean 27.211ms
<13>Apr 27 18:07:58 dataproc-startup-script[1265]: <13>Apr 27 18:07:58 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:07:58.172+0000][info][gc,cpu       ] GC(4) User=0.01s Sys=0.00s Real=0.02s
<13>Apr 27 18:07:58 dataproc-startup-script[1265]: <13>Apr 27 18:07:58 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:07:58.172+0000][info][gc           ] GC(4) Concurrent Abortable Preclean
<13>Apr 27 18:07:58 dataproc-startup-script[1265]: <13>Apr 27 18:07:58 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:07:58.708+0000][info][gc,start     ] GC(5) Pause Young (Allocation Failure)
<13>Apr 27 18:07:58 dataproc-startup-script[1265]: <13>Apr 27 18:07:58 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:07:58.708+0000][info][gc,task      ] GC(5) Using 2 workers of 2 for evacuation
<13>Apr 27 18:07:58 dataproc-startup-script[1265]: <13>Apr 27 18:07:58 dataproc-activate-component-hive-metastore[5099]: 'systemctl restart hive-metastore' succeeded after 1 execution(s).
<13>Apr 27 18:07:58 dataproc-startup-script[1265]: <13>Apr 27 18:07:58 dataproc-activate-component-hive-metastore[5099]: + return 0
<13>Apr 27 18:07:58 dataproc-startup-script[1265]: <13>Apr 27 18:07:58 dataproc-activate-component-hive-metastore[5099]: + wait_for_hive_metastore
<13>Apr 27 18:07:58 dataproc-startup-script[1265]: <13>Apr 27 18:07:58 dataproc-activate-component-hive-metastore[5099]: + local timeout
<13>Apr 27 18:07:58 dataproc-startup-script[1265]: <13>Apr 27 18:07:58 dataproc-activate-component-hive-metastore[5099]: ++ get_dataproc_property_or_default startup.component.service-binding-timeout.hive-metastore 300
<13>Apr 27 18:07:58 dataproc-startup-script[1265]: <13>Apr 27 18:07:58 dataproc-activate-component-hive-metastore[5099]: ++ set +x
<13>Apr 27 18:07:58 dataproc-startup-script[1265]: <13>Apr 27 18:07:58 dataproc-activate-component-hive-metastore[5099]: + timeout=300
<13>Apr 27 18:07:58 dataproc-startup-script[1265]: <13>Apr 27 18:07:58 dataproc-activate-component-hive-metastore[5099]: + local metastore_uri
<13>Apr 27 18:07:58 dataproc-startup-script[1265]: <13>Apr 27 18:07:58 dataproc-activate-component-hive-metastore[5099]: ++ get_hive_metastore_uri
<13>Apr 27 18:07:58 dataproc-startup-script[1265]: <13>Apr 27 18:07:58 dataproc-activate-component-hive-metastore[5099]: ++ local uris_str
<13>Apr 27 18:07:58 dataproc-startup-script[1265]: <13>Apr 27 18:07:58 dataproc-activate-component-hive-metastore[5099]: +++ get_property_in_xml /etc/hive/conf/hive-site.xml hive.metastore.uris
<13>Apr 27 18:07:58 dataproc-startup-script[1265]: <13>Apr 27 18:07:58 dataproc-activate-component-hive-metastore[5099]: +++ set +x
<13>Apr 27 18:07:58 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5098.exitcode ]]
<13>Apr 27 18:07:58 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:07:58 dataproc-startup-script[1265]: <13>Apr 27 18:07:58 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:07:58.811+0000][info][gc,heap      ] GC(5) ParNew: 36117K->4288K(38720K)
<13>Apr 27 18:07:58 dataproc-startup-script[1265]: <13>Apr 27 18:07:58 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:07:58.811+0000][info][gc,heap      ] GC(5) CMS: 45286K->50094K(86016K)
<13>Apr 27 18:07:58 dataproc-startup-script[1265]: <13>Apr 27 18:07:58 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:07:58.811+0000][info][gc,metaspace ] GC(5) Metaspace: 18967K(19456K)->18967K(19456K) NonClass: 17148K(17408K)->17148K(17408K) Class: 1818K(2048K)->1818K(2048K)
<13>Apr 27 18:07:58 dataproc-startup-script[1265]: <13>Apr 27 18:07:58 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:07:58.811+0000][info][gc           ] GC(5) Pause Young (Allocation Failure) 79M->53M(121M) 102.334ms
<13>Apr 27 18:07:58 dataproc-startup-script[1265]: <13>Apr 27 18:07:58 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:07:58.811+0000][info][gc,cpu       ] GC(5) User=0.03s Sys=0.00s Real=0.10s
<13>Apr 27 18:07:59 dataproc-startup-script[1265]: <13>Apr 27 18:07:59 dataproc-activate-component-hive-metastore[5099]: ++ uris_str=thrift://cluster-dip-01-m:9083
<13>Apr 27 18:07:59 dataproc-startup-script[1265]: <13>Apr 27 18:07:59 dataproc-activate-component-hive-metastore[5099]: ++ uris=('thrift://cluster-dip-01-m:9083')
<13>Apr 27 18:07:59 dataproc-startup-script[1265]: <13>Apr 27 18:07:59 dataproc-activate-component-hive-metastore[5099]: ++ local -a uris
<13>Apr 27 18:07:59 dataproc-startup-script[1265]: <13>Apr 27 18:07:59 dataproc-activate-component-hive-metastore[5099]: ++ for uri in "${uris[@]}"
<13>Apr 27 18:07:59 dataproc-startup-script[1265]: <13>Apr 27 18:07:59 dataproc-activate-component-hive-metastore[5099]: ++ [[ thrift://cluster-dip-01-m:9083 == *cluster-dip-01-m* ]]
<13>Apr 27 18:07:59 dataproc-startup-script[1265]: <13>Apr 27 18:07:59 dataproc-activate-component-hive-metastore[5099]: ++ echo thrift://cluster-dip-01-m:9083
<13>Apr 27 18:07:59 dataproc-startup-script[1265]: <13>Apr 27 18:07:59 dataproc-activate-component-hive-metastore[5099]: ++ return 0
<13>Apr 27 18:07:59 dataproc-startup-script[1265]: <13>Apr 27 18:07:59 dataproc-activate-component-hive-metastore[5099]: + metastore_uri=thrift://cluster-dip-01-m:9083
<13>Apr 27 18:07:59 dataproc-startup-script[1265]: <13>Apr 27 18:07:59 dataproc-activate-component-hive-metastore[5099]: + local host
<13>Apr 27 18:07:59 dataproc-startup-script[1265]: <13>Apr 27 18:07:59 dataproc-activate-component-hive-metastore[5099]: ++ echo thrift://cluster-dip-01-m:9083
<13>Apr 27 18:07:59 dataproc-startup-script[1265]: <13>Apr 27 18:07:59 dataproc-activate-component-hive-metastore[5099]: ++ sed -n 's#.*://\(.*\):.*#\1#p'
<13>Apr 27 18:07:59 dataproc-startup-script[1265]: <13>Apr 27 18:07:59 dataproc-activate-component-hive-metastore[5099]: + host=cluster-dip-01-m
<13>Apr 27 18:07:59 dataproc-startup-script[1265]: <13>Apr 27 18:07:59 dataproc-activate-component-hive-metastore[5099]: + [[ -z cluster-dip-01-m ]]
<13>Apr 27 18:07:59 dataproc-startup-script[1265]: <13>Apr 27 18:07:59 dataproc-activate-component-hive-metastore[5099]: + local port
<13>Apr 27 18:07:59 dataproc-startup-script[1265]: <13>Apr 27 18:07:59 dataproc-activate-component-hive-metastore[5099]: ++ echo thrift://cluster-dip-01-m:9083
<13>Apr 27 18:07:59 dataproc-startup-script[1265]: <13>Apr 27 18:07:59 dataproc-activate-component-hive-metastore[5099]: ++ sed -n 's#.*://.*:\(.*\)#\1#p'
<13>Apr 27 18:07:59 dataproc-startup-script[1265]: <13>Apr 27 18:07:59 dataproc-activate-component-hive-metastore[5099]: + port=9083
<13>Apr 27 18:07:59 dataproc-startup-script[1265]: <13>Apr 27 18:07:59 dataproc-activate-component-hive-metastore[5099]: + [[ -z 9083 ]]
<13>Apr 27 18:07:59 dataproc-startup-script[1265]: <13>Apr 27 18:07:59 dataproc-activate-component-hive-metastore[5099]: + wait_for_port hive-metastore cluster-dip-01-m 9083 300
<13>Apr 27 18:07:59 dataproc-startup-script[1265]: <13>Apr 27 18:07:59 dataproc-activate-component-hive-metastore[5099]: + local -r name=hive-metastore
<13>Apr 27 18:07:59 dataproc-startup-script[1265]: <13>Apr 27 18:07:59 dataproc-activate-component-hive-metastore[5099]: + local -r host=cluster-dip-01-m
<13>Apr 27 18:07:59 dataproc-startup-script[1265]: <13>Apr 27 18:07:59 dataproc-activate-component-hive-metastore[5099]: + local -r port=9083
<13>Apr 27 18:07:59 dataproc-startup-script[1265]: <13>Apr 27 18:07:59 dataproc-activate-component-hive-metastore[5099]: + local -r timeout=300
<13>Apr 27 18:07:59 dataproc-startup-script[1265]: <13>Apr 27 18:07:59 dataproc-activate-component-hive-metastore[5099]: + local -r capped_timeout=300
<13>Apr 27 18:07:59 dataproc-startup-script[1265]: <13>Apr 27 18:07:59 dataproc-activate-component-hive-metastore[5099]: + loginfo 'Waiting 300 seconds for service to come up on host=cluster-dip-01-m port=9083 name=hive-metastore.'
<13>Apr 27 18:07:59 dataproc-startup-script[1265]: <13>Apr 27 18:07:59 dataproc-activate-component-hive-metastore[5099]: + echo 'Waiting 300 seconds for service to come up on host=cluster-dip-01-m port=9083 name=hive-metastore.'
<13>Apr 27 18:07:59 dataproc-startup-script[1265]: <13>Apr 27 18:07:59 dataproc-activate-component-hive-metastore[5099]: Waiting 300 seconds for service to come up on host=cluster-dip-01-m port=9083 name=hive-metastore.
<13>Apr 27 18:07:59 dataproc-startup-script[1265]: <13>Apr 27 18:07:59 dataproc-activate-component-hive-metastore[5099]: + retry_constant_custom 300 1 nc -v -z -w 1 cluster-dip-01-m 9083
<13>Apr 27 18:07:59 dataproc-startup-script[1265]: <13>Apr 27 18:07:59 dataproc-activate-component-hive-metastore[5099]: + local -r max_retry_time=300
<13>Apr 27 18:07:59 dataproc-startup-script[1265]: <13>Apr 27 18:07:59 dataproc-activate-component-hive-metastore[5099]: + local -r retry_delay=1
<13>Apr 27 18:07:59 dataproc-startup-script[1265]: <13>Apr 27 18:07:59 dataproc-activate-component-hive-metastore[5099]: + cmd=('nc' '-v' '-z' '-w' '1' 'cluster-dip-01-m' '9083')
<13>Apr 27 18:07:59 dataproc-startup-script[1265]: <13>Apr 27 18:07:59 dataproc-activate-component-hive-metastore[5099]: + local -r cmd
<13>Apr 27 18:07:59 dataproc-startup-script[1265]: <13>Apr 27 18:07:59 dataproc-activate-component-hive-metastore[5099]: + local -r max_retries=300
<13>Apr 27 18:07:59 dataproc-startup-script[1265]: <13>Apr 27 18:07:59 dataproc-activate-component-hive-metastore[5099]: + local reenable_x=false
<13>Apr 27 18:07:59 dataproc-startup-script[1265]: <13>Apr 27 18:07:59 dataproc-activate-component-hive-metastore[5099]: + [[ -o xtrace ]]
<13>Apr 27 18:07:59 dataproc-startup-script[1265]: <13>Apr 27 18:07:59 dataproc-activate-component-hive-metastore[5099]: + set +x
<13>Apr 27 18:07:59 dataproc-startup-script[1265]: <13>Apr 27 18:07:59 dataproc-activate-component-hive-metastore[5099]: About to run 'nc -v -z -w 1 cluster-dip-01-m 9083' with retries...
<13>Apr 27 18:07:59 dataproc-startup-script[1265]: <13>Apr 27 18:07:59 dataproc-activate-component-hive-metastore[5099]: nc: connect to cluster-dip-01-m (10.142.0.8) port 9083 (tcp) failed: Connection refused
<13>Apr 27 18:07:59 dataproc-startup-script[1265]: <13>Apr 27 18:07:59 dataproc-activate-component-hive-metastore[5099]: 'nc -v -z -w 1 cluster-dip-01-m 9083' attempt 1/300 failed! Sleeping 1s.
<13>Apr 27 18:07:59 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5098.exitcode ]]
<13>Apr 27 18:07:59 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:08:00 dataproc-startup-script[1265]: <13>Apr 27 18:08:00 dataproc-activate-component-hive-metastore[5099]: nc: connect to cluster-dip-01-m (10.142.0.8) port 9083 (tcp) failed: Connection refused
<13>Apr 27 18:08:00 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5098.exitcode ]]
<13>Apr 27 18:08:00 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:08:00 dataproc-startup-script[1265]: <13>Apr 27 18:08:00 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:00.949+0000][info][gc           ] GC(4) Concurrent Abortable Preclean 2776.844ms
<13>Apr 27 18:08:00 dataproc-startup-script[1265]: <13>Apr 27 18:08:00 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:00.949+0000][info][gc,cpu       ] GC(4) User=0.61s Sys=0.01s Real=2.78s
<13>Apr 27 18:08:00 dataproc-startup-script[1265]: <13>Apr 27 18:08:00 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:00.962+0000][info][gc,start     ] GC(4) Pause Remark
<13>Apr 27 18:08:01 dataproc-startup-script[1265]: <13>Apr 27 18:08:01 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:00.999+0000][info][gc           ] GC(4) Pause Remark 74M->74M(121M) 37.339ms
<13>Apr 27 18:08:01 dataproc-startup-script[1265]: <13>Apr 27 18:08:01 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:00.999+0000][info][gc,cpu       ] GC(4) User=0.01s Sys=0.00s Real=0.04s
<13>Apr 27 18:08:01 dataproc-startup-script[1265]: <13>Apr 27 18:08:01 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:00.999+0000][info][gc           ] GC(4) Concurrent Sweep
<13>Apr 27 18:08:01 dataproc-startup-script[1265]: <13>Apr 27 18:08:01 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:01.002+0000][info][gc           ] GC(4) Concurrent Sweep 2.850ms
<13>Apr 27 18:08:01 dataproc-startup-script[1265]: <13>Apr 27 18:08:01 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:01.002+0000][info][gc,cpu       ] GC(4) User=0.00s Sys=0.00s Real=0.00s
<13>Apr 27 18:08:01 dataproc-startup-script[1265]: <13>Apr 27 18:08:01 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:01.002+0000][info][gc           ] GC(4) Concurrent Reset
<13>Apr 27 18:08:01 dataproc-startup-script[1265]: <13>Apr 27 18:08:01 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:01.041+0000][info][gc           ] GC(4) Concurrent Reset 38.759ms
<13>Apr 27 18:08:01 dataproc-startup-script[1265]: <13>Apr 27 18:08:01 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:01.041+0000][info][gc,cpu       ] GC(4) User=0.02s Sys=0.00s Real=0.04s
<13>Apr 27 18:08:01 dataproc-startup-script[1265]: <13>Apr 27 18:08:01 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:01.041+0000][info][gc,heap      ] GC(4) Old: 45286K->50069K(86016K)
<13>Apr 27 18:08:01 dataproc-startup-script[1265]: <13>Apr 27 18:08:01 dataproc-activate-component-hive-metastore[5099]: nc: connect to cluster-dip-01-m (10.142.0.8) port 9083 (tcp) failed: Connection refused
<13>Apr 27 18:08:01 dataproc-startup-script[1265]: <13>Apr 27 18:08:01 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:01.803+0000][info][gc,start     ] GC(6) Pause Young (Allocation Failure)
<13>Apr 27 18:08:01 dataproc-startup-script[1265]: <13>Apr 27 18:08:01 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:01.803+0000][info][gc,task      ] GC(6) Using 2 workers of 2 for evacuation
<13>Apr 27 18:08:01 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5098.exitcode ]]
<13>Apr 27 18:08:01 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:08:01 dataproc-startup-script[1265]: <13>Apr 27 18:08:01 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:01.865+0000][info][gc,heap      ] GC(6) ParNew: 38720K->2246K(38720K)
<13>Apr 27 18:08:01 dataproc-startup-script[1265]: <13>Apr 27 18:08:01 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:01.865+0000][info][gc,heap      ] GC(6) CMS: 50069K->52102K(86016K)
<13>Apr 27 18:08:01 dataproc-startup-script[1265]: <13>Apr 27 18:08:01 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:01.865+0000][info][gc,metaspace ] GC(6) Metaspace: 20106K(20608K)->20106K(20608K) NonClass: 18105K(18432K)->18105K(18432K) Class: 2000K(2176K)->2000K(2176K)
<13>Apr 27 18:08:01 dataproc-startup-script[1265]: <13>Apr 27 18:08:01 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:01.865+0000][info][gc           ] GC(6) Pause Young (Allocation Failure) 86M->53M(121M) 61.552ms
<13>Apr 27 18:08:01 dataproc-startup-script[1265]: <13>Apr 27 18:08:01 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:01.865+0000][info][gc,cpu       ] GC(6) User=0.01s Sys=0.00s Real=0.07s
<13>Apr 27 18:08:02 dataproc-startup-script[1265]: <13>Apr 27 18:08:02 dataproc-activate-component-hive-metastore[5099]: nc: connect to cluster-dip-01-m (10.142.0.8) port 9083 (tcp) failed: Connection refused
<13>Apr 27 18:08:02 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5098.exitcode ]]
<13>Apr 27 18:08:02 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:08:03 dataproc-startup-script[1265]: <13>Apr 27 18:08:03 dataproc-activate-component-hive-metastore[5099]: nc: connect to cluster-dip-01-m (10.142.0.8) port 9083 (tcp) failed: Connection refused
<13>Apr 27 18:08:03 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5098.exitcode ]]
<13>Apr 27 18:08:03 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:08:04 dataproc-startup-script[1265]: <13>Apr 27 18:08:04 dataproc-activate-component-hive-metastore[5099]: nc: connect to cluster-dip-01-m (10.142.0.8) port 9083 (tcp) failed: Connection refused
<13>Apr 27 18:08:04 dataproc-startup-script[1265]: <13>Apr 27 18:08:04 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:04,224 INFO common.Util: Assuming 'file' scheme for path /hadoop/dfs/name in configuration.
<13>Apr 27 18:08:04 dataproc-startup-script[1265]: <13>Apr 27 18:08:04 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:04,233 INFO common.Util: Assuming 'file' scheme for path /hadoop/dfs/name in configuration.
<13>Apr 27 18:08:04 dataproc-startup-script[1265]: <13>Apr 27 18:08:04 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:04,299 INFO namenode.NameNode: Formatting using clusterid: CID-434cbe11-9a51-4c5e-b528-0b2a1baf5569
<13>Apr 27 18:08:04 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5098.exitcode ]]
<13>Apr 27 18:08:04 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:08:04 dataproc-startup-script[1265]: <13>Apr 27 18:08:04 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:04,838 INFO namenode.FSEditLog: Edit logging is async:true
<13>Apr 27 18:08:05 dataproc-startup-script[1265]: <13>Apr 27 18:08:05 dataproc-activate-component-hive-metastore[5099]: nc: connect to cluster-dip-01-m (10.142.0.8) port 9083 (tcp) failed: Connection refused
<13>Apr 27 18:08:05 dataproc-startup-script[1265]: <13>Apr 27 18:08:05 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:05,376 INFO namenode.FSNamesystem: KeyProvider: null
<13>Apr 27 18:08:05 dataproc-startup-script[1265]: <13>Apr 27 18:08:05 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:05,378 INFO namenode.FSNamesystem: fsLock is fair: true
<13>Apr 27 18:08:05 dataproc-startup-script[1265]: <13>Apr 27 18:08:05 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:05,378 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
<13>Apr 27 18:08:05 dataproc-startup-script[1265]: <13>Apr 27 18:08:05 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:05,610 INFO namenode.FSNamesystem: fsOwner                = hdfs (auth:SIMPLE)
<13>Apr 27 18:08:05 dataproc-startup-script[1265]: <13>Apr 27 18:08:05 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:05,613 INFO namenode.FSNamesystem: supergroup             = hadoop
<13>Apr 27 18:08:05 dataproc-startup-script[1265]: <13>Apr 27 18:08:05 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:05,613 INFO namenode.FSNamesystem: isPermissionEnabled    = false
<13>Apr 27 18:08:05 dataproc-startup-script[1265]: <13>Apr 27 18:08:05 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:05,613 INFO namenode.FSNamesystem: isStoragePolicyEnabled = true
<13>Apr 27 18:08:05 dataproc-startup-script[1265]: <13>Apr 27 18:08:05 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:05,614 INFO namenode.FSNamesystem: HA Enabled: false
<13>Apr 27 18:08:05 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5098.exitcode ]]
<13>Apr 27 18:08:05 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:08:06 dataproc-startup-script[1265]: <13>Apr 27 18:08:06 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:06.120+0000][info][gc,start     ] GC(7) Pause Young (Allocation Failure)
<13>Apr 27 18:08:06 dataproc-startup-script[1265]: <13>Apr 27 18:08:06 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:06.120+0000][info][gc,task      ] GC(7) Using 2 workers of 2 for evacuation
<13>Apr 27 18:08:06 dataproc-startup-script[1265]: <13>Apr 27 18:08:06 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:06.132+0000][info][gc,heap      ] GC(7) ParNew: 36678K->3515K(38720K)
<13>Apr 27 18:08:06 dataproc-startup-script[1265]: <13>Apr 27 18:08:06 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:06.132+0000][info][gc,heap      ] GC(7) CMS: 52102K->52102K(86016K)
<13>Apr 27 18:08:06 dataproc-startup-script[1265]: <13>Apr 27 18:08:06 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:06.132+0000][info][gc,metaspace ] GC(7) Metaspace: 23517K(24064K)->23517K(24064K) NonClass: 21171K(21504K)->21171K(21504K) Class: 2346K(2560K)->2346K(2560K)
<13>Apr 27 18:08:06 dataproc-startup-script[1265]: <13>Apr 27 18:08:06 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:06.132+0000][info][gc           ] GC(7) Pause Young (Allocation Failure) 86M->54M(121M) 11.517ms
<13>Apr 27 18:08:06 dataproc-startup-script[1265]: <13>Apr 27 18:08:06 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:06.132+0000][info][gc,cpu       ] GC(7) User=0.01s Sys=0.00s Real=0.01s
<13>Apr 27 18:08:06 dataproc-startup-script[1265]: <13>Apr 27 18:08:06 dataproc-activate-component-hive-metastore[5099]: nc: connect to cluster-dip-01-m (10.142.0.8) port 9083 (tcp) failed: Connection refused
<13>Apr 27 18:08:06 dataproc-startup-script[1265]: <13>Apr 27 18:08:06 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:06,237 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
<13>Apr 27 18:08:06 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5098.exitcode ]]
<13>Apr 27 18:08:06 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:08:07 dataproc-startup-script[1265]: <13>Apr 27 18:08:07 dataproc-activate-component-hive-metastore[5099]: nc: connect to cluster-dip-01-m (10.142.0.8) port 9083 (tcp) failed: Connection refused
<13>Apr 27 18:08:07 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5098.exitcode ]]
<13>Apr 27 18:08:07 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:08:08 dataproc-startup-script[1265]: <13>Apr 27 18:08:08 dataproc-activate-component-hive-metastore[5099]: nc: connect to cluster-dip-01-m (10.142.0.8) port 9083 (tcp) failed: Connection refused
<13>Apr 27 18:08:08 dataproc-startup-script[1265]: <13>Apr 27 18:08:08 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:08.685+0000][info][gc,start     ] GC(8) Pause Young (Allocation Failure)
<13>Apr 27 18:08:08 dataproc-startup-script[1265]: <13>Apr 27 18:08:08 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:08.685+0000][info][gc,task      ] GC(8) Using 2 workers of 2 for evacuation
<13>Apr 27 18:08:08 dataproc-startup-script[1265]: <13>Apr 27 18:08:08 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:08.729+0000][info][gc,heap      ] GC(8) ParNew: 37947K->2646K(38720K)
<13>Apr 27 18:08:08 dataproc-startup-script[1265]: <13>Apr 27 18:08:08 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:08.729+0000][info][gc,heap      ] GC(8) CMS: 52102K->53694K(86016K)
<13>Apr 27 18:08:08 dataproc-startup-script[1265]: <13>Apr 27 18:08:08 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:08.729+0000][info][gc,metaspace ] GC(8) Metaspace: 27736K(28288K)->27736K(28288K) NonClass: 24938K(25344K)->24938K(25344K) Class: 2798K(2944K)->2798K(2944K)
<13>Apr 27 18:08:08 dataproc-startup-script[1265]: <13>Apr 27 18:08:08 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:08.729+0000][info][gc           ] GC(8) Pause Young (Allocation Failure) 87M->55M(121M) 44.089ms
<13>Apr 27 18:08:08 dataproc-startup-script[1265]: <13>Apr 27 18:08:08 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:08.729+0000][info][gc,cpu       ] GC(8) User=0.02s Sys=0.00s Real=0.04s
<13>Apr 27 18:08:08 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5098.exitcode ]]
<13>Apr 27 18:08:08 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:08:09 dataproc-startup-script[1265]: <13>Apr 27 18:08:09 dataproc-activate-component-hive-metastore[5099]: nc: connect to cluster-dip-01-m (10.142.0.8) port 9083 (tcp) failed: Connection refused
<13>Apr 27 18:08:09 dataproc-startup-script[1265]: <13>Apr 27 18:08:09 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:09.756+0000][info][gc,start     ] GC(9) Pause Young (Allocation Failure)
<13>Apr 27 18:08:09 dataproc-startup-script[1265]: <13>Apr 27 18:08:09 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:09.756+0000][info][gc,task      ] GC(9) Using 2 workers of 2 for evacuation
<13>Apr 27 18:08:09 dataproc-startup-script[1265]: <13>Apr 27 18:08:09 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:09.774+0000][info][gc,heap      ] GC(9) ParNew: 37078K->2955K(38720K)
<13>Apr 27 18:08:09 dataproc-startup-script[1265]: <13>Apr 27 18:08:09 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:09.774+0000][info][gc,heap      ] GC(9) CMS: 53694K->53694K(86016K)
<13>Apr 27 18:08:09 dataproc-startup-script[1265]: <13>Apr 27 18:08:09 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:09.774+0000][info][gc,metaspace ] GC(9) Metaspace: 29257K(29824K)->29257K(29824K) NonClass: 26264K(26624K)->26264K(26624K) Class: 2993K(3200K)->2993K(3200K)
<13>Apr 27 18:08:09 dataproc-startup-script[1265]: <13>Apr 27 18:08:09 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:09.775+0000][info][gc           ] GC(9) Pause Young (Allocation Failure) 88M->55M(121M) 18.674ms
<13>Apr 27 18:08:09 dataproc-startup-script[1265]: <13>Apr 27 18:08:09 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:09.775+0000][info][gc,cpu       ] GC(9) User=0.01s Sys=0.00s Real=0.02s
<13>Apr 27 18:08:09 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5098.exitcode ]]
<13>Apr 27 18:08:09 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:08:10 dataproc-startup-script[1265]: <13>Apr 27 18:08:10 dataproc-activate-component-hive-metastore[5099]: nc: connect to cluster-dip-01-m (10.142.0.8) port 9083 (tcp) failed: Connection refused
<13>Apr 27 18:08:10 dataproc-startup-script[1265]: <13>Apr 27 18:08:10 dataproc-activate-component-hive-metastore[5099]: 'nc -v -z -w 1 cluster-dip-01-m 9083' attempt 12/300 failed! Sleeping 1s.
<13>Apr 27 18:08:10 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5098.exitcode ]]
<13>Apr 27 18:08:10 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:08:11 dataproc-startup-script[1265]: <13>Apr 27 18:08:11 dataproc-activate-component-hive-metastore[5099]: nc: connect to cluster-dip-01-m (10.142.0.8) port 9083 (tcp) failed: Connection refused
<13>Apr 27 18:08:11 dataproc-startup-script[1265]: <13>Apr 27 18:08:11 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:11,352 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
<13>Apr 27 18:08:11 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5098.exitcode ]]
<13>Apr 27 18:08:11 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:08:12 dataproc-startup-script[1265]: <13>Apr 27 18:08:12 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:12,148 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
<13>Apr 27 18:08:12 dataproc-startup-script[1265]: <13>Apr 27 18:08:12 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:12,148 INFO impl.MetricsSystemImpl: google-hadoop-file-system metrics system started
<13>Apr 27 18:08:12 dataproc-startup-script[1265]: <13>Apr 27 18:08:12 dataproc-activate-component-hive-metastore[5099]: nc: connect to cluster-dip-01-m (10.142.0.8) port 9083 (tcp) failed: Connection refused
<13>Apr 27 18:08:12 dataproc-startup-script[1265]: <13>Apr 27 18:08:12 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:12.380+0000][info][gc,start     ] GC(10) Pause Young (Allocation Failure)
<13>Apr 27 18:08:12 dataproc-startup-script[1265]: <13>Apr 27 18:08:12 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:12.380+0000][info][gc,task      ] GC(10) Using 2 workers of 2 for evacuation
<13>Apr 27 18:08:12 dataproc-startup-script[1265]: <13>Apr 27 18:08:12 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:12.394+0000][info][gc,heap      ] GC(10) ParNew: 37387K->3854K(38720K)
<13>Apr 27 18:08:12 dataproc-startup-script[1265]: <13>Apr 27 18:08:12 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:12.394+0000][info][gc,heap      ] GC(10) CMS: 53694K->53694K(86016K)
<13>Apr 27 18:08:12 dataproc-startup-script[1265]: <13>Apr 27 18:08:12 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:12.394+0000][info][gc,metaspace ] GC(10) Metaspace: 33381K(34124K)->33381K(34124K) NonClass: 29877K(30412K)->29877K(30412K) Class: 3503K(3712K)->3503K(3712K)
<13>Apr 27 18:08:12 dataproc-startup-script[1265]: <13>Apr 27 18:08:12 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:12.394+0000][info][gc           ] GC(10) Pause Young (Allocation Failure) 88M->56M(121M) 13.485ms
<13>Apr 27 18:08:12 dataproc-startup-script[1265]: <13>Apr 27 18:08:12 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:12.394+0000][info][gc,cpu       ] GC(10) User=0.01s Sys=0.00s Real=0.01s
<13>Apr 27 18:08:12 dataproc-startup-script[1265]: <13>Apr 27 18:08:12 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:12.395+0000][info][gc,start     ] GC(11) Pause Initial Mark
<13>Apr 27 18:08:12 dataproc-startup-script[1265]: <13>Apr 27 18:08:12 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:12.401+0000][info][gc           ] GC(11) Pause Initial Mark 56M->56M(121M) 6.295ms
<13>Apr 27 18:08:12 dataproc-startup-script[1265]: <13>Apr 27 18:08:12 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:12.401+0000][info][gc,cpu       ] GC(11) User=0.00s Sys=0.00s Real=0.01s
<13>Apr 27 18:08:12 dataproc-startup-script[1265]: <13>Apr 27 18:08:12 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:12.401+0000][info][gc           ] GC(11) Concurrent Mark
<13>Apr 27 18:08:12 dataproc-startup-script[1265]: <13>Apr 27 18:08:12 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:12.525+0000][info][gc           ] GC(11) Concurrent Mark 123.935ms
<13>Apr 27 18:08:12 dataproc-startup-script[1265]: <13>Apr 27 18:08:12 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:12.525+0000][info][gc,cpu       ] GC(11) User=0.04s Sys=0.00s Real=0.13s
<13>Apr 27 18:08:12 dataproc-startup-script[1265]: <13>Apr 27 18:08:12 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:12.525+0000][info][gc           ] GC(11) Concurrent Preclean
<13>Apr 27 18:08:12 dataproc-startup-script[1265]: <13>Apr 27 18:08:12 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:12.534+0000][info][gc           ] GC(11) Concurrent Preclean 8.109ms
<13>Apr 27 18:08:12 dataproc-startup-script[1265]: <13>Apr 27 18:08:12 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:12.534+0000][info][gc,cpu       ] GC(11) User=0.00s Sys=0.00s Real=0.00s
<13>Apr 27 18:08:12 dataproc-startup-script[1265]: <13>Apr 27 18:08:12 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:12.544+0000][info][gc,start     ] GC(11) Pause Remark
<13>Apr 27 18:08:12 dataproc-startup-script[1265]: <13>Apr 27 18:08:12 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:12.576+0000][info][gc           ] GC(11) Pause Remark 57M->57M(121M) 31.638ms
<13>Apr 27 18:08:12 dataproc-startup-script[1265]: <13>Apr 27 18:08:12 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:12.576+0000][info][gc,cpu       ] GC(11) User=0.01s Sys=0.00s Real=0.02s
<13>Apr 27 18:08:12 dataproc-startup-script[1265]: <13>Apr 27 18:08:12 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:12.579+0000][info][gc           ] GC(11) Concurrent Sweep
<13>Apr 27 18:08:12 dataproc-startup-script[1265]: <13>Apr 27 18:08:12 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:12.598+0000][info][gc           ] GC(11) Concurrent Sweep 18.798ms
<13>Apr 27 18:08:12 dataproc-startup-script[1265]: <13>Apr 27 18:08:12 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:12.598+0000][info][gc,cpu       ] GC(11) User=0.02s Sys=0.00s Real=0.02s
<13>Apr 27 18:08:12 dataproc-startup-script[1265]: <13>Apr 27 18:08:12 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:12.598+0000][info][gc           ] GC(11) Concurrent Reset
<13>Apr 27 18:08:12 dataproc-startup-script[1265]: <13>Apr 27 18:08:12 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:12.630+0000][info][gc           ] GC(11) Concurrent Reset 32.146ms
<13>Apr 27 18:08:12 dataproc-startup-script[1265]: <13>Apr 27 18:08:12 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:12.630+0000][info][gc,cpu       ] GC(11) User=0.01s Sys=0.00s Real=0.03s
<13>Apr 27 18:08:12 dataproc-startup-script[1265]: <13>Apr 27 18:08:12 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:12.630+0000][info][gc,heap      ] GC(11) Old: 53694K->51162K(86016K)
<13>Apr 27 18:08:12 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5098.exitcode ]]
<13>Apr 27 18:08:12 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:08:13 dataproc-startup-script[1265]: <13>Apr 27 18:08:13 dataproc-activate-component-hive-metastore[5099]: nc: connect to cluster-dip-01-m (10.142.0.8) port 9083 (tcp) failed: Connection refused
<13>Apr 27 18:08:13 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5098.exitcode ]]
<13>Apr 27 18:08:13 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:08:14 dataproc-startup-script[1265]: <13>Apr 27 18:08:14 dataproc-activate-component-hive-metastore[5099]: nc: connect to cluster-dip-01-m (10.142.0.8) port 9083 (tcp) failed: Connection refused
<13>Apr 27 18:08:14 dataproc-startup-script[1265]: <13>Apr 27 18:08:14 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:14.755+0000][info][gc,start     ] GC(12) Pause Young (Allocation Failure)
<13>Apr 27 18:08:14 dataproc-startup-script[1265]: <13>Apr 27 18:08:14 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:14.755+0000][info][gc,task      ] GC(12) Using 2 workers of 2 for evacuation
<13>Apr 27 18:08:14 dataproc-startup-script[1265]: <13>Apr 27 18:08:14 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:14.812+0000][info][gc,heap      ] GC(12) ParNew: 38286K->4106K(38720K)
<13>Apr 27 18:08:14 dataproc-startup-script[1265]: <13>Apr 27 18:08:14 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:14.812+0000][info][gc,heap      ] GC(12) CMS: 51162K->52522K(86016K)
<13>Apr 27 18:08:14 dataproc-startup-script[1265]: <13>Apr 27 18:08:14 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:14.812+0000][info][gc,metaspace ] GC(12) Metaspace: 34972K(35660K)->34972K(35660K) NonClass: 31272K(31692K)->31272K(31692K) Class: 3700K(3968K)->3700K(3968K)
<13>Apr 27 18:08:14 dataproc-startup-script[1265]: <13>Apr 27 18:08:14 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:14.812+0000][info][gc           ] GC(12) Pause Young (Allocation Failure) 87M->55M(121M) 57.691ms
<13>Apr 27 18:08:14 dataproc-startup-script[1265]: <13>Apr 27 18:08:14 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:14.812+0000][info][gc,cpu       ] GC(12) User=0.01s Sys=0.00s Real=0.06s
<13>Apr 27 18:08:14 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5098.exitcode ]]
<13>Apr 27 18:08:14 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:08:15 dataproc-startup-script[1265]: <13>Apr 27 18:08:15 dataproc-activate-component-hive-metastore[5099]: nc: connect to cluster-dip-01-m (10.142.0.8) port 9083 (tcp) failed: Connection refused
<13>Apr 27 18:08:15 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5098.exitcode ]]
<13>Apr 27 18:08:15 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:08:16 dataproc-startup-script[1265]: <13>Apr 27 18:08:16 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:16.066+0000][info][gc,start     ] GC(13) Pause Young (Allocation Failure)
<13>Apr 27 18:08:16 dataproc-startup-script[1265]: <13>Apr 27 18:08:16 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:16.066+0000][info][gc,task      ] GC(13) Using 2 workers of 2 for evacuation
<13>Apr 27 18:08:16 dataproc-startup-script[1265]: <13>Apr 27 18:08:16 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:16.074+0000][info][gc,heap      ] GC(13) ParNew: 38538K->2921K(38720K)
<13>Apr 27 18:08:16 dataproc-startup-script[1265]: <13>Apr 27 18:08:16 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:16.074+0000][info][gc,heap      ] GC(13) CMS: 52522K->52522K(86016K)
<13>Apr 27 18:08:16 dataproc-startup-script[1265]: <13>Apr 27 18:08:16 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:16.074+0000][info][gc,metaspace ] GC(13) Metaspace: 36076K(36940K)->36076K(36940K) NonClass: 32211K(32716K)->32211K(32716K) Class: 3865K(4224K)->3865K(4224K)
<13>Apr 27 18:08:16 dataproc-startup-script[1265]: <13>Apr 27 18:08:16 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:16.074+0000][info][gc           ] GC(13) Pause Young (Allocation Failure) 88M->54M(121M) 8.525ms
<13>Apr 27 18:08:16 dataproc-startup-script[1265]: <13>Apr 27 18:08:16 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:16.074+0000][info][gc,cpu       ] GC(13) User=0.00s Sys=0.00s Real=0.00s
<13>Apr 27 18:08:16 dataproc-startup-script[1265]: <13>Apr 27 18:08:16 dataproc-activate-component-hive-metastore[5099]: nc: connect to cluster-dip-01-m (10.142.0.8) port 9083 (tcp) failed: Connection refused
<13>Apr 27 18:08:16 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5098.exitcode ]]
<13>Apr 27 18:08:16 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:08:17 dataproc-startup-script[1265]: <13>Apr 27 18:08:17 dataproc-activate-component-hive-metastore[5099]: nc: connect to cluster-dip-01-m (10.142.0.8) port 9083 (tcp) failed: Connection refused
<13>Apr 27 18:08:17 dataproc-startup-script[1265]: <13>Apr 27 18:08:17 dataproc-activate-component-hdfs[5098]: Apr 27, 2025 6:08:17 PM com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.RequestTracker stopTracking
<13>Apr 27 18:08:17 dataproc-startup-script[1265]: <13>Apr 27 18:08:17 dataproc-activate-component-hdfs[5098]: INFO: Detected high latency for [url=https://storage.googleapis.com/storage/v1/b/dataproc-staging-us-east1-679657336577-kutp8kah/o/google-cloud-dataproc-metainfo%2F0b63cfa9-1f8a-481e-87c8-a0cf661f3625%2Fnodes_include?fields=bucket,name,timeCreated,updated,generation,metageneration,size,contentType,contentEncoding,md5Hash,crc32c,metadata; invocationId=gl-java/11.0.20 gdcl/2.1.1 linux/6.1.0 gccl-invocation-id/bc088610-c879-4521-96a8-9cbf372dada1]. durationMs=626; method=GET; thread=main [CONTEXT ratelimit_period="10 SECONDS" ]
<13>Apr 27 18:08:17 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5098.exitcode ]]
<13>Apr 27 18:08:17 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:08:18 dataproc-startup-script[1265]: <13>Apr 27 18:08:18 dataproc-activate-component-hive-metastore[5099]: nc: connect to cluster-dip-01-m (10.142.0.8) port 9083 (tcp) failed: Connection refused
<13>Apr 27 18:08:18 dataproc-startup-script[1265]: <13>Apr 27 18:08:18 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:18,802 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit : configured=1000, counted=60, effected=1000
<13>Apr 27 18:08:18 dataproc-startup-script[1265]: <13>Apr 27 18:08:18 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:18,823 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=false
<13>Apr 27 18:08:18 dataproc-startup-script[1265]: <13>Apr 27 18:08:18 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:18,846 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
<13>Apr 27 18:08:18 dataproc-startup-script[1265]: <13>Apr 27 18:08:18 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:18,866 INFO blockmanagement.BlockManager: The block deletion will start around 2025 Apr 27 18:08:18
<13>Apr 27 18:08:18 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5098.exitcode ]]
<13>Apr 27 18:08:18 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:08:18 dataproc-startup-script[1265]: <13>Apr 27 18:08:18 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:18,875 INFO util.GSet: Computing capacity for map BlocksMap
<13>Apr 27 18:08:18 dataproc-startup-script[1265]: <13>Apr 27 18:08:18 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:18,875 INFO util.GSet: VM type       = 64-bit
<13>Apr 27 18:08:18 dataproc-startup-script[1265]: <13>Apr 27 18:08:18 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:18,902 INFO util.GSet: 2.0% max memory 1.5 GB = 31.5 MB
<13>Apr 27 18:08:18 dataproc-startup-script[1265]: <13>Apr 27 18:08:18 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:18,902 INFO util.GSet: capacity      = 2^22 = 4194304 entries
<13>Apr 27 18:08:18 dataproc-startup-script[1265]: <13>Apr 27 18:08:18 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:18.911+0000][info][gc,start     ] GC(14) Pause Young (Allocation Failure)
<13>Apr 27 18:08:18 dataproc-startup-script[1265]: <13>Apr 27 18:08:18 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:18.911+0000][info][gc,task      ] GC(14) Using 2 workers of 2 for evacuation
<13>Apr 27 18:08:18 dataproc-startup-script[1265]: <13>Apr 27 18:08:18 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:18.933+0000][info][gc,heap      ] GC(14) ParNew: 36285K->3571K(38720K)
<13>Apr 27 18:08:18 dataproc-startup-script[1265]: <13>Apr 27 18:08:18 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:18.933+0000][info][gc,heap      ] GC(14) CMS: 52522K->53071K(86016K)
<13>Apr 27 18:08:18 dataproc-startup-script[1265]: <13>Apr 27 18:08:18 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:18.933+0000][info][gc,metaspace ] GC(14) Metaspace: 39712K(40652K)->39712K(40652K) NonClass: 35424K(36044K)->35424K(36044K) Class: 4287K(4608K)->4287K(4608K)
<13>Apr 27 18:08:18 dataproc-startup-script[1265]: <13>Apr 27 18:08:18 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:18.933+0000][info][gc           ] GC(14) Pause Young (Allocation Failure) 86M->55M(121M) 22.955ms
<13>Apr 27 18:08:18 dataproc-startup-script[1265]: <13>Apr 27 18:08:18 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:18.934+0000][info][gc,cpu       ] GC(14) User=0.01s Sys=0.00s Real=0.02s
<13>Apr 27 18:08:19 dataproc-startup-script[1265]: <13>Apr 27 18:08:19 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:19,039 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled
<13>Apr 27 18:08:19 dataproc-startup-script[1265]: <13>Apr 27 18:08:19 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:19,043 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false
<13>Apr 27 18:08:19 dataproc-startup-script[1265]: <13>Apr 27 18:08:19 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:19,099 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.999
<13>Apr 27 18:08:19 dataproc-startup-script[1265]: <13>Apr 27 18:08:19 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:19,123 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
<13>Apr 27 18:08:19 dataproc-startup-script[1265]: <13>Apr 27 18:08:19 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:19,124 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
<13>Apr 27 18:08:19 dataproc-startup-script[1265]: <13>Apr 27 18:08:19 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:19,129 INFO blockmanagement.BlockManager: defaultReplication         = 2
<13>Apr 27 18:08:19 dataproc-startup-script[1265]: <13>Apr 27 18:08:19 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:19,132 INFO blockmanagement.BlockManager: maxReplication             = 512
<13>Apr 27 18:08:19 dataproc-startup-script[1265]: <13>Apr 27 18:08:19 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:19,133 INFO blockmanagement.BlockManager: minReplication             = 1
<13>Apr 27 18:08:19 dataproc-startup-script[1265]: <13>Apr 27 18:08:19 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:19,133 INFO blockmanagement.BlockManager: maxReplicationStreams      = 20
<13>Apr 27 18:08:19 dataproc-startup-script[1265]: <13>Apr 27 18:08:19 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:19,134 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms
<13>Apr 27 18:08:19 dataproc-startup-script[1265]: <13>Apr 27 18:08:19 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:19,136 INFO blockmanagement.BlockManager: encryptDataTransfer        = false
<13>Apr 27 18:08:19 dataproc-startup-script[1265]: <13>Apr 27 18:08:19 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:19,139 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
<13>Apr 27 18:08:19 dataproc-startup-script[1265]: <13>Apr 27 18:08:19 dataproc-activate-component-hive-metastore[5099]: nc: connect to cluster-dip-01-m (10.142.0.8) port 9083 (tcp) failed: Connection refused
<13>Apr 27 18:08:19 dataproc-startup-script[1265]: <13>Apr 27 18:08:19 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:19,445 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911
<13>Apr 27 18:08:19 dataproc-startup-script[1265]: <13>Apr 27 18:08:19 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:19,447 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215
<13>Apr 27 18:08:19 dataproc-startup-script[1265]: <13>Apr 27 18:08:19 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:19,448 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215
<13>Apr 27 18:08:19 dataproc-startup-script[1265]: <13>Apr 27 18:08:19 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:19,449 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215
<13>Apr 27 18:08:19 dataproc-startup-script[1265]: <13>Apr 27 18:08:19 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:19,575 INFO util.GSet: Computing capacity for map INodeMap
<13>Apr 27 18:08:19 dataproc-startup-script[1265]: <13>Apr 27 18:08:19 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:19,580 INFO util.GSet: VM type       = 64-bit
<13>Apr 27 18:08:19 dataproc-startup-script[1265]: <13>Apr 27 18:08:19 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:19,581 INFO util.GSet: 1.0% max memory 1.5 GB = 15.7 MB
<13>Apr 27 18:08:19 dataproc-startup-script[1265]: <13>Apr 27 18:08:19 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:19,585 INFO util.GSet: capacity      = 2^21 = 2097152 entries
<13>Apr 27 18:08:19 dataproc-startup-script[1265]: <13>Apr 27 18:08:19 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:19,591 INFO namenode.FSDirectory: ACLs enabled? true
<13>Apr 27 18:08:19 dataproc-startup-script[1265]: <13>Apr 27 18:08:19 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:19,595 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true
<13>Apr 27 18:08:19 dataproc-startup-script[1265]: <13>Apr 27 18:08:19 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:19,596 INFO namenode.FSDirectory: XAttrs enabled? true
<13>Apr 27 18:08:19 dataproc-startup-script[1265]: <13>Apr 27 18:08:19 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:19,596 INFO namenode.NameNode: Caching file names occurring more than 10 times
<13>Apr 27 18:08:19 dataproc-startup-script[1265]: <13>Apr 27 18:08:19 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:19,651 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
<13>Apr 27 18:08:19 dataproc-startup-script[1265]: <13>Apr 27 18:08:19 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:19,652 INFO snapshot.SnapshotManager: SkipList is disabled
<13>Apr 27 18:08:19 dataproc-startup-script[1265]: <13>Apr 27 18:08:19 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:19,758 INFO util.GSet: Computing capacity for map cachedBlocks
<13>Apr 27 18:08:19 dataproc-startup-script[1265]: <13>Apr 27 18:08:19 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:19,776 INFO util.GSet: VM type       = 64-bit
<13>Apr 27 18:08:19 dataproc-startup-script[1265]: <13>Apr 27 18:08:19 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:19,778 INFO util.GSet: 0.25% max memory 1.5 GB = 3.9 MB
<13>Apr 27 18:08:19 dataproc-startup-script[1265]: <13>Apr 27 18:08:19 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:19,784 INFO util.GSet: capacity      = 2^19 = 524288 entries
<13>Apr 27 18:08:19 dataproc-startup-script[1265]: <13>Apr 27 18:08:19 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:19.785+0000][info][gc,start     ] GC(15) Pause Young (Allocation Failure)
<13>Apr 27 18:08:19 dataproc-startup-script[1265]: <13>Apr 27 18:08:19 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:19.785+0000][info][gc,task      ] GC(15) Using 2 workers of 2 for evacuation
<13>Apr 27 18:08:19 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5098.exitcode ]]
<13>Apr 27 18:08:19 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:08:20 dataproc-startup-script[1265]: <13>Apr 27 18:08:20 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:20.036+0000][info][gc,heap      ] GC(15) ParNew: 37758K->4288K(38720K)
<13>Apr 27 18:08:20 dataproc-startup-script[1265]: <13>Apr 27 18:08:20 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:20.037+0000][info][gc,heap      ] GC(15) CMS: 53071K->79149K(86016K)
<13>Apr 27 18:08:20 dataproc-startup-script[1265]: <13>Apr 27 18:08:20 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:20.037+0000][info][gc,metaspace ] GC(15) Metaspace: 40567K(41548K)->40567K(41548K) NonClass: 36192K(36812K)->36192K(36812K) Class: 4374K(4736K)->4374K(4736K)
<13>Apr 27 18:08:20 dataproc-startup-script[1265]: <13>Apr 27 18:08:20 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:20.038+0000][info][gc           ] GC(15) Pause Young (Allocation Failure) 88M->81M(121M) 252.707ms
<13>Apr 27 18:08:20 dataproc-startup-script[1265]: <13>Apr 27 18:08:20 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:20.038+0000][info][gc,cpu       ] GC(15) User=0.05s Sys=0.00s Real=0.25s
<13>Apr 27 18:08:20 dataproc-startup-script[1265]: <13>Apr 27 18:08:20 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:20.040+0000][info][gc,start     ] GC(16) Pause Initial Mark
<13>Apr 27 18:08:20 dataproc-startup-script[1265]: <13>Apr 27 18:08:20 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:20.043+0000][info][gc           ] GC(16) Pause Initial Mark 84M->84M(121M) 3.772ms
<13>Apr 27 18:08:20 dataproc-startup-script[1265]: <13>Apr 27 18:08:20 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:20.044+0000][info][gc,cpu       ] GC(16) User=0.00s Sys=0.00s Real=0.00s
<13>Apr 27 18:08:20 dataproc-startup-script[1265]: <13>Apr 27 18:08:20 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:20.044+0000][info][gc           ] GC(16) Concurrent Mark
<13>Apr 27 18:08:20 dataproc-startup-script[1265]: <13>Apr 27 18:08:20 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:20,070 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
<13>Apr 27 18:08:20 dataproc-startup-script[1265]: <13>Apr 27 18:08:20 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:20,073 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
<13>Apr 27 18:08:20 dataproc-startup-script[1265]: <13>Apr 27 18:08:20 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:20,074 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
<13>Apr 27 18:08:20 dataproc-startup-script[1265]: <13>Apr 27 18:08:20 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:20,102 INFO namenode.FSNamesystem: Retry cache on namenode is enabled
<13>Apr 27 18:08:20 dataproc-startup-script[1265]: <13>Apr 27 18:08:20 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:20,124 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
<13>Apr 27 18:08:20 dataproc-startup-script[1265]: <13>Apr 27 18:08:20 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:20,143 INFO util.GSet: Computing capacity for map NameNodeRetryCache
<13>Apr 27 18:08:20 dataproc-startup-script[1265]: <13>Apr 27 18:08:20 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:20,144 INFO util.GSet: VM type       = 64-bit
<13>Apr 27 18:08:20 dataproc-startup-script[1265]: <13>Apr 27 18:08:20 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:20,147 INFO util.GSet: 0.029999999329447746% max memory 1.5 GB = 483.3 KB
<13>Apr 27 18:08:20 dataproc-startup-script[1265]: <13>Apr 27 18:08:20 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:20,152 INFO util.GSet: capacity      = 2^16 = 65536 entries
<13>Apr 27 18:08:20 dataproc-startup-script[1265]: <13>Apr 27 18:08:20 dataproc-activate-component-hive-metastore[5099]: nc: connect to cluster-dip-01-m (10.142.0.8) port 9083 (tcp) failed: Connection refused
<13>Apr 27 18:08:20 dataproc-startup-script[1265]: <13>Apr 27 18:08:20 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:20.336+0000][info][gc           ] GC(16) Concurrent Mark 292.292ms
<13>Apr 27 18:08:20 dataproc-startup-script[1265]: <13>Apr 27 18:08:20 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:20.344+0000][info][gc,cpu       ] GC(16) User=0.07s Sys=0.00s Real=0.30s
<13>Apr 27 18:08:20 dataproc-startup-script[1265]: <13>Apr 27 18:08:20 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:20.344+0000][info][gc           ] GC(16) Concurrent Preclean
<13>Apr 27 18:08:20 dataproc-startup-script[1265]: <13>Apr 27 18:08:20 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:20.350+0000][info][gc           ] GC(16) Concurrent Preclean 6.012ms
<13>Apr 27 18:08:20 dataproc-startup-script[1265]: <13>Apr 27 18:08:20 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:20.352+0000][info][gc,cpu       ] GC(16) User=0.00s Sys=0.00s Real=0.01s
<13>Apr 27 18:08:20 dataproc-startup-script[1265]: <13>Apr 27 18:08:20 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:20.353+0000][info][gc           ] GC(16) Concurrent Abortable Preclean
<13>Apr 27 18:08:20 dataproc-startup-script[1265]: <13>Apr 27 18:08:20 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:20,434 INFO namenode.FSImage: Allocated new BlockPoolId: BP-744010568-10.142.0.8-1745777300339
<13>Apr 27 18:08:20 dataproc-startup-script[1265]: <13>Apr 27 18:08:20 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:20,534 INFO common.Storage: Storage directory /hadoop/dfs/name has been successfully formatted.
<13>Apr 27 18:08:20 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5098.exitcode ]]
<13>Apr 27 18:08:20 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:08:21 dataproc-startup-script[1265]: <13>Apr 27 18:08:21 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:21,031 INFO namenode.FSImageFormatProtobuf: Saving image file /hadoop/dfs/name/current/fsimage.ckpt_0000000000000000000 using no compression
<13>Apr 27 18:08:21 dataproc-startup-script[1265]: <13>Apr 27 18:08:21 dataproc-activate-component-hive-metastore[5099]: nc: connect to cluster-dip-01-m (10.142.0.8) port 9083 (tcp) failed: Connection refused
<13>Apr 27 18:08:21 dataproc-startup-script[1265]: <13>Apr 27 18:08:21 dataproc-activate-component-hive-metastore[5099]: 'nc -v -z -w 1 cluster-dip-01-m 9083' attempt 23/300 failed! Sleeping 1s.
<13>Apr 27 18:08:21 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5098.exitcode ]]
<13>Apr 27 18:08:21 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:08:22 dataproc-startup-script[1265]: <13>Apr 27 18:08:22 dataproc-activate-component-hive-metastore[5099]: nc: connect to cluster-dip-01-m (10.142.0.8) port 9083 (tcp) failed: Connection refused
<13>Apr 27 18:08:22 dataproc-startup-script[1265]: <13>Apr 27 18:08:22 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:22,606 INFO namenode.FSImageFormatProtobuf: Image file /hadoop/dfs/name/current/fsimage.ckpt_0000000000000000000 of size 395 bytes saved in 1 seconds .
<13>Apr 27 18:08:22 dataproc-startup-script[1265]: <13>Apr 27 18:08:22 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:22,707 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0
<13>Apr 27 18:08:22 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5098.exitcode ]]
<13>Apr 27 18:08:22 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:08:23 dataproc-startup-script[1265]: <13>Apr 27 18:08:23 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:23,018 INFO namenode.FSNamesystem: Stopping services started for active state
<13>Apr 27 18:08:23 dataproc-startup-script[1265]: <13>Apr 27 18:08:23 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:23,056 INFO namenode.FSNamesystem: Stopping services started for standby state
<13>Apr 27 18:08:23 dataproc-startup-script[1265]: <13>Apr 27 18:08:23 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:23,118 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.
<13>Apr 27 18:08:23 dataproc-startup-script[1265]: <13>Apr 27 18:08:23 dataproc-activate-component-hdfs[5098]: 2025-04-27 18:08:23,165 INFO namenode.NameNode: SHUTDOWN_MSG: 
<13>Apr 27 18:08:23 dataproc-startup-script[1265]: <13>Apr 27 18:08:23 dataproc-activate-component-hdfs[5098]: /************************************************************
<13>Apr 27 18:08:23 dataproc-startup-script[1265]: <13>Apr 27 18:08:23 dataproc-activate-component-hdfs[5098]: SHUTDOWN_MSG: Shutting down NameNode at cluster-dip-01-m/10.142.0.8
<13>Apr 27 18:08:23 dataproc-startup-script[1265]: <13>Apr 27 18:08:23 dataproc-activate-component-hdfs[5098]: ************************************************************/
<13>Apr 27 18:08:23 dataproc-startup-script[1265]: <13>Apr 27 18:08:23 dataproc-activate-component-hive-metastore[5099]: nc: connect to cluster-dip-01-m (10.142.0.8) port 9083 (tcp) failed: Connection refused
<13>Apr 27 18:08:23 dataproc-startup-script[1265]: <13>Apr 27 18:08:23 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:23.391+0000][info][gc           ] GC(16) Concurrent Abortable Preclean 3037.682ms
<13>Apr 27 18:08:23 dataproc-startup-script[1265]: <13>Apr 27 18:08:23 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:23.391+0000][info][gc,cpu       ] GC(16) User=0.67s Sys=0.03s Real=3.04s
<13>Apr 27 18:08:23 dataproc-startup-script[1265]: <13>Apr 27 18:08:23 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:23.391+0000][info][gc,start     ] GC(16) Pause Remark
<13>Apr 27 18:08:23 dataproc-startup-script[1265]: <13>Apr 27 18:08:23 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:23.415+0000][info][gc           ] GC(16) Pause Remark 110M->110M(121M) 23.598ms
<13>Apr 27 18:08:23 dataproc-startup-script[1265]: <13>Apr 27 18:08:23 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:23.420+0000][info][gc,cpu       ] GC(16) User=0.02s Sys=0.00s Real=0.03s
<13>Apr 27 18:08:23 dataproc-startup-script[1265]: <13>Apr 27 18:08:23 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:23.420+0000][info][gc           ] GC(16) Concurrent Sweep
<13>Apr 27 18:08:23 dataproc-startup-script[1265]: <13>Apr 27 18:08:23 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:23.477+0000][info][gc           ] GC(16) Concurrent Sweep 56.937ms
<13>Apr 27 18:08:23 dataproc-startup-script[1265]: <13>Apr 27 18:08:23 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:23.480+0000][info][gc,cpu       ] GC(16) User=0.01s Sys=0.00s Real=0.06s
<13>Apr 27 18:08:23 dataproc-startup-script[1265]: <13>Apr 27 18:08:23 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:23.481+0000][info][gc           ] GC(16) Concurrent Reset
<13>Apr 27 18:08:23 dataproc-startup-script[1265]: <13>Apr 27 18:08:23 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:23.506+0000][info][gc           ] GC(16) Concurrent Reset 24.687ms
<13>Apr 27 18:08:23 dataproc-startup-script[1265]: <13>Apr 27 18:08:23 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:23.509+0000][info][gc,cpu       ] GC(16) User=0.01s Sys=0.00s Real=0.03s
<13>Apr 27 18:08:23 dataproc-startup-script[1265]: <13>Apr 27 18:08:23 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:23.509+0000][info][gc,heap      ] GC(16) Old: 79149K->79060K(131768K)
<13>Apr 27 18:08:23 dataproc-startup-script[1265]: <13>Apr 27 18:08:23 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:23.509+0000][info][gc,heap,exit ] Heap
<13>Apr 27 18:08:23 dataproc-startup-script[1265]: <13>Apr 27 18:08:23 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:23.509+0000][info][gc,heap,exit ]  par new generation   total 38720K, used 34134K [0x000000009ca00000, 0x000000009f400000, 0x00000000a7060000)
<13>Apr 27 18:08:23 dataproc-startup-script[1265]: <13>Apr 27 18:08:23 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:23.509+0000][info][gc,heap,exit ]   eden space 34432K,  86% used [0x000000009ca00000, 0x000000009e725910, 0x000000009eba0000)
<13>Apr 27 18:08:23 dataproc-startup-script[1265]: <13>Apr 27 18:08:23 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:23.509+0000][info][gc,heap,exit ]   from space 4288K, 100% used [0x000000009efd0000, 0x000000009f400000, 0x000000009f400000)
<13>Apr 27 18:08:23 dataproc-startup-script[1265]: <13>Apr 27 18:08:23 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:23.510+0000][info][gc,heap,exit ]   to   space 4288K,   0% used [0x000000009eba0000, 0x000000009eba0000, 0x000000009efd0000)
<13>Apr 27 18:08:23 dataproc-startup-script[1265]: <13>Apr 27 18:08:23 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:23.510+0000][info][gc,heap,exit ]  concurrent mark-sweep generation total 131768K, used 79060K [0x00000000a7060000, 0x00000000af10e000, 0x0000000100000000)
<13>Apr 27 18:08:23 dataproc-startup-script[1265]: <13>Apr 27 18:08:23 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:23.510+0000][info][gc,heap,exit ]  Metaspace       used 43373K, capacity 44070K, committed 44364K, reserved 1089536K
<13>Apr 27 18:08:23 dataproc-startup-script[1265]: <13>Apr 27 18:08:23 dataproc-activate-component-hdfs[5098]: [2025-04-27T18:08:23.510+0000][info][gc,heap,exit ]   class space    used 4621K, capacity 4928K, committed 4992K, reserved 1048576K
<13>Apr 27 18:08:23 dataproc-startup-script[1265]: <13>Apr 27 18:08:23 dataproc-activate-component-hdfs[5098]: 'su -s /bin/bash hdfs -c source /etc/default/hadoop-hdfs-namenode &&           hdfs namenode -format -nonInteractive' succeeded after 1 execution(s).
<13>Apr 27 18:08:23 dataproc-startup-script[1265]: <13>Apr 27 18:08:23 dataproc-activate-component-hdfs[5098]: + return 0
<13>Apr 27 18:08:23 dataproc-startup-script[1265]: <13>Apr 27 18:08:23 dataproc-activate-component-hdfs[5098]: + enable_and_start_service hadoop-hdfs-namenode
<13>Apr 27 18:08:23 dataproc-startup-script[1265]: <13>Apr 27 18:08:23 dataproc-activate-component-hdfs[5098]: + local -r service=hadoop-hdfs-namenode
<13>Apr 27 18:08:23 dataproc-startup-script[1265]: <13>Apr 27 18:08:23 dataproc-activate-component-hdfs[5098]: + enable_service hadoop-hdfs-namenode
<13>Apr 27 18:08:23 dataproc-startup-script[1265]: <13>Apr 27 18:08:23 dataproc-activate-component-hdfs[5098]: + local -r service=hadoop-hdfs-namenode
<13>Apr 27 18:08:23 dataproc-startup-script[1265]: <13>Apr 27 18:08:23 dataproc-activate-component-hdfs[5098]: + local -r unit=hadoop-hdfs-namenode.service
<13>Apr 27 18:08:23 dataproc-startup-script[1265]: <13>Apr 27 18:08:23 dataproc-activate-component-hdfs[5098]: + retry_constant_short systemctl enable hadoop-hdfs-namenode.service
<13>Apr 27 18:08:23 dataproc-startup-script[1265]: <13>Apr 27 18:08:23 dataproc-activate-component-hdfs[5098]: + retry_constant_custom 30 1 systemctl enable hadoop-hdfs-namenode.service
<13>Apr 27 18:08:23 dataproc-startup-script[1265]: <13>Apr 27 18:08:23 dataproc-activate-component-hdfs[5098]: + local -r max_retry_time=30
<13>Apr 27 18:08:23 dataproc-startup-script[1265]: <13>Apr 27 18:08:23 dataproc-activate-component-hdfs[5098]: + local -r retry_delay=1
<13>Apr 27 18:08:23 dataproc-startup-script[1265]: <13>Apr 27 18:08:23 dataproc-activate-component-hdfs[5098]: + cmd=('systemctl' 'enable' 'hadoop-hdfs-namenode.service')
<13>Apr 27 18:08:23 dataproc-startup-script[1265]: <13>Apr 27 18:08:23 dataproc-activate-component-hdfs[5098]: + local -r cmd
<13>Apr 27 18:08:23 dataproc-startup-script[1265]: <13>Apr 27 18:08:23 dataproc-activate-component-hdfs[5098]: + local -r max_retries=30
<13>Apr 27 18:08:23 dataproc-startup-script[1265]: <13>Apr 27 18:08:23 dataproc-activate-component-hdfs[5098]: + local reenable_x=false
<13>Apr 27 18:08:23 dataproc-startup-script[1265]: <13>Apr 27 18:08:23 dataproc-activate-component-hdfs[5098]: + [[ -o xtrace ]]
<13>Apr 27 18:08:23 dataproc-startup-script[1265]: <13>Apr 27 18:08:23 dataproc-activate-component-hdfs[5098]: + set +x
<13>Apr 27 18:08:23 dataproc-startup-script[1265]: <13>Apr 27 18:08:23 dataproc-activate-component-hdfs[5098]: About to run 'systemctl enable hadoop-hdfs-namenode.service' with retries...
<13>Apr 27 18:08:23 dataproc-startup-script[1265]: <13>Apr 27 18:08:23 dataproc-activate-component-hdfs[5098]: hadoop-hdfs-namenode.service is not a native service, redirecting to systemd-sysv-install.
<13>Apr 27 18:08:23 dataproc-startup-script[1265]: <13>Apr 27 18:08:23 dataproc-activate-component-hdfs[5098]: Executing: /lib/systemd/systemd-sysv-install enable hadoop-hdfs-namenode
<13>Apr 27 18:08:23 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5098.exitcode ]]
<13>Apr 27 18:08:23 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:08:24 dataproc-startup-script[1265]: <13>Apr 27 18:08:24 dataproc-activate-component-hive-metastore[5099]: nc: connect to cluster-dip-01-m (10.142.0.8) port 9083 (tcp) failed: Connection refused
<13>Apr 27 18:08:24 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5098.exitcode ]]
<13>Apr 27 18:08:24 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:08:24 dataproc-startup-script[1265]: <13>Apr 27 18:08:24 dataproc-activate-component-hdfs[5098]: 'systemctl enable hadoop-hdfs-namenode.service' succeeded after 1 execution(s).
<13>Apr 27 18:08:24 dataproc-startup-script[1265]: <13>Apr 27 18:08:24 dataproc-activate-component-hdfs[5098]: + return 0
<13>Apr 27 18:08:24 dataproc-startup-script[1265]: <13>Apr 27 18:08:24 dataproc-activate-component-hdfs[5098]: + local -r drop_in_dir=/etc/systemd/system/hadoop-hdfs-namenode.service.d
<13>Apr 27 18:08:24 dataproc-startup-script[1265]: <13>Apr 27 18:08:24 dataproc-activate-component-hdfs[5098]: + mkdir -p /etc/systemd/system/hadoop-hdfs-namenode.service.d
<13>Apr 27 18:08:24 dataproc-startup-script[1265]: <13>Apr 27 18:08:24 dataproc-activate-component-hdfs[5098]: + local props
<13>Apr 27 18:08:24 dataproc-startup-script[1265]: <13>Apr 27 18:08:24 dataproc-activate-component-hdfs[5098]: ++ retry_constant_short systemctl show hadoop-hdfs-namenode.service -p Restart,RemainAfterExit
<13>Apr 27 18:08:24 dataproc-startup-script[1265]: <13>Apr 27 18:08:24 dataproc-activate-component-hdfs[5098]: ++ retry_constant_custom 30 1 systemctl show hadoop-hdfs-namenode.service -p Restart,RemainAfterExit
<13>Apr 27 18:08:24 dataproc-startup-script[1265]: <13>Apr 27 18:08:24 dataproc-activate-component-hdfs[5098]: ++ local -r max_retry_time=30
<13>Apr 27 18:08:24 dataproc-startup-script[1265]: <13>Apr 27 18:08:24 dataproc-activate-component-hdfs[5098]: ++ local -r retry_delay=1
<13>Apr 27 18:08:24 dataproc-startup-script[1265]: <13>Apr 27 18:08:24 dataproc-activate-component-hdfs[5098]: ++ cmd=('systemctl' 'show' 'hadoop-hdfs-namenode.service' '-p' 'Restart,RemainAfterExit')
<13>Apr 27 18:08:24 dataproc-startup-script[1265]: <13>Apr 27 18:08:24 dataproc-activate-component-hdfs[5098]: ++ local -r cmd
<13>Apr 27 18:08:24 dataproc-startup-script[1265]: <13>Apr 27 18:08:24 dataproc-activate-component-hdfs[5098]: ++ local -r max_retries=30
<13>Apr 27 18:08:24 dataproc-startup-script[1265]: <13>Apr 27 18:08:24 dataproc-activate-component-hdfs[5098]: ++ local reenable_x=false
<13>Apr 27 18:08:24 dataproc-startup-script[1265]: <13>Apr 27 18:08:24 dataproc-activate-component-hdfs[5098]: ++ [[ -o xtrace ]]
<13>Apr 27 18:08:24 dataproc-startup-script[1265]: <13>Apr 27 18:08:24 dataproc-activate-component-hdfs[5098]: ++ set +x
<13>Apr 27 18:08:24 dataproc-startup-script[1265]: <13>Apr 27 18:08:24 dataproc-activate-component-hdfs[5098]: About to run 'systemctl show hadoop-hdfs-namenode.service -p Restart,RemainAfterExit' with retries...
<13>Apr 27 18:08:24 dataproc-startup-script[1265]: <13>Apr 27 18:08:24 dataproc-activate-component-hdfs[5098]: 'systemctl show hadoop-hdfs-namenode.service -p Restart,RemainAfterExit' succeeded after 1 execution(s).
<13>Apr 27 18:08:24 dataproc-startup-script[1265]: <13>Apr 27 18:08:24 dataproc-activate-component-hdfs[5098]: ++ return 0
<13>Apr 27 18:08:24 dataproc-startup-script[1265]: <13>Apr 27 18:08:24 dataproc-activate-component-hdfs[5098]: + props='Restart=no
<13>Apr 27 18:08:24 dataproc-startup-script[1265]: <13>Apr 27 18:08:24 dataproc-activate-component-hdfs[5098]: RemainAfterExit=no'
<13>Apr 27 18:08:24 dataproc-startup-script[1265]: <13>Apr 27 18:08:24 dataproc-activate-component-hdfs[5098]: + [[ hadoop-hdfs-namenode != \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Apr 27 18:08:24 dataproc-startup-script[1265]: <13>Apr 27 18:08:24 dataproc-activate-component-hdfs[5098]: + [[ hadoop-hdfs-namenode != \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Apr 27 18:08:24 dataproc-startup-script[1265]: <13>Apr 27 18:08:24 dataproc-activate-component-hdfs[5098]: + [[ Restart=no
<13>Apr 27 18:08:24 dataproc-startup-script[1265]: <13>Apr 27 18:08:24 dataproc-activate-component-hdfs[5098]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Apr 27 18:08:24 dataproc-startup-script[1265]: <13>Apr 27 18:08:24 dataproc-activate-component-hdfs[5098]: + [[ Restart=no
<13>Apr 27 18:08:24 dataproc-startup-script[1265]: <13>Apr 27 18:08:24 dataproc-activate-component-hdfs[5098]: RemainAfterExit=no == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Apr 27 18:08:24 dataproc-startup-script[1265]: <13>Apr 27 18:08:24 dataproc-activate-component-hdfs[5098]: + ln -s -f /etc/systemd/system/common/restart.conf /etc/systemd/system/hadoop-hdfs-namenode.service.d
<13>Apr 27 18:08:24 dataproc-startup-script[1265]: <13>Apr 27 18:08:24 dataproc-activate-component-hdfs[5098]: + local -r gate_start_on_agent_success_drop_in=/etc/systemd/system/common/agent-gate.conf
<13>Apr 27 18:08:24 dataproc-startup-script[1265]: <13>Apr 27 18:08:24 dataproc-activate-component-hdfs[5098]: + [[ ! -f /etc/systemd/system/common/agent-gate.conf ]]
<13>Apr 27 18:08:24 dataproc-startup-script[1265]: <13>Apr 27 18:08:24 dataproc-activate-component-hdfs[5098]: + [[ hadoop-hdfs-namenode == \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Apr 27 18:08:24 dataproc-startup-script[1265]: <13>Apr 27 18:08:24 dataproc-activate-component-hdfs[5098]: + [[ hadoop-hdfs-namenode == \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Apr 27 18:08:24 dataproc-startup-script[1265]: <13>Apr 27 18:08:24 dataproc-activate-component-hdfs[5098]: + start_service hadoop-hdfs-namenode
<13>Apr 27 18:08:24 dataproc-startup-script[1265]: <13>Apr 27 18:08:24 dataproc-activate-component-hdfs[5098]: + start_services hadoop-hdfs-namenode
<13>Apr 27 18:08:24 dataproc-startup-script[1265]: <13>Apr 27 18:08:24 dataproc-activate-component-hdfs[5098]: + units=('hadoop-hdfs-namenode.service')
<13>Apr 27 18:08:24 dataproc-startup-script[1265]: <13>Apr 27 18:08:24 dataproc-activate-component-hdfs[5098]: + local -r units
<13>Apr 27 18:08:24 dataproc-startup-script[1265]: <13>Apr 27 18:08:24 dataproc-activate-component-hdfs[5098]: + retry_constant_short systemctl start hadoop-hdfs-namenode.service
<13>Apr 27 18:08:24 dataproc-startup-script[1265]: <13>Apr 27 18:08:24 dataproc-activate-component-hdfs[5098]: + retry_constant_custom 30 1 systemctl start hadoop-hdfs-namenode.service
<13>Apr 27 18:08:24 dataproc-startup-script[1265]: <13>Apr 27 18:08:24 dataproc-activate-component-hdfs[5098]: + local -r max_retry_time=30
<13>Apr 27 18:08:24 dataproc-startup-script[1265]: <13>Apr 27 18:08:24 dataproc-activate-component-hdfs[5098]: + local -r retry_delay=1
<13>Apr 27 18:08:24 dataproc-startup-script[1265]: <13>Apr 27 18:08:24 dataproc-activate-component-hdfs[5098]: + cmd=('systemctl' 'start' 'hadoop-hdfs-namenode.service')
<13>Apr 27 18:08:24 dataproc-startup-script[1265]: <13>Apr 27 18:08:24 dataproc-activate-component-hdfs[5098]: + local -r cmd
<13>Apr 27 18:08:24 dataproc-startup-script[1265]: <13>Apr 27 18:08:24 dataproc-activate-component-hdfs[5098]: + local -r max_retries=30
<13>Apr 27 18:08:24 dataproc-startup-script[1265]: <13>Apr 27 18:08:24 dataproc-activate-component-hdfs[5098]: + local reenable_x=false
<13>Apr 27 18:08:24 dataproc-startup-script[1265]: <13>Apr 27 18:08:24 dataproc-activate-component-hdfs[5098]: + [[ -o xtrace ]]
<13>Apr 27 18:08:24 dataproc-startup-script[1265]: <13>Apr 27 18:08:24 dataproc-activate-component-hdfs[5098]: + set +x
<13>Apr 27 18:08:24 dataproc-startup-script[1265]: <13>Apr 27 18:08:24 dataproc-activate-component-hdfs[5098]: About to run 'systemctl start hadoop-hdfs-namenode.service' with retries...
<13>Apr 27 18:08:24 dataproc-startup-script[1265]: <13>Apr 27 18:08:24 dataproc-activate-component-hdfs[5098]: Warning: The unit file, source configuration file or drop-ins of hadoop-hdfs-namenode.service changed on disk. Run 'systemctl daemon-reload' to reload units.
<13>Apr 27 18:08:25 dataproc-startup-script[1265]: <13>Apr 27 18:08:25 dataproc-activate-component-hive-metastore[5099]: nc: connect to cluster-dip-01-m (10.142.0.8) port 9083 (tcp) failed: Connection refused
<13>Apr 27 18:08:25 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5098.exitcode ]]
<13>Apr 27 18:08:25 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:08:26 dataproc-startup-script[1265]: <13>Apr 27 18:08:26 dataproc-activate-component-hive-metastore[5099]: nc: connect to cluster-dip-01-m (10.142.0.8) port 9083 (tcp) failed: Connection refused
<13>Apr 27 18:08:26 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5098.exitcode ]]
<13>Apr 27 18:08:26 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:08:27 dataproc-startup-script[1265]: <13>Apr 27 18:08:27 dataproc-activate-component-hive-metastore[5099]: nc: connect to cluster-dip-01-m (10.142.0.8) port 9083 (tcp) failed: Connection refused
<13>Apr 27 18:08:27 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5098.exitcode ]]
<13>Apr 27 18:08:27 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:08:27 dataproc-startup-script[1265]: <13>Apr 27 18:08:27 dataproc-activate-component-knox[5101]: 'systemctl start knox' succeeded after 1 execution(s).
<13>Apr 27 18:08:27 dataproc-startup-script[1265]: <13>Apr 27 18:08:27 dataproc-activate-component-knox[5101]: + return 0
<13>Apr 27 18:08:27 dataproc-startup-script[1265]: <13>Apr 27 18:08:27 dataproc-activate-component-knox[5101]: ++ date +%s.%N
<13>Apr 27 18:08:27 dataproc-startup-script[1265]: <13>Apr 27 18:08:27 dataproc-activate-component-knox[5101]: + local -r end=1745777307.912323860
<13>Apr 27 18:08:27 dataproc-startup-script[1265]: <13>Apr 27 18:08:27 dataproc-activate-component-knox[5101]: + local -r runtime_s=44
<13>Apr 27 18:08:27 dataproc-startup-script[1265]: <13>Apr 27 18:08:27 dataproc-activate-component-knox[5101]: + echo 'Component knox took 44s to activate'
<13>Apr 27 18:08:27 dataproc-startup-script[1265]: <13>Apr 27 18:08:27 dataproc-activate-component-knox[5101]: Component knox took 44s to activate
<13>Apr 27 18:08:27 dataproc-startup-script[1265]: <13>Apr 27 18:08:27 dataproc-activate-component-knox[5101]: + local -r time_file=/tmp/dataproc/components/activate/knox.time
<13>Apr 27 18:08:27 dataproc-startup-script[1265]: <13>Apr 27 18:08:27 dataproc-activate-component-knox[5101]: + touch /tmp/dataproc/components/activate/knox.time
<13>Apr 27 18:08:27 dataproc-startup-script[1265]: <13>Apr 27 18:08:27 dataproc-activate-component-knox[5101]: + cat
<13>Apr 27 18:08:27 dataproc-startup-script[1265]: <13>Apr 27 18:08:27 dataproc-activate-component-knox[5101]: + [[ 0 -ne 0 ]]
<13>Apr 27 18:08:27 dataproc-startup-script[1265]: <13>Apr 27 18:08:27 dataproc-activate-component-knox[5101]: + echo 'Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/knox.sh'
<13>Apr 27 18:08:27 dataproc-startup-script[1265]: <13>Apr 27 18:08:27 dataproc-activate-component-knox[5101]: Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/knox.sh
<13>Apr 27 18:08:27 dataproc-startup-script[1265]: <13>Apr 27 18:08:27 dataproc-activate-component-knox[5101]: + touch /tmp/dataproc/components/activate/knox.done
<13>Apr 27 18:08:27 dataproc-startup-script[1265]: ++ echo 0
<13>Apr 27 18:08:28 dataproc-startup-script[1265]: <13>Apr 27 18:08:28 dataproc-activate-component-hive-metastore[5099]: nc: connect to cluster-dip-01-m (10.142.0.8) port 9083 (tcp) failed: Connection refused
<13>Apr 27 18:08:28 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5098.exitcode ]]
<13>Apr 27 18:08:28 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:08:29 dataproc-startup-script[1265]: <13>Apr 27 18:08:29 dataproc-activate-component-hive-metastore[5099]: nc: connect to cluster-dip-01-m (10.142.0.8) port 9083 (tcp) failed: Connection refused
<13>Apr 27 18:08:29 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5098.exitcode ]]
<13>Apr 27 18:08:29 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:08:30 dataproc-startup-script[1265]: <13>Apr 27 18:08:30 dataproc-activate-component-hive-metastore[5099]: nc: connect to cluster-dip-01-m (10.142.0.8) port 9083 (tcp) failed: Connection refused
<13>Apr 27 18:08:30 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5098.exitcode ]]
<13>Apr 27 18:08:30 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:08:31 dataproc-startup-script[1265]: <13>Apr 27 18:08:31 dataproc-activate-component-hive-metastore[5099]: nc: connect to cluster-dip-01-m (10.142.0.8) port 9083 (tcp) failed: Connection refused
<13>Apr 27 18:08:31 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5098.exitcode ]]
<13>Apr 27 18:08:31 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:08:32 dataproc-startup-script[1265]: <13>Apr 27 18:08:32 dataproc-activate-component-hive-metastore[5099]: nc: connect to cluster-dip-01-m (10.142.0.8) port 9083 (tcp) failed: Connection refused
<13>Apr 27 18:08:32 dataproc-startup-script[1265]: <13>Apr 27 18:08:32 dataproc-activate-component-hive-metastore[5099]: 'nc -v -z -w 1 cluster-dip-01-m 9083' attempt 34/300 failed! Sleeping 1s.
<13>Apr 27 18:08:32 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5098.exitcode ]]
<13>Apr 27 18:08:32 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:08:32 dataproc-startup-script[1265]: <13>Apr 27 18:08:32 dataproc-activate-component-hdfs[5098]: 'systemctl start hadoop-hdfs-namenode.service' succeeded after 1 execution(s).
<13>Apr 27 18:08:32 dataproc-startup-script[1265]: <13>Apr 27 18:08:32 dataproc-activate-component-hdfs[5098]: + return 0
<13>Apr 27 18:08:32 dataproc-startup-script[1265]: <13>Apr 27 18:08:32 dataproc-activate-component-hdfs[5098]: + start_hdfs_secondarynamenode
<13>Apr 27 18:08:32 dataproc-startup-script[1265]: <13>Apr 27 18:08:32 dataproc-activate-component-hdfs[5098]: + enable_and_start_service hadoop-hdfs-secondarynamenode
<13>Apr 27 18:08:32 dataproc-startup-script[1265]: <13>Apr 27 18:08:32 dataproc-activate-component-hdfs[5098]: + local -r service=hadoop-hdfs-secondarynamenode
<13>Apr 27 18:08:32 dataproc-startup-script[1265]: <13>Apr 27 18:08:32 dataproc-activate-component-hdfs[5098]: + enable_service hadoop-hdfs-secondarynamenode
<13>Apr 27 18:08:32 dataproc-startup-script[1265]: <13>Apr 27 18:08:32 dataproc-activate-component-hdfs[5098]: + local -r service=hadoop-hdfs-secondarynamenode
<13>Apr 27 18:08:32 dataproc-startup-script[1265]: <13>Apr 27 18:08:32 dataproc-activate-component-hdfs[5098]: + local -r unit=hadoop-hdfs-secondarynamenode.service
<13>Apr 27 18:08:32 dataproc-startup-script[1265]: <13>Apr 27 18:08:32 dataproc-activate-component-hdfs[5098]: + retry_constant_short systemctl enable hadoop-hdfs-secondarynamenode.service
<13>Apr 27 18:08:32 dataproc-startup-script[1265]: <13>Apr 27 18:08:32 dataproc-activate-component-hdfs[5098]: + retry_constant_custom 30 1 systemctl enable hadoop-hdfs-secondarynamenode.service
<13>Apr 27 18:08:32 dataproc-startup-script[1265]: <13>Apr 27 18:08:32 dataproc-activate-component-hdfs[5098]: + local -r max_retry_time=30
<13>Apr 27 18:08:32 dataproc-startup-script[1265]: <13>Apr 27 18:08:32 dataproc-activate-component-hdfs[5098]: + local -r retry_delay=1
<13>Apr 27 18:08:32 dataproc-startup-script[1265]: <13>Apr 27 18:08:32 dataproc-activate-component-hdfs[5098]: + cmd=('systemctl' 'enable' 'hadoop-hdfs-secondarynamenode.service')
<13>Apr 27 18:08:32 dataproc-startup-script[1265]: <13>Apr 27 18:08:32 dataproc-activate-component-hdfs[5098]: + local -r cmd
<13>Apr 27 18:08:32 dataproc-startup-script[1265]: <13>Apr 27 18:08:32 dataproc-activate-component-hdfs[5098]: + local -r max_retries=30
<13>Apr 27 18:08:32 dataproc-startup-script[1265]: <13>Apr 27 18:08:32 dataproc-activate-component-hdfs[5098]: + local reenable_x=false
<13>Apr 27 18:08:32 dataproc-startup-script[1265]: <13>Apr 27 18:08:32 dataproc-activate-component-hdfs[5098]: + [[ -o xtrace ]]
<13>Apr 27 18:08:32 dataproc-startup-script[1265]: <13>Apr 27 18:08:32 dataproc-activate-component-hdfs[5098]: + set +x
<13>Apr 27 18:08:32 dataproc-startup-script[1265]: <13>Apr 27 18:08:32 dataproc-activate-component-hdfs[5098]: About to run 'systemctl enable hadoop-hdfs-secondarynamenode.service' with retries...
<13>Apr 27 18:08:32 dataproc-startup-script[1265]: <13>Apr 27 18:08:32 dataproc-activate-component-hdfs[5098]: hadoop-hdfs-secondarynamenode.service is not a native service, redirecting to systemd-sysv-install.
<13>Apr 27 18:08:32 dataproc-startup-script[1265]: <13>Apr 27 18:08:32 dataproc-activate-component-hdfs[5098]: Executing: /lib/systemd/systemd-sysv-install enable hadoop-hdfs-secondarynamenode
<13>Apr 27 18:08:33 dataproc-startup-script[1265]: <13>Apr 27 18:08:33 dataproc-activate-component-hive-metastore[5099]: nc: connect to cluster-dip-01-m (10.142.0.8) port 9083 (tcp) failed: Connection refused
<13>Apr 27 18:08:33 dataproc-startup-script[1265]: <13>Apr 27 18:08:33 dataproc-activate-component-hdfs[5098]: 'systemctl enable hadoop-hdfs-secondarynamenode.service' succeeded after 1 execution(s).
<13>Apr 27 18:08:33 dataproc-startup-script[1265]: <13>Apr 27 18:08:33 dataproc-activate-component-hdfs[5098]: + return 0
<13>Apr 27 18:08:33 dataproc-startup-script[1265]: <13>Apr 27 18:08:33 dataproc-activate-component-hdfs[5098]: + local -r drop_in_dir=/etc/systemd/system/hadoop-hdfs-secondarynamenode.service.d
<13>Apr 27 18:08:33 dataproc-startup-script[1265]: <13>Apr 27 18:08:33 dataproc-activate-component-hdfs[5098]: + mkdir -p /etc/systemd/system/hadoop-hdfs-secondarynamenode.service.d
<13>Apr 27 18:08:33 dataproc-startup-script[1265]: <13>Apr 27 18:08:33 dataproc-activate-component-hdfs[5098]: + local props
<13>Apr 27 18:08:33 dataproc-startup-script[1265]: <13>Apr 27 18:08:33 dataproc-activate-component-hdfs[5098]: ++ retry_constant_short systemctl show hadoop-hdfs-secondarynamenode.service -p Restart,RemainAfterExit
<13>Apr 27 18:08:33 dataproc-startup-script[1265]: <13>Apr 27 18:08:33 dataproc-activate-component-hdfs[5098]: ++ retry_constant_custom 30 1 systemctl show hadoop-hdfs-secondarynamenode.service -p Restart,RemainAfterExit
<13>Apr 27 18:08:33 dataproc-startup-script[1265]: <13>Apr 27 18:08:33 dataproc-activate-component-hdfs[5098]: ++ local -r max_retry_time=30
<13>Apr 27 18:08:33 dataproc-startup-script[1265]: <13>Apr 27 18:08:33 dataproc-activate-component-hdfs[5098]: ++ local -r retry_delay=1
<13>Apr 27 18:08:33 dataproc-startup-script[1265]: <13>Apr 27 18:08:33 dataproc-activate-component-hdfs[5098]: ++ cmd=('systemctl' 'show' 'hadoop-hdfs-secondarynamenode.service' '-p' 'Restart,RemainAfterExit')
<13>Apr 27 18:08:33 dataproc-startup-script[1265]: <13>Apr 27 18:08:33 dataproc-activate-component-hdfs[5098]: ++ local -r cmd
<13>Apr 27 18:08:33 dataproc-startup-script[1265]: <13>Apr 27 18:08:33 dataproc-activate-component-hdfs[5098]: ++ local -r max_retries=30
<13>Apr 27 18:08:33 dataproc-startup-script[1265]: <13>Apr 27 18:08:33 dataproc-activate-component-hdfs[5098]: ++ local reenable_x=false
<13>Apr 27 18:08:33 dataproc-startup-script[1265]: <13>Apr 27 18:08:33 dataproc-activate-component-hdfs[5098]: ++ [[ -o xtrace ]]
<13>Apr 27 18:08:33 dataproc-startup-script[1265]: <13>Apr 27 18:08:33 dataproc-activate-component-hdfs[5098]: ++ set +x
<13>Apr 27 18:08:33 dataproc-startup-script[1265]: <13>Apr 27 18:08:33 dataproc-activate-component-hdfs[5098]: About to run 'systemctl show hadoop-hdfs-secondarynamenode.service -p Restart,RemainAfterExit' with retries...
<13>Apr 27 18:08:33 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5098.exitcode ]]
<13>Apr 27 18:08:33 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:08:33 dataproc-startup-script[1265]: <13>Apr 27 18:08:33 dataproc-activate-component-hdfs[5098]: 'systemctl show hadoop-hdfs-secondarynamenode.service -p Restart,RemainAfterExit' succeeded after 1 execution(s).
<13>Apr 27 18:08:33 dataproc-startup-script[1265]: <13>Apr 27 18:08:33 dataproc-activate-component-hdfs[5098]: ++ return 0
<13>Apr 27 18:08:33 dataproc-startup-script[1265]: <13>Apr 27 18:08:33 dataproc-activate-component-hdfs[5098]: + props='Restart=no
<13>Apr 27 18:08:33 dataproc-startup-script[1265]: <13>Apr 27 18:08:33 dataproc-activate-component-hdfs[5098]: RemainAfterExit=no'
<13>Apr 27 18:08:33 dataproc-startup-script[1265]: <13>Apr 27 18:08:33 dataproc-activate-component-hdfs[5098]: + [[ hadoop-hdfs-secondarynamenode != \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Apr 27 18:08:33 dataproc-startup-script[1265]: <13>Apr 27 18:08:33 dataproc-activate-component-hdfs[5098]: + [[ hadoop-hdfs-secondarynamenode != \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Apr 27 18:08:33 dataproc-startup-script[1265]: <13>Apr 27 18:08:33 dataproc-activate-component-hdfs[5098]: + [[ Restart=no
<13>Apr 27 18:08:33 dataproc-startup-script[1265]: <13>Apr 27 18:08:33 dataproc-activate-component-hdfs[5098]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Apr 27 18:08:33 dataproc-startup-script[1265]: <13>Apr 27 18:08:33 dataproc-activate-component-hdfs[5098]: + [[ Restart=no
<13>Apr 27 18:08:33 dataproc-startup-script[1265]: <13>Apr 27 18:08:33 dataproc-activate-component-hdfs[5098]: RemainAfterExit=no == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Apr 27 18:08:33 dataproc-startup-script[1265]: <13>Apr 27 18:08:33 dataproc-activate-component-hdfs[5098]: + ln -s -f /etc/systemd/system/common/restart.conf /etc/systemd/system/hadoop-hdfs-secondarynamenode.service.d
<13>Apr 27 18:08:33 dataproc-startup-script[1265]: <13>Apr 27 18:08:33 dataproc-activate-component-hdfs[5098]: + local -r gate_start_on_agent_success_drop_in=/etc/systemd/system/common/agent-gate.conf
<13>Apr 27 18:08:33 dataproc-startup-script[1265]: <13>Apr 27 18:08:33 dataproc-activate-component-hdfs[5098]: + [[ ! -f /etc/systemd/system/common/agent-gate.conf ]]
<13>Apr 27 18:08:33 dataproc-startup-script[1265]: <13>Apr 27 18:08:33 dataproc-activate-component-hdfs[5098]: + [[ hadoop-hdfs-secondarynamenode == \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Apr 27 18:08:33 dataproc-startup-script[1265]: <13>Apr 27 18:08:33 dataproc-activate-component-hdfs[5098]: + [[ hadoop-hdfs-secondarynamenode == \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Apr 27 18:08:33 dataproc-startup-script[1265]: <13>Apr 27 18:08:33 dataproc-activate-component-hdfs[5098]: + start_service hadoop-hdfs-secondarynamenode
<13>Apr 27 18:08:33 dataproc-startup-script[1265]: <13>Apr 27 18:08:33 dataproc-activate-component-hdfs[5098]: + start_services hadoop-hdfs-secondarynamenode
<13>Apr 27 18:08:33 dataproc-startup-script[1265]: <13>Apr 27 18:08:33 dataproc-activate-component-hdfs[5098]: + units=('hadoop-hdfs-secondarynamenode.service')
<13>Apr 27 18:08:33 dataproc-startup-script[1265]: <13>Apr 27 18:08:33 dataproc-activate-component-hdfs[5098]: + local -r units
<13>Apr 27 18:08:33 dataproc-startup-script[1265]: <13>Apr 27 18:08:33 dataproc-activate-component-hdfs[5098]: + retry_constant_short systemctl start hadoop-hdfs-secondarynamenode.service
<13>Apr 27 18:08:33 dataproc-startup-script[1265]: <13>Apr 27 18:08:33 dataproc-activate-component-hdfs[5098]: + retry_constant_custom 30 1 systemctl start hadoop-hdfs-secondarynamenode.service
<13>Apr 27 18:08:33 dataproc-startup-script[1265]: <13>Apr 27 18:08:33 dataproc-activate-component-hdfs[5098]: + local -r max_retry_time=30
<13>Apr 27 18:08:33 dataproc-startup-script[1265]: <13>Apr 27 18:08:33 dataproc-activate-component-hdfs[5098]: + local -r retry_delay=1
<13>Apr 27 18:08:33 dataproc-startup-script[1265]: <13>Apr 27 18:08:33 dataproc-activate-component-hdfs[5098]: + cmd=('systemctl' 'start' 'hadoop-hdfs-secondarynamenode.service')
<13>Apr 27 18:08:33 dataproc-startup-script[1265]: <13>Apr 27 18:08:33 dataproc-activate-component-hdfs[5098]: + local -r cmd
<13>Apr 27 18:08:33 dataproc-startup-script[1265]: <13>Apr 27 18:08:33 dataproc-activate-component-hdfs[5098]: + local -r max_retries=30
<13>Apr 27 18:08:33 dataproc-startup-script[1265]: <13>Apr 27 18:08:33 dataproc-activate-component-hdfs[5098]: + local reenable_x=false
<13>Apr 27 18:08:33 dataproc-startup-script[1265]: <13>Apr 27 18:08:33 dataproc-activate-component-hdfs[5098]: + [[ -o xtrace ]]
<13>Apr 27 18:08:33 dataproc-startup-script[1265]: <13>Apr 27 18:08:33 dataproc-activate-component-hdfs[5098]: + set +x
<13>Apr 27 18:08:33 dataproc-startup-script[1265]: <13>Apr 27 18:08:33 dataproc-activate-component-hdfs[5098]: About to run 'systemctl start hadoop-hdfs-secondarynamenode.service' with retries...
<13>Apr 27 18:08:33 dataproc-startup-script[1265]: <13>Apr 27 18:08:33 dataproc-activate-component-hdfs[5098]: Warning: The unit file, source configuration file or drop-ins of hadoop-hdfs-secondarynamenode.service changed on disk. Run 'systemctl daemon-reload' to reload units.
<13>Apr 27 18:08:34 dataproc-startup-script[1265]: <13>Apr 27 18:08:34 dataproc-activate-component-hive-metastore[5099]: nc: connect to cluster-dip-01-m (10.142.0.8) port 9083 (tcp) failed: Connection refused
<13>Apr 27 18:08:34 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5098.exitcode ]]
<13>Apr 27 18:08:34 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:08:35 dataproc-startup-script[1265]: <13>Apr 27 18:08:35 dataproc-activate-component-hive-metastore[5099]: nc: connect to cluster-dip-01-m (10.142.0.8) port 9083 (tcp) failed: Connection refused
<13>Apr 27 18:08:35 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5098.exitcode ]]
<13>Apr 27 18:08:35 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:08:36 dataproc-startup-script[1265]: <13>Apr 27 18:08:36 dataproc-activate-component-hive-metastore[5099]: nc: connect to cluster-dip-01-m (10.142.0.8) port 9083 (tcp) failed: Connection refused
<13>Apr 27 18:08:36 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5098.exitcode ]]
<13>Apr 27 18:08:36 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:08:37 dataproc-startup-script[1265]: <13>Apr 27 18:08:37 dataproc-activate-component-hive-metastore[5099]: nc: connect to cluster-dip-01-m (10.142.0.8) port 9083 (tcp) failed: Connection refused
<13>Apr 27 18:08:37 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5098.exitcode ]]
<13>Apr 27 18:08:37 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:08:38 dataproc-startup-script[1265]: <13>Apr 27 18:08:38 dataproc-activate-component-hive-metastore[5099]: nc: connect to cluster-dip-01-m (10.142.0.8) port 9083 (tcp) failed: Connection refused
<13>Apr 27 18:08:38 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5098.exitcode ]]
<13>Apr 27 18:08:38 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:08:39 dataproc-startup-script[1265]: <13>Apr 27 18:08:39 dataproc-activate-component-hive-metastore[5099]: nc: connect to cluster-dip-01-m (10.142.0.8) port 9083 (tcp) failed: Connection refused
<13>Apr 27 18:08:39 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5098.exitcode ]]
<13>Apr 27 18:08:39 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:08:40 dataproc-startup-script[1265]: <13>Apr 27 18:08:40 dataproc-activate-component-hive-metastore[5099]: nc: connect to cluster-dip-01-m (10.142.0.8) port 9083 (tcp) failed: Connection refused
<13>Apr 27 18:08:40 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5098.exitcode ]]
<13>Apr 27 18:08:40 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:08:41 dataproc-startup-script[1265]: <13>Apr 27 18:08:41 dataproc-activate-component-hive-metastore[5099]: nc: connect to cluster-dip-01-m (10.142.0.8) port 9083 (tcp) failed: Connection refused
<13>Apr 27 18:08:41 dataproc-startup-script[1265]: <13>Apr 27 18:08:41 dataproc-activate-component-hdfs[5098]: 'systemctl start hadoop-hdfs-secondarynamenode.service' succeeded after 1 execution(s).
<13>Apr 27 18:08:41 dataproc-startup-script[1265]: <13>Apr 27 18:08:41 dataproc-activate-component-hdfs[5098]: + return 0
<13>Apr 27 18:08:41 dataproc-startup-script[1265]: <13>Apr 27 18:08:41 dataproc-activate-component-hdfs[5098]: + init_hcfs_dirs
<13>Apr 27 18:08:41 dataproc-startup-script[1265]: <13>Apr 27 18:08:41 dataproc-activate-component-hdfs[5098]: + is_in_cluster_hdfs hdfs://cluster-dip-01-m
<13>Apr 27 18:08:41 dataproc-startup-script[1265]: <13>Apr 27 18:08:41 dataproc-activate-component-hdfs[5098]: + local uri=hdfs://cluster-dip-01-m
<13>Apr 27 18:08:41 dataproc-startup-script[1265]: <13>Apr 27 18:08:41 dataproc-activate-component-hdfs[5098]: + [[ hdfs://cluster-dip-01-m != hdfs://* ]]
<13>Apr 27 18:08:41 dataproc-startup-script[1265]: <13>Apr 27 18:08:41 dataproc-activate-component-hdfs[5098]: + local host_name
<13>Apr 27 18:08:41 dataproc-startup-script[1265]: <13>Apr 27 18:08:41 dataproc-activate-component-hdfs[5098]: + host_name=cluster-dip-01-m
<13>Apr 27 18:08:41 dataproc-startup-script[1265]: <13>Apr 27 18:08:41 dataproc-activate-component-hdfs[5098]: + host_name=cluster-dip-01-m
<13>Apr 27 18:08:41 dataproc-startup-script[1265]: <13>Apr 27 18:08:41 dataproc-activate-component-hdfs[5098]: + host_name=cluster-dip-01-m
<13>Apr 27 18:08:41 dataproc-startup-script[1265]: <13>Apr 27 18:08:41 dataproc-activate-component-hdfs[5098]: + [[ cluster-dip-01-m == \c\l\u\s\t\e\r\-\d\i\p\-\0\1\-\m ]]
<13>Apr 27 18:08:41 dataproc-startup-script[1265]: <13>Apr 27 18:08:41 dataproc-activate-component-hdfs[5098]: + init_internal_hdfs
<13>Apr 27 18:08:41 dataproc-startup-script[1265]: <13>Apr 27 18:08:41 dataproc-activate-component-hdfs[5098]: + wait_for_hdfs cluster-dip-01-m
<13>Apr 27 18:08:41 dataproc-startup-script[1265]: <13>Apr 27 18:08:41 dataproc-activate-component-hdfs[5098]: + local -r hostname=cluster-dip-01-m
<13>Apr 27 18:08:41 dataproc-startup-script[1265]: <13>Apr 27 18:08:41 dataproc-activate-component-hdfs[5098]: + loginfo 'Waiting for NameNode to listen on RPC port'
<13>Apr 27 18:08:41 dataproc-startup-script[1265]: <13>Apr 27 18:08:41 dataproc-activate-component-hdfs[5098]: + echo 'Waiting for NameNode to listen on RPC port'
<13>Apr 27 18:08:41 dataproc-startup-script[1265]: <13>Apr 27 18:08:41 dataproc-activate-component-hdfs[5098]: Waiting for NameNode to listen on RPC port
<13>Apr 27 18:08:41 dataproc-startup-script[1265]: <13>Apr 27 18:08:41 dataproc-activate-component-hdfs[5098]: + local namenode_port_binding_timeout
<13>Apr 27 18:08:41 dataproc-startup-script[1265]: <13>Apr 27 18:08:41 dataproc-activate-component-hdfs[5098]: ++ get_dataproc_property_or_default startup.component.service-binding-timeout.hadoop-hdfs-namenode 300
<13>Apr 27 18:08:41 dataproc-startup-script[1265]: <13>Apr 27 18:08:41 dataproc-activate-component-hdfs[5098]: ++ set +x
<13>Apr 27 18:08:41 dataproc-startup-script[1265]: <13>Apr 27 18:08:41 dataproc-activate-component-hdfs[5098]: + namenode_port_binding_timeout=300
<13>Apr 27 18:08:41 dataproc-startup-script[1265]: <13>Apr 27 18:08:41 dataproc-activate-component-hdfs[5098]: + wait_for_port hadoop-hdfs-namenode cluster-dip-01-m 8020 300
<13>Apr 27 18:08:41 dataproc-startup-script[1265]: <13>Apr 27 18:08:41 dataproc-activate-component-hdfs[5098]: + local -r name=hadoop-hdfs-namenode
<13>Apr 27 18:08:41 dataproc-startup-script[1265]: <13>Apr 27 18:08:41 dataproc-activate-component-hdfs[5098]: + local -r host=cluster-dip-01-m
<13>Apr 27 18:08:41 dataproc-startup-script[1265]: <13>Apr 27 18:08:41 dataproc-activate-component-hdfs[5098]: + local -r port=8020
<13>Apr 27 18:08:41 dataproc-startup-script[1265]: <13>Apr 27 18:08:41 dataproc-activate-component-hdfs[5098]: + local -r timeout=300
<13>Apr 27 18:08:41 dataproc-startup-script[1265]: <13>Apr 27 18:08:41 dataproc-activate-component-hdfs[5098]: + local -r capped_timeout=300
<13>Apr 27 18:08:41 dataproc-startup-script[1265]: <13>Apr 27 18:08:41 dataproc-activate-component-hdfs[5098]: + loginfo 'Waiting 300 seconds for service to come up on host=cluster-dip-01-m port=8020 name=hadoop-hdfs-namenode.'
<13>Apr 27 18:08:41 dataproc-startup-script[1265]: <13>Apr 27 18:08:41 dataproc-activate-component-hdfs[5098]: + echo 'Waiting 300 seconds for service to come up on host=cluster-dip-01-m port=8020 name=hadoop-hdfs-namenode.'
<13>Apr 27 18:08:41 dataproc-startup-script[1265]: <13>Apr 27 18:08:41 dataproc-activate-component-hdfs[5098]: Waiting 300 seconds for service to come up on host=cluster-dip-01-m port=8020 name=hadoop-hdfs-namenode.
<13>Apr 27 18:08:41 dataproc-startup-script[1265]: <13>Apr 27 18:08:41 dataproc-activate-component-hdfs[5098]: + retry_constant_custom 300 1 nc -v -z -w 1 cluster-dip-01-m 8020
<13>Apr 27 18:08:41 dataproc-startup-script[1265]: <13>Apr 27 18:08:41 dataproc-activate-component-hdfs[5098]: + local -r max_retry_time=300
<13>Apr 27 18:08:41 dataproc-startup-script[1265]: <13>Apr 27 18:08:41 dataproc-activate-component-hdfs[5098]: + local -r retry_delay=1
<13>Apr 27 18:08:41 dataproc-startup-script[1265]: <13>Apr 27 18:08:41 dataproc-activate-component-hdfs[5098]: + cmd=('nc' '-v' '-z' '-w' '1' 'cluster-dip-01-m' '8020')
<13>Apr 27 18:08:41 dataproc-startup-script[1265]: <13>Apr 27 18:08:41 dataproc-activate-component-hdfs[5098]: + local -r cmd
<13>Apr 27 18:08:41 dataproc-startup-script[1265]: <13>Apr 27 18:08:41 dataproc-activate-component-hdfs[5098]: + local -r max_retries=300
<13>Apr 27 18:08:41 dataproc-startup-script[1265]: <13>Apr 27 18:08:41 dataproc-activate-component-hdfs[5098]: + local reenable_x=false
<13>Apr 27 18:08:41 dataproc-startup-script[1265]: <13>Apr 27 18:08:41 dataproc-activate-component-hdfs[5098]: + [[ -o xtrace ]]
<13>Apr 27 18:08:41 dataproc-startup-script[1265]: <13>Apr 27 18:08:41 dataproc-activate-component-hdfs[5098]: + set +x
<13>Apr 27 18:08:41 dataproc-startup-script[1265]: <13>Apr 27 18:08:41 dataproc-activate-component-hdfs[5098]: About to run 'nc -v -z -w 1 cluster-dip-01-m 8020' with retries...
<13>Apr 27 18:08:41 dataproc-startup-script[1265]: <13>Apr 27 18:08:41 dataproc-activate-component-hdfs[5098]: nc: connect to cluster-dip-01-m (10.142.0.8) port 8020 (tcp) failed: Connection refused
<13>Apr 27 18:08:41 dataproc-startup-script[1265]: <13>Apr 27 18:08:41 dataproc-activate-component-hdfs[5098]: 'nc -v -z -w 1 cluster-dip-01-m 8020' attempt 1/300 failed! Sleeping 1s.
<13>Apr 27 18:08:41 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5098.exitcode ]]
<13>Apr 27 18:08:41 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:08:42 dataproc-startup-script[1265]: <13>Apr 27 18:08:42 dataproc-activate-component-hive-metastore[5099]: nc: connect to cluster-dip-01-m (10.142.0.8) port 9083 (tcp) failed: Connection refused
<13>Apr 27 18:08:42 dataproc-startup-script[1265]: <13>Apr 27 18:08:42 dataproc-activate-component-hdfs[5098]: nc: connect to cluster-dip-01-m (10.142.0.8) port 8020 (tcp) failed: Connection refused
<13>Apr 27 18:08:42 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5098.exitcode ]]
<13>Apr 27 18:08:42 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hive-metastore[5099]: nc: connect to cluster-dip-01-m (10.142.0.8) port 9083 (tcp) failed: Connection refused
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hive-metastore[5099]: 'nc -v -z -w 1 cluster-dip-01-m 9083' attempt 45/300 failed! Sleeping 1s.
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-delayed_uninstall_artifacts[5070]: + uninstall_artifacts
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: Connection to cluster-dip-01-m (10.142.0.8) 8020 port [tcp/*] succeeded!
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: 'nc -v -z -w 1 cluster-dip-01-m 8020' succeeded after 3 execution(s).
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + return 0
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + loginfo 'Service up on host=cluster-dip-01-m port=8020 name=hadoop-hdfs-namenode.'
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + echo 'Service up on host=cluster-dip-01-m port=8020 name=hadoop-hdfs-namenode.'
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: Service up on host=cluster-dip-01-m port=8020 name=hadoop-hdfs-namenode.
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + create_hcfs_dirs true false
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + local use_webhdfs=true
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + local set_permission=false
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + [[ true == \t\r\u\e ]]
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + init_webhdfs_uri
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + local protocol
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + local port
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + [[ false == \t\r\u\e ]]
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + protocol=http
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: ++ cut -d: -f2
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: ++ get_property_in_xml /etc/hadoop/conf/hdfs-site.xml dfs.namenode.http-address 0.0.0.0:9870
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: ++ set +x
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + port=9870
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + local active_host
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + [[ 1 -gt 1 ]]
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + active_host=cluster-dip-01-m
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + [[ -n cluster-dip-01-m ]]
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + WEBHDFS_BASE_URI=http://cluster-dip-01-m:9870/webhdfs/v1
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + [[ -z http://cluster-dip-01-m:9870/webhdfs/v1 ]]
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + local -a dirs
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + local -a owner_groups
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + local -a modes
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + add_user_dirs
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + system_users=('hdfs' 'mapred' 'yarn' 'spark' 'pig' 'hive' 'hbase' 'zookeeper' 'solr' 'zeppelin')
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + local -a system_users
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + local -a real_users
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + real_users=($(getent passwd | awk -F: '1000 < $3 && $3 < 6000 { print $1}'))
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: ++ awk -F: '1000 < $3 && $3 < 6000 { print $1}'
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: ++ getent passwd
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + users=('hdfs' 'mapred' 'yarn' 'spark' 'pig' 'hive' 'hbase' 'zookeeper' 'solr' 'zeppelin' 'pwani')
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + local -a users
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + local user
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + for user in "${users[@]}"
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + HCFS_DIRS+=("/user/${user} ${user}:${user} 700")
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + for user in "${users[@]}"
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + HCFS_DIRS+=("/user/${user} ${user}:${user} 700")
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + for user in "${users[@]}"
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + HCFS_DIRS+=("/user/${user} ${user}:${user} 700")
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + for user in "${users[@]}"
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + HCFS_DIRS+=("/user/${user} ${user}:${user} 700")
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + for user in "${users[@]}"
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + HCFS_DIRS+=("/user/${user} ${user}:${user} 700")
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + for user in "${users[@]}"
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + HCFS_DIRS+=("/user/${user} ${user}:${user} 700")
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + for user in "${users[@]}"
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + HCFS_DIRS+=("/user/${user} ${user}:${user} 700")
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + for user in "${users[@]}"
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + HCFS_DIRS+=("/user/${user} ${user}:${user} 700")
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + for user in "${users[@]}"
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + HCFS_DIRS+=("/user/${user} ${user}:${user} 700")
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + for user in "${users[@]}"
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + HCFS_DIRS+=("/user/${user} ${user}:${user} 700")
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + for user in "${users[@]}"
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + HCFS_DIRS+=("/user/${user} ${user}:${user} 700")
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + local row
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + local -a columns
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + local encoded_path
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + for row in "${HCFS_DIRS[@]}"
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + columns=(${row})
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + [[ true == \t\r\u\e ]]
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: ++ encode_path_for_webhdfs /
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: ++ local encoded
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: +++ perl -MURI::Escape -s -e 'print uri_escape($query);' -- -query=/
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: ++ encoded=%2F
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: ++ echo /
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + encoded_path=/
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + dirs+=("${encoded_path}")
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + owner_groups+=("${columns[1]}")
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + modes+=("${columns[2]}")
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + for row in "${HCFS_DIRS[@]}"
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + columns=(${row})
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + [[ true == \t\r\u\e ]]
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: ++ encode_path_for_webhdfs /tmp
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: ++ local encoded
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: +++ perl -MURI::Escape -s -e 'print uri_escape($query);' -- -query=/tmp
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: ++ encoded=%2Ftmp
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: ++ echo /tmp
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + encoded_path=/tmp
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + dirs+=("${encoded_path}")
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + owner_groups+=("${columns[1]}")
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + modes+=("${columns[2]}")
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + for row in "${HCFS_DIRS[@]}"
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + columns=(${row})
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + [[ true == \t\r\u\e ]]
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: ++ encode_path_for_webhdfs /tmp/hadoop-yarn/staging
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: ++ local encoded
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: +++ perl -MURI::Escape -s -e 'print uri_escape($query);' -- -query=/tmp/hadoop-yarn/staging
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: ++ encoded=%2Ftmp%2Fhadoop-yarn%2Fstaging
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: ++ echo /tmp/hadoop-yarn/staging
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + encoded_path=/tmp/hadoop-yarn/staging
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + dirs+=("${encoded_path}")
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + owner_groups+=("${columns[1]}")
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + modes+=("${columns[2]}")
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + for row in "${HCFS_DIRS[@]}"
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + columns=(${row})
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + [[ true == \t\r\u\e ]]
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: ++ encode_path_for_webhdfs /tmp/hadoop-yarn/staging/history
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: ++ local encoded
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: +++ perl -MURI::Escape -s -e 'print uri_escape($query);' -- -query=/tmp/hadoop-yarn/staging/history
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: ++ encoded=%2Ftmp%2Fhadoop-yarn%2Fstaging%2Fhistory
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: ++ echo /tmp/hadoop-yarn/staging/history
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + encoded_path=/tmp/hadoop-yarn/staging/history
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + dirs+=("${encoded_path}")
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + owner_groups+=("${columns[1]}")
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + modes+=("${columns[2]}")
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + for row in "${HCFS_DIRS[@]}"
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + columns=(${row})
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + [[ true == \t\r\u\e ]]
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: ++ encode_path_for_webhdfs /user
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: ++ local encoded
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: +++ perl -MURI::Escape -s -e 'print uri_escape($query);' -- -query=/user
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: ++ encoded=%2Fuser
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: ++ echo /user
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + encoded_path=/user
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + dirs+=("${encoded_path}")
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + owner_groups+=("${columns[1]}")
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + modes+=("${columns[2]}")
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + for row in "${HCFS_DIRS[@]}"
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + columns=(${row})
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + [[ true == \t\r\u\e ]]
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: ++ encode_path_for_webhdfs /var
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: ++ local encoded
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: +++ perl -MURI::Escape -s -e 'print uri_escape($query);' -- -query=/var
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: ++ encoded=%2Fvar
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: ++ echo /var
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + encoded_path=/var
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + dirs+=("${encoded_path}")
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + owner_groups+=("${columns[1]}")
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + modes+=("${columns[2]}")
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + for row in "${HCFS_DIRS[@]}"
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + columns=(${row})
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + [[ true == \t\r\u\e ]]
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: ++ encode_path_for_webhdfs /var/tmp
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: ++ local encoded
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: +++ perl -MURI::Escape -s -e 'print uri_escape($query);' -- -query=/var/tmp
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5098.exitcode ]]
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: ++ encoded=%2Fvar%2Ftmp
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: ++ echo /var/tmp
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + encoded_path=/var/tmp
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + dirs+=("${encoded_path}")
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + owner_groups+=("${columns[1]}")
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + modes+=("${columns[2]}")
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + for row in "${HCFS_DIRS[@]}"
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + columns=(${row})
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + [[ true == \t\r\u\e ]]
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: ++ encode_path_for_webhdfs /user/hdfs
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: ++ local encoded
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: +++ perl -MURI::Escape -s -e 'print uri_escape($query);' -- -query=/user/hdfs
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: ++ encoded=%2Fuser%2Fhdfs
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: ++ echo /user/hdfs
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + encoded_path=/user/hdfs
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + dirs+=("${encoded_path}")
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + owner_groups+=("${columns[1]}")
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + modes+=("${columns[2]}")
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + for row in "${HCFS_DIRS[@]}"
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + columns=(${row})
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + [[ true == \t\r\u\e ]]
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: ++ encode_path_for_webhdfs /user/mapred
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: ++ local encoded
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: +++ perl -MURI::Escape -s -e 'print uri_escape($query);' -- -query=/user/mapred
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: ++ encoded=%2Fuser%2Fmapred
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: ++ echo /user/mapred
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + encoded_path=/user/mapred
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + dirs+=("${encoded_path}")
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + owner_groups+=("${columns[1]}")
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + modes+=("${columns[2]}")
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + for row in "${HCFS_DIRS[@]}"
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + columns=(${row})
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: + [[ true == \t\r\u\e ]]
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: ++ encode_path_for_webhdfs /user/yarn
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: ++ local encoded
<13>Apr 27 18:08:43 dataproc-startup-script[1265]: <13>Apr 27 18:08:43 dataproc-activate-component-hdfs[5098]: +++ perl -MURI::Escape -s -e 'print uri_escape($query);' -- -query=/user/yarn
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: ++ encoded=%2Fuser%2Fyarn
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: ++ echo /user/yarn
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + encoded_path=/user/yarn
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + dirs+=("${encoded_path}")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + owner_groups+=("${columns[1]}")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + modes+=("${columns[2]}")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + for row in "${HCFS_DIRS[@]}"
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + columns=(${row})
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ true == \t\r\u\e ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: ++ encode_path_for_webhdfs /user/spark
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: ++ local encoded
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: +++ perl -MURI::Escape -s -e 'print uri_escape($query);' -- -query=/user/spark
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: ++ encoded=%2Fuser%2Fspark
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: ++ echo /user/spark
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + encoded_path=/user/spark
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + dirs+=("${encoded_path}")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + owner_groups+=("${columns[1]}")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + modes+=("${columns[2]}")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + for row in "${HCFS_DIRS[@]}"
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + columns=(${row})
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ true == \t\r\u\e ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: ++ encode_path_for_webhdfs /user/pig
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: ++ local encoded
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: +++ perl -MURI::Escape -s -e 'print uri_escape($query);' -- -query=/user/pig
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: ++ encoded=%2Fuser%2Fpig
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: ++ echo /user/pig
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + encoded_path=/user/pig
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + dirs+=("${encoded_path}")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + owner_groups+=("${columns[1]}")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + modes+=("${columns[2]}")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + for row in "${HCFS_DIRS[@]}"
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + columns=(${row})
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ true == \t\r\u\e ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: ++ encode_path_for_webhdfs /user/hive
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: ++ local encoded
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: +++ perl -MURI::Escape -s -e 'print uri_escape($query);' -- -query=/user/hive
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: ++ encoded=%2Fuser%2Fhive
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: ++ echo /user/hive
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + encoded_path=/user/hive
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + dirs+=("${encoded_path}")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + owner_groups+=("${columns[1]}")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + modes+=("${columns[2]}")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + for row in "${HCFS_DIRS[@]}"
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + columns=(${row})
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ true == \t\r\u\e ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: ++ encode_path_for_webhdfs /user/hbase
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: ++ local encoded
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: +++ perl -MURI::Escape -s -e 'print uri_escape($query);' -- -query=/user/hbase
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: ++ encoded=%2Fuser%2Fhbase
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: ++ echo /user/hbase
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + encoded_path=/user/hbase
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + dirs+=("${encoded_path}")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + owner_groups+=("${columns[1]}")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + modes+=("${columns[2]}")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + for row in "${HCFS_DIRS[@]}"
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + columns=(${row})
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ true == \t\r\u\e ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: ++ encode_path_for_webhdfs /user/zookeeper
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: ++ local encoded
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: +++ perl -MURI::Escape -s -e 'print uri_escape($query);' -- -query=/user/zookeeper
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: ++ encoded=%2Fuser%2Fzookeeper
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: ++ echo /user/zookeeper
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + encoded_path=/user/zookeeper
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + dirs+=("${encoded_path}")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + owner_groups+=("${columns[1]}")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + modes+=("${columns[2]}")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + for row in "${HCFS_DIRS[@]}"
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + columns=(${row})
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ true == \t\r\u\e ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: ++ encode_path_for_webhdfs /user/solr
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: ++ local encoded
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: +++ perl -MURI::Escape -s -e 'print uri_escape($query);' -- -query=/user/solr
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: ++ encoded=%2Fuser%2Fsolr
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: ++ echo /user/solr
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + encoded_path=/user/solr
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + dirs+=("${encoded_path}")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + owner_groups+=("${columns[1]}")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + modes+=("${columns[2]}")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + for row in "${HCFS_DIRS[@]}"
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + columns=(${row})
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ true == \t\r\u\e ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: ++ encode_path_for_webhdfs /user/zeppelin
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: ++ local encoded
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: +++ perl -MURI::Escape -s -e 'print uri_escape($query);' -- -query=/user/zeppelin
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: ++ encoded=%2Fuser%2Fzeppelin
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: ++ echo /user/zeppelin
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + encoded_path=/user/zeppelin
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + dirs+=("${encoded_path}")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + owner_groups+=("${columns[1]}")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + modes+=("${columns[2]}")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + for row in "${HCFS_DIRS[@]}"
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + columns=(${row})
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ true == \t\r\u\e ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: ++ encode_path_for_webhdfs /user/pwani
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: ++ local encoded
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: +++ perl -MURI::Escape -s -e 'print uri_escape($query);' -- -query=/user/pwani
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: ++ encoded=%2Fuser%2Fpwani
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: ++ echo /user/pwani
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + encoded_path=/user/pwani
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + dirs+=("${encoded_path}")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + owner_groups+=("${columns[1]}")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + modes+=("${columns[2]}")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ true == \t\r\u\e ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + echo 'Creating HCFS dirs with WebHDFS ...'
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: Creating HCFS dirs with WebHDFS ...
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local batch_size=20
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + pids=()
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -a pids
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + (( i=0 ))
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + (( i<18 ))
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ false == \t\r\u\e ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + pids+=("$!")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + (( (i+1)%20==0 ))
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + (( i++ ))
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + (( i<18 ))
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ false == \t\r\u\e ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + pids+=("$!")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + (( (i+1)%20==0 ))
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + (( i++ ))
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + (( i<18 ))
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ false == \t\r\u\e ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + pids+=("$!")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + (( (i+1)%20==0 ))
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + (( i++ ))
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + (( i<18 ))
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ false == \t\r\u\e ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + pids+=("$!")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + (( (i+1)%20==0 ))
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + (( i++ ))
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + (( i<18 ))
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ false == \t\r\u\e ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + pids+=("$!")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + (( (i+1)%20==0 ))
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + (( i++ ))
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + (( i<18 ))
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ false == \t\r\u\e ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + pids+=("$!")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + (( (i+1)%20==0 ))
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + (( i++ ))
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + (( i<18 ))
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ false == \t\r\u\e ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + pids+=("$!")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + (( (i+1)%20==0 ))
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + (( i++ ))
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + (( i<18 ))
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ false == \t\r\u\e ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + pids+=("$!")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + (( (i+1)%20==0 ))
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + (( i++ ))
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + (( i<18 ))
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ false == \t\r\u\e ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + pids+=("$!")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + (( (i+1)%20==0 ))
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + (( i++ ))
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + (( i<18 ))
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ false == \t\r\u\e ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + pids+=("$!")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + (( (i+1)%20==0 ))
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + (( i++ ))
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + (( i<18 ))
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ false == \t\r\u\e ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + pids+=("$!")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + (( (i+1)%20==0 ))
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + (( i++ ))
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + (( i<18 ))
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ false == \t\r\u\e ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + pids+=("$!")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + (( (i+1)%20==0 ))
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + (( i++ ))
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + (( i<18 ))
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ false == \t\r\u\e ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + pids+=("$!")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + (( (i+1)%20==0 ))
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + (( i++ ))
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + (( i<18 ))
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ false == \t\r\u\e ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + pids+=("$!")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + (( (i+1)%20==0 ))
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + (( i++ ))
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + (( i<18 ))
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ false == \t\r\u\e ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + pids+=("$!")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + (( (i+1)%20==0 ))
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + (( i++ ))
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + (( i<18 ))
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ false == \t\r\u\e ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + pids+=("$!")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + (( (i+1)%20==0 ))
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + (( i++ ))
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + (( i<18 ))
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ false == \t\r\u\e ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + pids+=("$!")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + (( (i+1)%20==0 ))
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + (( i++ ))
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + (( i<18 ))
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ false == \t\r\u\e ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + pids+=("$!")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + (( (i+1)%20==0 ))
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + (( i++ ))
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + (( i<18 ))
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + wait_all 8908 8909 8910 8911 8912 8913 8914 8915 8916 8917 8918 8919 8920 8921 8922 8923 8924 8925
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + pids=('8908' '8909' '8910' '8911' '8912' '8913' '8914' '8915' '8916' '8917' '8918' '8919' '8920' '8921' '8922' '8923' '8924' '8925')
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -a pids
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + for pid in "${pids[@]}"
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + wait 8908
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + webhdfs_mkdir /user/yarn hdfs
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + webhdfs_mkdir /user/hive hdfs
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -r dir=/user/hive
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -r owner=hdfs
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ -n hdfs ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + webhdfs 'http://cluster-dip-01-m:9870/webhdfs/v1/user/hive?op=MKDIRS&user.name=hdfs'
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -r 'uri=http://cluster-dip-01-m:9870/webhdfs/v1/user/hive?op=MKDIRS&user.name=hdfs'
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -a command
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + command=(curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30)
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ false == \t\r\u\e ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + command+=("${uri}")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + sudo -u hdfs curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30 'http://cluster-dip-01-m:9870/webhdfs/v1/user/hive?op=MKDIRS&user.name=hdfs'
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + webhdfs_mkdir /user/hbase hdfs
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + webhdfs_mkdir /user/zookeeper hdfs
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -r dir=/user/zookeeper
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -r owner=hdfs
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ -n hdfs ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + webhdfs 'http://cluster-dip-01-m:9870/webhdfs/v1/user/zookeeper?op=MKDIRS&user.name=hdfs'
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -r 'uri=http://cluster-dip-01-m:9870/webhdfs/v1/user/zookeeper?op=MKDIRS&user.name=hdfs'
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -a command
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + command=(curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30)
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ false == \t\r\u\e ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + command+=("${uri}")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + sudo -u hdfs curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30 'http://cluster-dip-01-m:9870/webhdfs/v1/user/zookeeper?op=MKDIRS&user.name=hdfs'
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + webhdfs_mkdir /user/solr hdfs
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -r dir=/user/solr
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -r owner=hdfs
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ -n hdfs ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + webhdfs_mkdir /user/zeppelin hdfs
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -r dir=/user/zeppelin
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -r owner=hdfs
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ -n hdfs ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + webhdfs 'http://cluster-dip-01-m:9870/webhdfs/v1/user/zeppelin?op=MKDIRS&user.name=hdfs'
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -r 'uri=http://cluster-dip-01-m:9870/webhdfs/v1/user/zeppelin?op=MKDIRS&user.name=hdfs'
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -a command
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + command=(curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30)
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ false == \t\r\u\e ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + command+=("${uri}")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + sudo -u hdfs curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30 'http://cluster-dip-01-m:9870/webhdfs/v1/user/zeppelin?op=MKDIRS&user.name=hdfs'
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + webhdfs_mkdir /user/pwani hdfs
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -r dir=/user/pwani
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -r owner=hdfs
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ -n hdfs ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + webhdfs_mkdir /user/spark hdfs
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -r dir=/user/spark
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -r owner=hdfs
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ -n hdfs ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + webhdfs 'http://cluster-dip-01-m:9870/webhdfs/v1/user/spark?op=MKDIRS&user.name=hdfs'
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -r 'uri=http://cluster-dip-01-m:9870/webhdfs/v1/user/spark?op=MKDIRS&user.name=hdfs'
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -a command
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + command=(curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30)
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ false == \t\r\u\e ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + command+=("${uri}")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + sudo -u hdfs curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30 'http://cluster-dip-01-m:9870/webhdfs/v1/user/spark?op=MKDIRS&user.name=hdfs'
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + webhdfs_mkdir /user/hdfs hdfs
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -r dir=/user/hdfs
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -r owner=hdfs
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ -n hdfs ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + webhdfs 'http://cluster-dip-01-m:9870/webhdfs/v1/user/hdfs?op=MKDIRS&user.name=hdfs'
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -r 'uri=http://cluster-dip-01-m:9870/webhdfs/v1/user/hdfs?op=MKDIRS&user.name=hdfs'
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -a command
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + command=(curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30)
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ false == \t\r\u\e ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + command+=("${uri}")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + webhdfs_mkdir /user/pig hdfs
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -r dir=/user/pig
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -r owner=hdfs
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ -n hdfs ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + webhdfs 'http://cluster-dip-01-m:9870/webhdfs/v1/user/pig?op=MKDIRS&user.name=hdfs'
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -r 'uri=http://cluster-dip-01-m:9870/webhdfs/v1/user/pig?op=MKDIRS&user.name=hdfs'
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -a command
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + command=(curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30)
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ false == \t\r\u\e ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + command+=("${uri}")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + sudo -u hdfs curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30 'http://cluster-dip-01-m:9870/webhdfs/v1/user/pig?op=MKDIRS&user.name=hdfs'
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + webhdfs_mkdir /var hdfs
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -r dir=/var
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -r owner=hdfs
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ -n hdfs ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + webhdfs 'http://cluster-dip-01-m:9870/webhdfs/v1/var?op=MKDIRS&user.name=hdfs'
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -r 'uri=http://cluster-dip-01-m:9870/webhdfs/v1/var?op=MKDIRS&user.name=hdfs'
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -a command
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + webhdfs_mkdir /user/mapred hdfs
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -r dir=/user/mapred
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -r owner=hdfs
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ -n hdfs ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + webhdfs 'http://cluster-dip-01-m:9870/webhdfs/v1/user/mapred?op=MKDIRS&user.name=hdfs'
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -r 'uri=http://cluster-dip-01-m:9870/webhdfs/v1/user/mapred?op=MKDIRS&user.name=hdfs'
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -a command
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + command=(curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30)
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + webhdfs_mkdir /var/tmp hdfs
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -r dir=/var/tmp
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -r owner=hdfs
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ -n hdfs ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + webhdfs 'http://cluster-dip-01-m:9870/webhdfs/v1/var/tmp?op=MKDIRS&user.name=hdfs'
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -r 'uri=http://cluster-dip-01-m:9870/webhdfs/v1/var/tmp?op=MKDIRS&user.name=hdfs'
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -a command
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + command=(curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30)
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ false == \t\r\u\e ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + command+=("${uri}")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -r dir=/user/yarn
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -r owner=hdfs
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ -n hdfs ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + webhdfs 'http://cluster-dip-01-m:9870/webhdfs/v1/user/yarn?op=MKDIRS&user.name=hdfs'
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -r 'uri=http://cluster-dip-01-m:9870/webhdfs/v1/user/yarn?op=MKDIRS&user.name=hdfs'
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -a command
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + command=(curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30)
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -r dir=/user/hbase
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -r owner=hdfs
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ -n hdfs ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + webhdfs 'http://cluster-dip-01-m:9870/webhdfs/v1/user/hbase?op=MKDIRS&user.name=hdfs'
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -r 'uri=http://cluster-dip-01-m:9870/webhdfs/v1/user/hbase?op=MKDIRS&user.name=hdfs'
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -a command
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + command=(curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30)
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ false == \t\r\u\e ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + command+=("${uri}")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + webhdfs 'http://cluster-dip-01-m:9870/webhdfs/v1/user/pwani?op=MKDIRS&user.name=hdfs'
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -r 'uri=http://cluster-dip-01-m:9870/webhdfs/v1/user/pwani?op=MKDIRS&user.name=hdfs'
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -a command
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + command=(curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30)
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ false == \t\r\u\e ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + command+=("${uri}")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + sudo -u hdfs curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30 'http://cluster-dip-01-m:9870/webhdfs/v1/user/pwani?op=MKDIRS&user.name=hdfs'
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + command=(curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30)
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + webhdfs_mkdir /user hdfs
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ false == \t\r\u\e ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + command+=("${uri}")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + sudo -u hdfs curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30 'http://cluster-dip-01-m:9870/webhdfs/v1/user/yarn?op=MKDIRS&user.name=hdfs'
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ false == \t\r\u\e ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + command+=("${uri}")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + sudo -u hdfs curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30 'http://cluster-dip-01-m:9870/webhdfs/v1/var?op=MKDIRS&user.name=hdfs'
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ false == \t\r\u\e ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + command+=("${uri}")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + sudo -u hdfs curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30 'http://cluster-dip-01-m:9870/webhdfs/v1/user/mapred?op=MKDIRS&user.name=hdfs'
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + sudo -u hdfs curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30 'http://cluster-dip-01-m:9870/webhdfs/v1/var/tmp?op=MKDIRS&user.name=hdfs'
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + sudo -u hdfs curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30 'http://cluster-dip-01-m:9870/webhdfs/v1/user/hdfs?op=MKDIRS&user.name=hdfs'
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + sudo -u hdfs curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30 'http://cluster-dip-01-m:9870/webhdfs/v1/user/hbase?op=MKDIRS&user.name=hdfs'
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -r dir=/user
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -r owner=hdfs
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ -n hdfs ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + webhdfs 'http://cluster-dip-01-m:9870/webhdfs/v1/user?op=MKDIRS&user.name=hdfs'
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -r 'uri=http://cluster-dip-01-m:9870/webhdfs/v1/user?op=MKDIRS&user.name=hdfs'
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -a command
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + command=(curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30)
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ false == \t\r\u\e ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + command+=("${uri}")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + sudo -u hdfs curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30 'http://cluster-dip-01-m:9870/webhdfs/v1/user?op=MKDIRS&user.name=hdfs'
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + webhdfs_mkdir /tmp/hadoop-yarn/staging/history hdfs
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -r dir=/tmp/hadoop-yarn/staging/history
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -r owner=hdfs
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ -n hdfs ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + webhdfs 'http://cluster-dip-01-m:9870/webhdfs/v1/tmp/hadoop-yarn/staging/history?op=MKDIRS&user.name=hdfs'
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -r 'uri=http://cluster-dip-01-m:9870/webhdfs/v1/tmp/hadoop-yarn/staging/history?op=MKDIRS&user.name=hdfs'
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -a command
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + command=(curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30)
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ false == \t\r\u\e ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + command+=("${uri}")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + sudo -u hdfs curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30 'http://cluster-dip-01-m:9870/webhdfs/v1/tmp/hadoop-yarn/staging/history?op=MKDIRS&user.name=hdfs'
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + webhdfs 'http://cluster-dip-01-m:9870/webhdfs/v1/user/solr?op=MKDIRS&user.name=hdfs'
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -r 'uri=http://cluster-dip-01-m:9870/webhdfs/v1/user/solr?op=MKDIRS&user.name=hdfs'
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -a command
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + command=(curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30)
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ false == \t\r\u\e ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + command+=("${uri}")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + sudo -u hdfs curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30 'http://cluster-dip-01-m:9870/webhdfs/v1/user/solr?op=MKDIRS&user.name=hdfs'
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + webhdfs_mkdir /tmp/hadoop-yarn/staging hdfs
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -r dir=/tmp/hadoop-yarn/staging
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -r owner=hdfs
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ -n hdfs ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + webhdfs 'http://cluster-dip-01-m:9870/webhdfs/v1/tmp/hadoop-yarn/staging?op=MKDIRS&user.name=hdfs'
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -r 'uri=http://cluster-dip-01-m:9870/webhdfs/v1/tmp/hadoop-yarn/staging?op=MKDIRS&user.name=hdfs'
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -a command
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + command=(curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30)
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ false == \t\r\u\e ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + command+=("${uri}")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + sudo -u hdfs curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30 'http://cluster-dip-01-m:9870/webhdfs/v1/tmp/hadoop-yarn/staging?op=MKDIRS&user.name=hdfs'
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + webhdfs_mkdir /tmp hdfs
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -r dir=/tmp
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -r owner=hdfs
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ -n hdfs ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + webhdfs 'http://cluster-dip-01-m:9870/webhdfs/v1/tmp?op=MKDIRS&user.name=hdfs'
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -r 'uri=http://cluster-dip-01-m:9870/webhdfs/v1/tmp?op=MKDIRS&user.name=hdfs'
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -a command
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + command=(curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30)
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ false == \t\r\u\e ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + command+=("${uri}")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + sudo -u hdfs curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30 'http://cluster-dip-01-m:9870/webhdfs/v1/tmp?op=MKDIRS&user.name=hdfs'
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + webhdfs_mkdir / hdfs
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -r dir=/
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -r owner=hdfs
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ -n hdfs ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + webhdfs 'http://cluster-dip-01-m:9870/webhdfs/v1/?op=MKDIRS&user.name=hdfs'
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -r 'uri=http://cluster-dip-01-m:9870/webhdfs/v1/?op=MKDIRS&user.name=hdfs'
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + local -a command
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + command=(curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30)
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + [[ false == \t\r\u\e ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + command+=("${uri}")
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hdfs[5098]: + sudo -u hdfs curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30 'http://cluster-dip-01-m:9870/webhdfs/v1/?op=MKDIRS&user.name=hdfs'
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: <13>Apr 27 18:08:44 dataproc-activate-component-hive-metastore[5099]: nc: connect to cluster-dip-01-m (10.142.0.8) port 9083 (tcp) failed: Connection refused
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5098.exitcode ]]
<13>Apr 27 18:08:44 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:08:45 dataproc-startup-script[1265]: <13>Apr 27 18:08:45 dataproc-activate-component-hive-metastore[5099]: nc: connect to cluster-dip-01-m (10.142.0.8) port 9083 (tcp) failed: Connection refused
<13>Apr 27 18:08:45 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5098.exitcode ]]
<13>Apr 27 18:08:45 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:08:46 dataproc-startup-script[1265]: <13>Apr 27 18:08:46 dataproc-activate-component-hive-metastore[5099]: nc: connect to cluster-dip-01-m (10.142.0.8) port 9083 (tcp) failed: Connection refused
<13>Apr 27 18:08:46 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5098.exitcode ]]
<13>Apr 27 18:08:46 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:08:47 dataproc-startup-script[1265]: <13>Apr 27 18:08:47 dataproc-activate-component-hive-metastore[5099]: Connection to cluster-dip-01-m (10.142.0.8) 9083 port [tcp/*] succeeded!
<13>Apr 27 18:08:47 dataproc-startup-script[1265]: <13>Apr 27 18:08:47 dataproc-activate-component-hive-metastore[5099]: 'nc -v -z -w 1 cluster-dip-01-m 9083' succeeded after 49 execution(s).
<13>Apr 27 18:08:47 dataproc-startup-script[1265]: <13>Apr 27 18:08:47 dataproc-activate-component-hive-metastore[5099]: + return 0
<13>Apr 27 18:08:47 dataproc-startup-script[1265]: <13>Apr 27 18:08:47 dataproc-activate-component-hive-metastore[5099]: + loginfo 'Service up on host=cluster-dip-01-m port=9083 name=hive-metastore.'
<13>Apr 27 18:08:47 dataproc-startup-script[1265]: <13>Apr 27 18:08:47 dataproc-activate-component-hive-metastore[5099]: + echo 'Service up on host=cluster-dip-01-m port=9083 name=hive-metastore.'
<13>Apr 27 18:08:47 dataproc-startup-script[1265]: <13>Apr 27 18:08:47 dataproc-activate-component-hive-metastore[5099]: Service up on host=cluster-dip-01-m port=9083 name=hive-metastore.
<13>Apr 27 18:08:47 dataproc-startup-script[1265]: <13>Apr 27 18:08:47 dataproc-activate-component-hive-metastore[5099]: ++ date +%s.%N
<13>Apr 27 18:08:47 dataproc-startup-script[1265]: <13>Apr 27 18:08:47 dataproc-activate-component-hive-metastore[5099]: + local -r end=1745777327.474394581
<13>Apr 27 18:08:47 dataproc-startup-script[1265]: <13>Apr 27 18:08:47 dataproc-activate-component-hive-metastore[5099]: + local -r runtime_s=64
<13>Apr 27 18:08:47 dataproc-startup-script[1265]: <13>Apr 27 18:08:47 dataproc-activate-component-hive-metastore[5099]: + echo 'Component hive-metastore took 64s to activate'
<13>Apr 27 18:08:47 dataproc-startup-script[1265]: <13>Apr 27 18:08:47 dataproc-activate-component-hive-metastore[5099]: Component hive-metastore took 64s to activate
<13>Apr 27 18:08:47 dataproc-startup-script[1265]: <13>Apr 27 18:08:47 dataproc-activate-component-hive-metastore[5099]: + local -r time_file=/tmp/dataproc/components/activate/hive-metastore.time
<13>Apr 27 18:08:47 dataproc-startup-script[1265]: <13>Apr 27 18:08:47 dataproc-activate-component-hive-metastore[5099]: + touch /tmp/dataproc/components/activate/hive-metastore.time
<13>Apr 27 18:08:47 dataproc-startup-script[1265]: <13>Apr 27 18:08:47 dataproc-activate-component-hive-metastore[5099]: + cat
<13>Apr 27 18:08:47 dataproc-startup-script[1265]: <13>Apr 27 18:08:47 dataproc-activate-component-hive-metastore[5099]: + [[ 0 -ne 0 ]]
<13>Apr 27 18:08:47 dataproc-startup-script[1265]: <13>Apr 27 18:08:47 dataproc-activate-component-hive-metastore[5099]: + echo 'Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/hive-metastore.sh'
<13>Apr 27 18:08:47 dataproc-startup-script[1265]: <13>Apr 27 18:08:47 dataproc-activate-component-hive-metastore[5099]: Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/hive-metastore.sh
<13>Apr 27 18:08:47 dataproc-startup-script[1265]: <13>Apr 27 18:08:47 dataproc-activate-component-hive-metastore[5099]: + touch /tmp/dataproc/components/activate/hive-metastore.done
<13>Apr 27 18:08:47 dataproc-startup-script[1265]: ++ echo 0
<13>Apr 27 18:08:47 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5098.exitcode ]]
<13>Apr 27 18:08:47 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: HTTP/1.1 200 OK
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Date: Sun, 27 Apr 2025 18:08:44 GMT
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Cache-Control: no-cache
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Expires: Sun, 27 Apr 2025 18:08:48 GMT
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Date: Sun, 27 Apr 2025 18:08:48 GMT
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Pragma: no-cache
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: X-Content-Type-Options: nosniff
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: X-FRAME-OPTIONS: SAMEORIGIN
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: X-XSS-Protection: 1; mode=block
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Set-Cookie: hadoop.auth="u=hdfs&p=hdfs&t=simple&e=1745813328390&s=rop78KMd5GNzvaB6MHrmujF5+/tTr+07X3n1yESAHIU="; Path=/; HttpOnly
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Content-Type: application/json
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Transfer-Encoding: chunked
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: 
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: {"boolean":true}HTTP/1.1 200 OK
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Date: Sun, 27 Apr 2025 18:08:44 GMT
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Cache-Control: no-cache
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Expires: Sun, 27 Apr 2025 18:08:48 GMT
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Date: Sun, 27 Apr 2025 18:08:48 GMT
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Pragma: no-cache
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: X-Content-Type-Options: nosniff
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: X-FRAME-OPTIONS: SAMEORIGIN
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: X-XSS-Protection: 1; mode=block
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Set-Cookie: hadoop.auth="u=hdfs&p=hdfs&t=simple&e=1745813328390&s=rop78KMd5GNzvaB6MHrmujF5+/tTr+07X3n1yESAHIU="; Path=/; HttpOnly
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Content-Type: application/json
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Transfer-Encoding: chunked
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: 
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: {"boolean":true}HTTP/1.1 200 OK
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Date: Sun, 27 Apr 2025 18:08:44 GMT
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Cache-Control: no-cache
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Expires: Sun, 27 Apr 2025 18:08:48 GMT
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Date: Sun, 27 Apr 2025 18:08:48 GMT
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Pragma: no-cache
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: X-Content-Type-Options: nosniff
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: X-FRAME-OPTIONS: SAMEORIGIN
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: X-XSS-Protection: 1; mode=block
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Set-Cookie: hadoop.auth="u=hdfs&p=hdfs&t=simple&e=1745813328390&s=rop78KMd5GNzvaB6MHrmujF5+/tTr+07X3n1yESAHIU="; Path=/; HttpOnly
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Content-Type: application/json
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Transfer-Encoding: chunked
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: 
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: {"boolean":true}+ for pid in "${pids[@]}"
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: + wait 8909
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: HTTP/1.1 200 OK
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Date: Sun, 27 Apr 2025 18:08:44 GMT
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Cache-Control: no-cache
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Expires: Sun, 27 Apr 2025 18:08:48 GMT
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Date: Sun, 27 Apr 2025 18:08:48 GMT
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Pragma: no-cache
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: X-Content-Type-Options: nosniff
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: X-FRAME-OPTIONS: SAMEORIGIN
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: X-XSS-Protection: 1; mode=block
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Set-Cookie: hadoop.auth="u=hdfs&p=hdfs&t=simple&e=1745813328390&s=rop78KMd5GNzvaB6MHrmujF5+/tTr+07X3n1yESAHIU="; Path=/; HttpOnly
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Content-Type: application/json
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Transfer-Encoding: chunked
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: 
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: {"boolean":true}HTTP/1.1 200 OK
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Date: Sun, 27 Apr 2025 18:08:44 GMT
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Cache-Control: no-cache
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Expires: Sun, 27 Apr 2025 18:08:48 GMT
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Date: Sun, 27 Apr 2025 18:08:48 GMT
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Pragma: no-cache
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: X-Content-Type-Options: nosniff
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: X-FRAME-OPTIONS: SAMEORIGIN
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: X-XSS-Protection: 1; mode=block
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Set-Cookie: hadoop.auth="u=hdfs&p=hdfs&t=simple&e=1745813328390&s=rop78KMd5GNzvaB6MHrmujF5+/tTr+07X3n1yESAHIU="; Path=/; HttpOnly
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Content-Type: application/json
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Transfer-Encoding: chunked
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: 
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: {"boolean":true}HTTP/1.1 200 OK
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Date: Sun, 27 Apr 2025 18:08:44 GMT
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Cache-Control: no-cache
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Expires: Sun, 27 Apr 2025 18:08:48 GMT
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Date: Sun, 27 Apr 2025 18:08:48 GMT
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Pragma: no-cache
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: X-Content-Type-Options: nosniff
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: X-FRAME-OPTIONS: SAMEORIGIN
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: X-XSS-Protection: 1; mode=block
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Set-Cookie: hadoop.auth="u=hdfs&p=hdfs&t=simple&e=1745813328390&s=rop78KMd5GNzvaB6MHrmujF5+/tTr+07X3n1yESAHIU="; Path=/; HttpOnly
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Content-Type: application/json
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Transfer-Encoding: chunked
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: 
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: {"boolean":true}HTTP/1.1 200 OK
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Date: Sun, 27 Apr 2025 18:08:44 GMT
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Cache-Control: no-cache
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Expires: Sun, 27 Apr 2025 18:08:48 GMT
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Date: Sun, 27 Apr 2025 18:08:48 GMT
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Pragma: no-cache
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: X-Content-Type-Options: nosniff
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: X-FRAME-OPTIONS: SAMEORIGIN
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: X-XSS-Protection: 1; mode=block
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Set-Cookie: hadoop.auth="u=hdfs&p=hdfs&t=simple&e=1745813328390&s=rop78KMd5GNzvaB6MHrmujF5+/tTr+07X3n1yESAHIU="; Path=/; HttpOnly
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Content-Type: application/json
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Transfer-Encoding: chunked
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: 
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: {"boolean":true}HTTP/1.1 200 OK
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Date: Sun, 27 Apr 2025 18:08:44 GMT
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Cache-Control: no-cache
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Expires: Sun, 27 Apr 2025 18:08:48 GMT
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Date: Sun, 27 Apr 2025 18:08:48 GMT
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Pragma: no-cache
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: X-Content-Type-Options: nosniff
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: X-FRAME-OPTIONS: SAMEORIGIN
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: X-XSS-Protection: 1; mode=block
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Set-Cookie: hadoop.auth="u=hdfs&p=hdfs&t=simple&e=1745813328390&s=rop78KMd5GNzvaB6MHrmujF5+/tTr+07X3n1yESAHIU="; Path=/; HttpOnly
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Content-Type: application/json
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Transfer-Encoding: chunked
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: 
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: {"boolean":true}HTTP/1.1 200 OK
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Date: Sun, 27 Apr 2025 18:08:44 GMT
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Cache-Control: no-cache
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Expires: Sun, 27 Apr 2025 18:08:48 GMT
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Date: Sun, 27 Apr 2025 18:08:48 GMT
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Pragma: no-cache
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: X-Content-Type-Options: nosniff
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: X-FRAME-OPTIONS: SAMEORIGIN
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: X-XSS-Protection: 1; mode=block
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Set-Cookie: hadoop.auth="u=hdfs&p=hdfs&t=simple&e=1745813328389&s=+h04NcjQIsIA6PjNzUGANI5R68GAEYAzDbJ/Zc0/x0Y="; Path=/; HttpOnly
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Content-Type: application/json
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Transfer-Encoding: chunked
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: 
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: {"boolean":true}HTTP/1.1 200 OK
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Date: Sun, 27 Apr 2025 18:08:44 GMT
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Cache-Control: no-cache
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Expires: Sun, 27 Apr 2025 18:08:48 GMT
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Date: Sun, 27 Apr 2025 18:08:48 GMT
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Pragma: no-cache
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: X-Content-Type-Options: nosniff
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: X-FRAME-OPTIONS: SAMEORIGIN
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: X-XSS-Protection: 1; mode=block
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Set-Cookie: hadoop.auth="u=hdfs&p=hdfs&t=simple&e=1745813328389&s=+h04NcjQIsIA6PjNzUGANI5R68GAEYAzDbJ/Zc0/x0Y="; Path=/; HttpOnly
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Content-Type: application/json
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Transfer-Encoding: chunked
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: 
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: {"boolean":true}HTTP/1.1 200 OK
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Date: Sun, 27 Apr 2025 18:08:44 GMT
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Cache-Control: no-cache
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Expires: Sun, 27 Apr 2025 18:08:48 GMT
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Date: Sun, 27 Apr 2025 18:08:48 GMT
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Pragma: no-cache
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: X-Content-Type-Options: nosniff
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: X-FRAME-OPTIONS: SAMEORIGIN
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: X-XSS-Protection: 1; mode=block
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Set-Cookie: hadoop.auth="u=hdfs&p=hdfs&t=simple&e=1745813328390&s=rop78KMd5GNzvaB6MHrmujF5+/tTr+07X3n1yESAHIU="; Path=/; HttpOnly
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Content-Type: application/json
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Transfer-Encoding: chunked
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: 
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: {"boolean":true}+ for pid in "${pids[@]}"
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: + wait 8910
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: HTTP/1.1 200 OK
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Date: Sun, 27 Apr 2025 18:08:44 GMT
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Cache-Control: no-cache
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Expires: Sun, 27 Apr 2025 18:08:48 GMT
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Date: Sun, 27 Apr 2025 18:08:48 GMT
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Pragma: no-cache
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: X-Content-Type-Options: nosniff
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: X-FRAME-OPTIONS: SAMEORIGIN
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: X-XSS-Protection: 1; mode=block
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Set-Cookie: hadoop.auth="u=hdfs&p=hdfs&t=simple&e=1745813328390&s=rop78KMd5GNzvaB6MHrmujF5+/tTr+07X3n1yESAHIU="; Path=/; HttpOnly
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Content-Type: application/json
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Transfer-Encoding: chunked
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: 
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: {"boolean":true}HTTP/1.1 200 OK
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Date: Sun, 27 Apr 2025 18:08:44 GMT
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Cache-Control: no-cache
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Expires: Sun, 27 Apr 2025 18:08:48 GMT
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Date: Sun, 27 Apr 2025 18:08:48 GMT
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Pragma: no-cache
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: X-Content-Type-Options: nosniff
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: X-FRAME-OPTIONS: SAMEORIGIN
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: X-XSS-Protection: 1; mode=block
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Set-Cookie: hadoop.auth="u=hdfs&p=hdfs&t=simple&e=1745813328390&s=rop78KMd5GNzvaB6MHrmujF5+/tTr+07X3n1yESAHIU="; Path=/; HttpOnly
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Content-Type: application/json
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Transfer-Encoding: chunked
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: 
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: {"boolean":true}HTTP/1.1 200 OK
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Date: Sun, 27 Apr 2025 18:08:44 GMT
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Cache-Control: no-cache
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Expires: Sun, 27 Apr 2025 18:08:48 GMT
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Date: Sun, 27 Apr 2025 18:08:48 GMT
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Pragma: no-cache
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: X-Content-Type-Options: nosniff
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: X-FRAME-OPTIONS: SAMEORIGIN
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: X-XSS-Protection: 1; mode=block
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Set-Cookie: hadoop.auth="u=hdfs&p=hdfs&t=simple&e=1745813328390&s=rop78KMd5GNzvaB6MHrmujF5+/tTr+07X3n1yESAHIU="; Path=/; HttpOnly
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Content-Type: application/json
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Transfer-Encoding: chunked
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: 
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: {"boolean":true}HTTP/1.1 200 OK
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Date: Sun, 27 Apr 2025 18:08:44 GMT
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Cache-Control: no-cache
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Expires: Sun, 27 Apr 2025 18:08:48 GMT
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Date: Sun, 27 Apr 2025 18:08:48 GMT
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Pragma: no-cache
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: X-Content-Type-Options: nosniff
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: X-FRAME-OPTIONS: SAMEORIGIN
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: X-XSS-Protection: 1; mode=block
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Set-Cookie: hadoop.auth="u=hdfs&p=hdfs&t=simple&e=1745813328391&s=trqJVNAQpi+LWZHoVhJf+oegt4yMXBCJ3PNoVws437w="; Path=/; HttpOnly
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Content-Type: application/json
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Transfer-Encoding: chunked
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: 
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: {"boolean":true}HTTP/1.1 200 OK
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Date: Sun, 27 Apr 2025 18:08:44 GMT
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Cache-Control: no-cache
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Expires: Sun, 27 Apr 2025 18:08:48 GMT
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Date: Sun, 27 Apr 2025 18:08:48 GMT
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Pragma: no-cache
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: X-Content-Type-Options: nosniff
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: X-FRAME-OPTIONS: SAMEORIGIN
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: X-XSS-Protection: 1; mode=block
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Set-Cookie: hadoop.auth="u=hdfs&p=hdfs&t=simple&e=1745813328389&s=+h04NcjQIsIA6PjNzUGANI5R68GAEYAzDbJ/Zc0/x0Y="; Path=/; HttpOnly
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Content-Type: application/json
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Transfer-Encoding: chunked
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: 
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: {"boolean":true}HTTP/1.1 200 OK
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Date: Sun, 27 Apr 2025 18:08:44 GMT
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Cache-Control: no-cache
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Expires: Sun, 27 Apr 2025 18:08:48 GMT
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Date: Sun, 27 Apr 2025 18:08:48 GMT
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Pragma: no-cache
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: X-Content-Type-Options: nosniff
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: X-FRAME-OPTIONS: SAMEORIGIN
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: X-XSS-Protection: 1; mode=block
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Set-Cookie: hadoop.auth="u=hdfs&p=hdfs&t=simple&e=1745813328390&s=rop78KMd5GNzvaB6MHrmujF5+/tTr+07X3n1yESAHIU="; Path=/; HttpOnly
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Content-Type: application/json
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Transfer-Encoding: chunked
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: 
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: {"boolean":true}HTTP/1.1 200 OK
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Date: Sun, 27 Apr 2025 18:08:44 GMT
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Cache-Control: no-cache
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Expires: Sun, 27 Apr 2025 18:08:48 GMT
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Date: Sun, 27 Apr 2025 18:08:48 GMT
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Pragma: no-cache
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: X-Content-Type-Options: nosniff
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: X-FRAME-OPTIONS: SAMEORIGIN
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: X-XSS-Protection: 1; mode=block
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Set-Cookie: hadoop.auth="u=hdfs&p=hdfs&t=simple&e=1745813328390&s=rop78KMd5GNzvaB6MHrmujF5+/tTr+07X3n1yESAHIU="; Path=/; HttpOnly
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Content-Type: application/json
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: Transfer-Encoding: chunked
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: 
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: {"boolean":true}+ for pid in "${pids[@]}"
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: + wait 8911
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: + for pid in "${pids[@]}"
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: + wait 8912
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: + for pid in "${pids[@]}"
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: + wait 8913
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: + for pid in "${pids[@]}"
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: + wait 8914
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: + for pid in "${pids[@]}"
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: + wait 8915
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: + for pid in "${pids[@]}"
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: + wait 8916
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: + for pid in "${pids[@]}"
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: + wait 8917
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: + for pid in "${pids[@]}"
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: + wait 8918
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: + for pid in "${pids[@]}"
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: + wait 8919
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: + for pid in "${pids[@]}"
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: + wait 8920
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: + for pid in "${pids[@]}"
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: + wait 8921
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: + for pid in "${pids[@]}"
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: + wait 8922
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: + for pid in "${pids[@]}"
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: + wait 8923
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: + for pid in "${pids[@]}"
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: + wait 8924
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: + for pid in "${pids[@]}"
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: + wait 8925
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: + [[ false == \f\a\l\s\e ]]
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: + retry_constant_short run_hdfs_command_as_hdfs -chmod -R 1777 /
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: + retry_constant_custom 30 1 run_hdfs_command_as_hdfs -chmod -R 1777 /
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: + local -r max_retry_time=30
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: + local -r retry_delay=1
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: + cmd=('run_hdfs_command_as_hdfs' '-chmod' '-R' '1777' '/')
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: + local -r cmd
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: + local -r max_retries=30
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: + local reenable_x=false
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: + [[ -o xtrace ]]
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: + set +x
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: <13>Apr 27 18:08:48 dataproc-activate-component-hdfs[5098]: About to run 'run_hdfs_command_as_hdfs -chmod -R 1777 /' with retries...
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5098.exitcode ]]
<13>Apr 27 18:08:48 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:08:49 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5098.exitcode ]]
<13>Apr 27 18:08:49 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:08:50 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5098.exitcode ]]
<13>Apr 27 18:08:50 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:08:51 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5098.exitcode ]]
<13>Apr 27 18:08:51 dataproc-startup-script[1265]: + sleep 1
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: <13>Apr 27 18:08:52 dataproc-activate-component-hdfs[5098]: 'run_hdfs_command_as_hdfs -chmod -R 1777 /' succeeded after 1 execution(s).
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: <13>Apr 27 18:08:52 dataproc-activate-component-hdfs[5098]: + return 0
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: <13>Apr 27 18:08:52 dataproc-activate-component-hdfs[5098]: + return
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: <13>Apr 27 18:08:52 dataproc-activate-component-hdfs[5098]: + [[ 3 == 0 ]]
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: <13>Apr 27 18:08:52 dataproc-activate-component-hdfs[5098]: ++ date +%s.%N
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: <13>Apr 27 18:08:52 dataproc-activate-component-hdfs[5098]: + local -r end=1745777332.750381451
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: <13>Apr 27 18:08:52 dataproc-activate-component-hdfs[5098]: + local -r runtime_s=69
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: <13>Apr 27 18:08:52 dataproc-activate-component-hdfs[5098]: + echo 'Component hdfs took 69s to activate'
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: <13>Apr 27 18:08:52 dataproc-activate-component-hdfs[5098]: Component hdfs took 69s to activate
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: <13>Apr 27 18:08:52 dataproc-activate-component-hdfs[5098]: + local -r time_file=/tmp/dataproc/components/activate/hdfs.time
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: <13>Apr 27 18:08:52 dataproc-activate-component-hdfs[5098]: + touch /tmp/dataproc/components/activate/hdfs.time
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: <13>Apr 27 18:08:52 dataproc-activate-component-hdfs[5098]: + cat
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: <13>Apr 27 18:08:52 dataproc-activate-component-hdfs[5098]: + [[ 0 -ne 0 ]]
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: <13>Apr 27 18:08:52 dataproc-activate-component-hdfs[5098]: + echo 'Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/hdfs.sh'
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: <13>Apr 27 18:08:52 dataproc-activate-component-hdfs[5098]: Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/hdfs.sh
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: <13>Apr 27 18:08:52 dataproc-activate-component-hdfs[5098]: + touch /tmp/dataproc/components/activate/hdfs.done
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: ++ echo 0
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5098.exitcode ]]
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + local status
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + status=0
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + (( status != 0 ))
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + tee /tmp/dataproc/commands/5098.done
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + echo 'Command cmd=[activate_component hdfs] pid=5098 exited with 0'
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: Command cmd=[activate_component hdfs] pid=5098 exited with 0
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + rm /tmp/dataproc/commands/5098.exitcode /tmp/dataproc/commands/5098.running
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + for running_file in "${COMMANDS_TMP_DIR}/"*'.running'
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + local pid
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: ++ basename /tmp/dataproc/commands/5099
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + pid=5099
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + local cmd
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + cmd='activate_component hive-metastore'
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + loginfo 'Waiting on pid=5099 cmd=[activate_component hive-metastore]'
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + echo 'Waiting on pid=5099 cmd=[activate_component hive-metastore]'
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: Waiting on pid=5099 cmd=[activate_component hive-metastore]
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + echo 'activate_component hive-metastore'
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + local exitcode_file=/tmp/dataproc/commands/5099.exitcode
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5099.exitcode ]]
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + local status
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + status=0
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + (( status != 0 ))
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + tee /tmp/dataproc/commands/5099.done
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + echo 'Command cmd=[activate_component hive-metastore] pid=5099 exited with 0'
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: Command cmd=[activate_component hive-metastore] pid=5099 exited with 0
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + rm /tmp/dataproc/commands/5099.exitcode /tmp/dataproc/commands/5099.running
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + for running_file in "${COMMANDS_TMP_DIR}/"*'.running'
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + local pid
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: ++ basename /tmp/dataproc/commands/5100
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + pid=5100
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + local cmd
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + cmd='activate_component hive-server2'
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + loginfo 'Waiting on pid=5100 cmd=[activate_component hive-server2]'
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + echo 'Waiting on pid=5100 cmd=[activate_component hive-server2]'
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: Waiting on pid=5100 cmd=[activate_component hive-server2]
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + echo 'activate_component hive-server2'
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + local exitcode_file=/tmp/dataproc/commands/5100.exitcode
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5100.exitcode ]]
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + local status
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + status=0
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + (( status != 0 ))
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + tee /tmp/dataproc/commands/5100.done
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + echo 'Command cmd=[activate_component hive-server2] pid=5100 exited with 0'
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: Command cmd=[activate_component hive-server2] pid=5100 exited with 0
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + rm /tmp/dataproc/commands/5100.exitcode /tmp/dataproc/commands/5100.running
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + for running_file in "${COMMANDS_TMP_DIR}/"*'.running'
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + local pid
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: ++ basename /tmp/dataproc/commands/5101
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + pid=5101
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + local cmd
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + cmd='activate_component knox'
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + loginfo 'Waiting on pid=5101 cmd=[activate_component knox]'
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + echo 'Waiting on pid=5101 cmd=[activate_component knox]'
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: Waiting on pid=5101 cmd=[activate_component knox]
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + echo 'activate_component knox'
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + local exitcode_file=/tmp/dataproc/commands/5101.exitcode
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5101.exitcode ]]
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + local status
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + status=0
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + (( status != 0 ))
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + tee /tmp/dataproc/commands/5101.done
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + echo 'Command cmd=[activate_component knox] pid=5101 exited with 0'
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: Command cmd=[activate_component knox] pid=5101 exited with 0
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + rm /tmp/dataproc/commands/5101.exitcode /tmp/dataproc/commands/5101.running
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + for running_file in "${COMMANDS_TMP_DIR}/"*'.running'
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + local pid
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: ++ basename /tmp/dataproc/commands/5103
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + pid=5103
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + local cmd
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + cmd='activate_component mapreduce'
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + loginfo 'Waiting on pid=5103 cmd=[activate_component mapreduce]'
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + echo 'Waiting on pid=5103 cmd=[activate_component mapreduce]'
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: Waiting on pid=5103 cmd=[activate_component mapreduce]
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + echo 'activate_component mapreduce'
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + local exitcode_file=/tmp/dataproc/commands/5103.exitcode
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5103.exitcode ]]
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + local status
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + status=0
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + (( status != 0 ))
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + tee /tmp/dataproc/commands/5103.done
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + echo 'Command cmd=[activate_component mapreduce] pid=5103 exited with 0'
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: Command cmd=[activate_component mapreduce] pid=5103 exited with 0
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + rm /tmp/dataproc/commands/5103.exitcode /tmp/dataproc/commands/5103.running
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + for running_file in "${COMMANDS_TMP_DIR}/"*'.running'
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + local pid
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: ++ basename /tmp/dataproc/commands/5105
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + pid=5105
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + local cmd
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + cmd='activate_component miniconda3'
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + loginfo 'Waiting on pid=5105 cmd=[activate_component miniconda3]'
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + echo 'Waiting on pid=5105 cmd=[activate_component miniconda3]'
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: Waiting on pid=5105 cmd=[activate_component miniconda3]
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + echo 'activate_component miniconda3'
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + local exitcode_file=/tmp/dataproc/commands/5105.exitcode
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5105.exitcode ]]
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + local status
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + status=0
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + (( status != 0 ))
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + tee /tmp/dataproc/commands/5105.done
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + echo 'Command cmd=[activate_component miniconda3] pid=5105 exited with 0'
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: Command cmd=[activate_component miniconda3] pid=5105 exited with 0
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + rm /tmp/dataproc/commands/5105.exitcode /tmp/dataproc/commands/5105.running
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + for running_file in "${COMMANDS_TMP_DIR}/"*'.running'
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + local pid
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: ++ basename /tmp/dataproc/commands/5106
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + pid=5106
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + local cmd
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + cmd='activate_component mysql'
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + loginfo 'Waiting on pid=5106 cmd=[activate_component mysql]'
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + echo 'Waiting on pid=5106 cmd=[activate_component mysql]'
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: Waiting on pid=5106 cmd=[activate_component mysql]
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + echo 'activate_component mysql'
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + local exitcode_file=/tmp/dataproc/commands/5106.exitcode
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5106.exitcode ]]
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + local status
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + status=0
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + (( status != 0 ))
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + tee /tmp/dataproc/commands/5106.done
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + echo 'Command cmd=[activate_component mysql] pid=5106 exited with 0'
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: Command cmd=[activate_component mysql] pid=5106 exited with 0
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + rm /tmp/dataproc/commands/5106.exitcode /tmp/dataproc/commands/5106.running
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + for running_file in "${COMMANDS_TMP_DIR}/"*'.running'
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + local pid
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: ++ basename /tmp/dataproc/commands/5107
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + pid=5107
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + local cmd
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + cmd='activate_component npd'
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + loginfo 'Waiting on pid=5107 cmd=[activate_component npd]'
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + echo 'Waiting on pid=5107 cmd=[activate_component npd]'
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: Waiting on pid=5107 cmd=[activate_component npd]
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + echo 'activate_component npd'
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + local exitcode_file=/tmp/dataproc/commands/5107.exitcode
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5107.exitcode ]]
<13>Apr 27 18:08:52 dataproc-startup-script[1265]: + local status
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + status=0
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + (( status != 0 ))
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + tee /tmp/dataproc/commands/5107.done
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + echo 'Command cmd=[activate_component npd] pid=5107 exited with 0'
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: Command cmd=[activate_component npd] pid=5107 exited with 0
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + rm /tmp/dataproc/commands/5107.exitcode /tmp/dataproc/commands/5107.running
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + for running_file in "${COMMANDS_TMP_DIR}/"*'.running'
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + local pid
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: ++ basename /tmp/dataproc/commands/5108
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + pid=5108
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + local cmd
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + cmd='activate_component otel-ucp'
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + loginfo 'Waiting on pid=5108 cmd=[activate_component otel-ucp]'
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + echo 'Waiting on pid=5108 cmd=[activate_component otel-ucp]'
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: Waiting on pid=5108 cmd=[activate_component otel-ucp]
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + echo 'activate_component otel-ucp'
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + local exitcode_file=/tmp/dataproc/commands/5108.exitcode
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5108.exitcode ]]
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + local status
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + status=0
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + (( status != 0 ))
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + tee /tmp/dataproc/commands/5108.done
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + echo 'Command cmd=[activate_component otel-ucp] pid=5108 exited with 0'
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: Command cmd=[activate_component otel-ucp] pid=5108 exited with 0
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + rm /tmp/dataproc/commands/5108.exitcode /tmp/dataproc/commands/5108.running
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + for running_file in "${COMMANDS_TMP_DIR}/"*'.running'
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + local pid
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: ++ basename /tmp/dataproc/commands/5109
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + pid=5109
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + local cmd
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + cmd='activate_component pig'
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + loginfo 'Waiting on pid=5109 cmd=[activate_component pig]'
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + echo 'Waiting on pid=5109 cmd=[activate_component pig]'
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: Waiting on pid=5109 cmd=[activate_component pig]
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + echo 'activate_component pig'
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + local exitcode_file=/tmp/dataproc/commands/5109.exitcode
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5109.exitcode ]]
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + local status
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + status=0
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + (( status != 0 ))
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + tee /tmp/dataproc/commands/5109.done
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + echo 'Command cmd=[activate_component pig] pid=5109 exited with 0'
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: Command cmd=[activate_component pig] pid=5109 exited with 0
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + rm /tmp/dataproc/commands/5109.exitcode /tmp/dataproc/commands/5109.running
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + for running_file in "${COMMANDS_TMP_DIR}/"*'.running'
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + local pid
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: ++ basename /tmp/dataproc/commands/5110
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + pid=5110
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + local cmd
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + cmd='activate_component proxy-agent'
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + loginfo 'Waiting on pid=5110 cmd=[activate_component proxy-agent]'
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + echo 'Waiting on pid=5110 cmd=[activate_component proxy-agent]'
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: Waiting on pid=5110 cmd=[activate_component proxy-agent]
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + echo 'activate_component proxy-agent'
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + local exitcode_file=/tmp/dataproc/commands/5110.exitcode
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5110.exitcode ]]
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + local status
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + status=0
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + (( status != 0 ))
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + echo 'Command cmd=[activate_component proxy-agent] pid=5110 exited with 0'
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + tee /tmp/dataproc/commands/5110.done
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: Command cmd=[activate_component proxy-agent] pid=5110 exited with 0
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + rm /tmp/dataproc/commands/5110.exitcode /tmp/dataproc/commands/5110.running
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + for running_file in "${COMMANDS_TMP_DIR}/"*'.running'
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + local pid
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: ++ basename /tmp/dataproc/commands/5111
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + pid=5111
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + local cmd
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + cmd='activate_component spark'
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + loginfo 'Waiting on pid=5111 cmd=[activate_component spark]'
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + echo 'Waiting on pid=5111 cmd=[activate_component spark]'
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: Waiting on pid=5111 cmd=[activate_component spark]
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + echo 'activate_component spark'
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + local exitcode_file=/tmp/dataproc/commands/5111.exitcode
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5111.exitcode ]]
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + local status
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + status=0
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + (( status != 0 ))
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + tee /tmp/dataproc/commands/5111.done
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + echo 'Command cmd=[activate_component spark] pid=5111 exited with 0'
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: Command cmd=[activate_component spark] pid=5111 exited with 0
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + rm /tmp/dataproc/commands/5111.exitcode /tmp/dataproc/commands/5111.running
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + for running_file in "${COMMANDS_TMP_DIR}/"*'.running'
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + local pid
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: ++ basename /tmp/dataproc/commands/5113
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + pid=5113
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + local cmd
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + cmd='activate_component tez'
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + loginfo 'Waiting on pid=5113 cmd=[activate_component tez]'
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + echo 'Waiting on pid=5113 cmd=[activate_component tez]'
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: Waiting on pid=5113 cmd=[activate_component tez]
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + echo 'activate_component tez'
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + local exitcode_file=/tmp/dataproc/commands/5113.exitcode
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5113.exitcode ]]
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + local status
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + status=0
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + (( status != 0 ))
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + tee /tmp/dataproc/commands/5113.done
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + echo 'Command cmd=[activate_component tez] pid=5113 exited with 0'
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: Command cmd=[activate_component tez] pid=5113 exited with 0
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + rm /tmp/dataproc/commands/5113.exitcode /tmp/dataproc/commands/5113.running
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + for running_file in "${COMMANDS_TMP_DIR}/"*'.running'
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + local pid
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: ++ basename /tmp/dataproc/commands/5114
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + pid=5114
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + local cmd
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + cmd='activate_component yarn'
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + loginfo 'Waiting on pid=5114 cmd=[activate_component yarn]'
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + echo 'Waiting on pid=5114 cmd=[activate_component yarn]'
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: Waiting on pid=5114 cmd=[activate_component yarn]
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + echo 'activate_component yarn'
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + local exitcode_file=/tmp/dataproc/commands/5114.exitcode
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/5114.exitcode ]]
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + local status
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + status=0
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + (( status != 0 ))
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + tee /tmp/dataproc/commands/5114.done
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + echo 'Command cmd=[activate_component yarn] pid=5114 exited with 0'
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: Command cmd=[activate_component yarn] pid=5114 exited with 0
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + rm /tmp/dataproc/commands/5114.exitcode /tmp/dataproc/commands/5114.running
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + for running_file in "${COMMANDS_TMP_DIR}/"*'.running'
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + local pid
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: ++ basename /tmp/dataproc/commands/6135
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + pid=6135
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + local cmd
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + cmd='enable_and_start_service google-osconfig-agent'
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + loginfo 'Waiting on pid=6135 cmd=[enable_and_start_service google-osconfig-agent]'
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + echo 'Waiting on pid=6135 cmd=[enable_and_start_service google-osconfig-agent]'
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: Waiting on pid=6135 cmd=[enable_and_start_service google-osconfig-agent]
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + echo 'enable_and_start_service google-osconfig-agent'
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + local exitcode_file=/tmp/dataproc/commands/6135.exitcode
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/6135.exitcode ]]
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + local status
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + status=0
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + (( status != 0 ))
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + tee /tmp/dataproc/commands/6135.done
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + echo 'Command cmd=[enable_and_start_service google-osconfig-agent] pid=6135 exited with 0'
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: Command cmd=[enable_and_start_service google-osconfig-agent] pid=6135 exited with 0
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + rm /tmp/dataproc/commands/6135.exitcode /tmp/dataproc/commands/6135.running
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + for running_file in "${COMMANDS_TMP_DIR}/"*'.running'
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + local pid
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: ++ basename /tmp/dataproc/commands/6842
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + pid=6842
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + local cmd
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + cmd=backup_original_configs
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + loginfo 'Waiting on pid=6842 cmd=[backup_original_configs]'
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + echo 'Waiting on pid=6842 cmd=[backup_original_configs]'
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: Waiting on pid=6842 cmd=[backup_original_configs]
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + echo backup_original_configs
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + local exitcode_file=/tmp/dataproc/commands/6842.exitcode
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + [[ ! -f /tmp/dataproc/commands/6842.exitcode ]]
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + local status
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + status=0
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + (( status != 0 ))
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + tee /tmp/dataproc/commands/6842.done
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + echo 'Command cmd=[backup_original_configs] pid=6842 exited with 0'
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: Command cmd=[backup_original_configs] pid=6842 exited with 0
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + rm /tmp/dataproc/commands/6842.exitcode /tmp/dataproc/commands/6842.running
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + is_ubuntu
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: ++ os_id
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: ++ xargs
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: ++ grep '^ID=' /etc/os-release
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: ++ cut -d= -f2
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + [[ debian == \u\b\u\n\t\u ]]
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + loginfo 'All done'
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: + echo 'All done'
<13>Apr 27 18:08:53 dataproc-startup-script[1265]: All done
